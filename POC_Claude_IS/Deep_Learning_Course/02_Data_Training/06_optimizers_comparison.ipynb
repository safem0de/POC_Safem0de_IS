{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ ‡∏ö‡∏ó‡∏ó‡∏µ‡πà 2.6: Optimizers Comparison - ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Optimizers\n",
    "\n",
    "## üèÉ ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡πà‡∏á 4 ‡∏Ñ‡∏ô\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏¥‡πà‡∏á‡∏•‡∏á‡πÄ‡∏Ç‡∏≤‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á**‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î**‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Minimize Loss)\n",
    "\n",
    "‡∏°‡∏µ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡πà‡∏á 4 ‡∏Ñ‡∏ô ‡∏ß‡∏¥‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô:\n",
    "\n",
    "### üêå SGD (Stochastic Gradient Descent)\n",
    "- ‡∏ß‡∏¥‡πà‡∏á**‡∏ä‡πâ‡∏≤‡πÜ ‡πÅ‡∏ï‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á**\n",
    "- ‡∏°‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏£‡∏≠‡∏ö‡∏ï‡∏±‡∏ß ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏¥‡πà‡∏á‡∏•‡∏á‡πÄ‡∏ô‡∏¥‡∏ô\n",
    "- ‡∏ï‡∏¥‡∏î‡∏´‡∏•‡∏∏‡∏° (local minimum) ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢, ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:** ‡∏ä‡πâ‡∏≤, ‡∏ï‡∏¥‡∏î‡∏´‡∏•‡∏∏‡∏°\n",
    "\n",
    "### üí® Momentum\n",
    "- ‡∏ß‡∏¥‡πà‡∏á**‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ** ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏•‡∏π‡∏Å‡∏ö‡∏≠‡∏•‡∏Å‡∏•‡∏¥‡πâ‡∏á\n",
    "- ‡∏ñ‡πâ‡∏≤‡∏ß‡∏¥‡πà‡∏á‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ô‡∏≤‡∏ô ‚Üí ‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
    "- ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏•‡∏∏‡∏°‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡πÑ‡∏î‡πâ (‡∏°‡∏µ momentum)\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ SGD, ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏•‡∏∏‡∏°‡πÑ‡∏î‡πâ\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:** ‡∏≠‡∏≤‡∏à‡πÅ‡∏Å‡∏ß‡πà‡∏á‡∏Ç‡πâ‡∏≤‡∏°‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î\n",
    "\n",
    "### üéØ RMSprop\n",
    "- ‡∏ß‡∏¥‡πà‡∏á**‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏ï‡∏≤‡∏°‡∏†‡∏π‡∏°‡∏¥‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®**\n",
    "- ‡∏ó‡∏≤‡∏á‡∏ä‡∏±‡∏ô ‚Üí ‡∏ß‡∏¥‡πà‡∏á‡∏ä‡πâ‡∏≤ (‡∏£‡∏∞‡∏ß‡∏±‡∏á)\n",
    "- ‡∏ó‡∏≤‡∏á‡∏£‡∏≤‡∏ö ‚Üí ‡∏ß‡∏¥‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÑ‡∏î‡πâ‡∏î‡∏µ\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:** ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤\n",
    "\n",
    "### üèÜ Adam (Adaptive Moment Estimation)\n",
    "- **‡∏£‡∏ß‡∏°‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô!**\n",
    "- ‡∏°‡∏µ momentum + ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
    "- **All-rounder** - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡πÄ‡∏£‡πá‡∏ß, ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÑ‡∏î‡πâ‡∏î‡∏µ, ‡πÉ‡∏ä‡πâ‡∏á‡πà‡∏≤‡∏¢\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:** ‡πÉ‡∏ä‡πâ memory ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤\n",
    "\n",
    "---\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** ‡πÉ‡∏Ñ‡∏£‡∏à‡∏∞‡∏ñ‡∏∂‡∏á‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î? ü§î\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:** ‡∏°‡∏≤‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏¢! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print(\"‚úÖ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 1: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å Optimizers\n",
    "\n",
    "### Optimizer ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï weights ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î loss\n",
    "\n",
    "### ‡∏™‡∏π‡∏ï‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô:\n",
    "```\n",
    "weight_new = weight_old - learning_rate √ó gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Loss Landscape (‡∏†‡∏π‡πÄ‡∏Ç‡∏≤)\n",
    "def create_loss_landscape():\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    y = np.linspace(-5, 5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Loss function (‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏•‡∏∏‡∏°)\n",
    "    Z = (X**2 + Y**2) / 20 + np.sin(X) * np.cos(Y) + 5\n",
    "    \n",
    "    return X, Y, Z\n",
    "\n",
    "X, Y, Z = create_loss_landscape()\n",
    "\n",
    "# Plot 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 3D view\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('Weight 1')\n",
    "ax1.set_ylabel('Weight 2')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('Loss Landscape (3D View)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Contour view\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('Weight 1')\n",
    "ax2.set_ylabel('Weight 2')\n",
    "ax2.set_title('Loss Landscape (Contour View)', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üóª This is the 'mountain' our optimizers need to descend!\")\n",
    "print(\"   Goal: Reach the LOWEST point (minimum loss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Part 2: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Optimizers ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\n",
    "\n",
    "### ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "comparison = pd.DataFrame({\n",
    "    'Optimizer': ['SGD', 'SGD + Momentum', 'RMSprop', 'Adam', 'AdaGrad'],\n",
    "    'Learning Rate': ['‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏á', '‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏á', '‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥', '‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥', '‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥'],\n",
    "    'Speed': ['‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê'],\n",
    "    'Stability': ['‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê', '‚≠ê‚≠ê‚≠ê'],\n",
    "    'Memory': ['‡∏ô‡πâ‡∏≠‡∏¢', '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á', '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á', '‡∏°‡∏≤‡∏Å', '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'],\n",
    "    'Best For': [\n",
    "        '‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÜ, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢',\n",
    "        'CNNs, ‡∏ä‡∏≠‡∏ö‡∏ß‡∏¥‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß',\n",
    "        'RNNs, Time Series',\n",
    "        '‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á! (All-rounder)',\n",
    "        'Sparse data'\n",
    "    ],\n",
    "    'Default LR': ['0.01', '0.01', '0.001', '0.001', '0.01']\n",
    "})\n",
    "\n",
    "print(\"üìä Optimizers Comparison Table\")\n",
    "print(\"=\"*100)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\n‚≠ê = Rating (1-5 stars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Formulas\n",
    "print(\"\\nüìê Mathematical Formulas:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ SGD (Stochastic Gradient Descent):\")\n",
    "print(\"\"\"\n",
    "   w = w - Œ± √ó ‚àáL\n",
    "   \n",
    "   w = weight\n",
    "   Œ± = learning rate\n",
    "   ‚àáL = gradient of loss\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ SGD with Momentum:\")\n",
    "print(\"\"\"\n",
    "   v = Œ≤ √ó v + ‚àáL\n",
    "   w = w - Œ± √ó v\n",
    "   \n",
    "   v = velocity (momentum)\n",
    "   Œ≤ = momentum coefficient (usually 0.9)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ RMSprop:\")\n",
    "print(\"\"\"\n",
    "   s = Œ≤ √ó s + (1-Œ≤) √ó (‚àáL)¬≤\n",
    "   w = w - Œ± √ó ‚àáL / ‚àö(s + Œµ)\n",
    "   \n",
    "   s = squared gradient average\n",
    "   Œµ = small number (1e-8) to avoid division by zero\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Adam (Adaptive Moment Estimation):\")\n",
    "print(\"\"\"\n",
    "   m = Œ≤‚ÇÅ √ó m + (1-Œ≤‚ÇÅ) √ó ‚àáL        # First moment (mean)\n",
    "   v = Œ≤‚ÇÇ √ó v + (1-Œ≤‚ÇÇ) √ó (‚àáL)¬≤     # Second moment (variance)\n",
    "   \n",
    "   mÃÇ = m / (1 - Œ≤‚ÇÅ·µó)               # Bias correction\n",
    "   vÃÇ = v / (1 - Œ≤‚ÇÇ·µó)\n",
    "   \n",
    "   w = w - Œ± √ó mÃÇ / (‚àövÃÇ + Œµ)\n",
    "   \n",
    "   Œ≤‚ÇÅ = 0.9 (default)\n",
    "   Œ≤‚ÇÇ = 0.999 (default)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÅ Part 3: ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏±‡∏ö MNIST Dataset\n",
    "\n",
    "‡∏°‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MNIST ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π‡∏ß‡πà‡∏≤ optimizer ‡πÑ‡∏´‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏±‡∏ô!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess\n",
    "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà subset ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
    "x_train_small = x_train[:10000]\n",
    "y_train_small = y_train[:10000]\n",
    "x_test_small = x_test[:2000]\n",
    "y_test_small = y_test[:2000]\n",
    "\n",
    "print(\"‚úÖ MNIST Data loaded\")\n",
    "print(f\"   Training: {x_train_small.shape}\")\n",
    "print(f\"   Test: {x_test_small.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å optimizer\n",
    "def create_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Optimizers ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏î‡∏•‡∏≠‡∏á\n",
    "optimizers_to_test = {\n",
    "    'SGD': SGD(learning_rate=0.01),\n",
    "    'SGD+Momentum': SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'RMSprop': RMSprop(learning_rate=0.001),\n",
    "    'Adam': Adam(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "print(\"üîß Models ready!\")\n",
    "print(f\"   Testing {len(optimizers_to_test)} optimizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "histories = {}\n",
    "epochs = 20\n",
    "\n",
    "print(\"üèÉ Training models with different optimizers...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, optimizer in optimizers_to_test.items():\n",
    "    print(f\"\\nüîÑ Training with {name}...\")\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà\n",
    "    model = create_model()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        x_train_small, y_train_small,\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_test_small, y_test_small),\n",
    "        verbose=0  # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á progress\n",
    "    )\n",
    "    \n",
    "    histories[name] = history.history\n",
    "    \n",
    "    final_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"   ‚úì Final Train Acc: {final_acc:.4f}\")\n",
    "    print(f\"   ‚úì Final Val Acc: {final_val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "colors = {'SGD': 'red', 'SGD+Momentum': 'orange', 'RMSprop': 'green', 'Adam': 'blue'}\n",
    "\n",
    "# Training Loss\n",
    "for name, history in histories.items():\n",
    "    axes[0, 0].plot(history['loss'], label=name, color=colors[name], linewidth=2)\n",
    "axes[0, 0].set_title('Training Loss', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "for name, history in histories.items():\n",
    "    axes[0, 1].plot(history['val_loss'], label=name, color=colors[name], linewidth=2)\n",
    "axes[0, 1].set_title('Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training Accuracy\n",
    "for name, history in histories.items():\n",
    "    axes[1, 0].plot(history['accuracy'], label=name, color=colors[name], linewidth=2)\n",
    "axes[1, 0].set_title('Training Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "for name, history in histories.items():\n",
    "    axes[1, 1].plot(history['val_accuracy'], label=name, color=colors[name], linewidth=2)\n",
    "axes[1, 1].set_title('Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "print(\"\\nüìä Final Results Summary:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for name, history in histories.items():\n",
    "    results.append({\n",
    "        'Optimizer': name,\n",
    "        'Final Train Acc': f\"{history['accuracy'][-1]:.4f}\",\n",
    "        'Final Val Acc': f\"{history['val_accuracy'][-1]:.4f}\",\n",
    "        'Final Train Loss': f\"{history['loss'][-1]:.4f}\",\n",
    "        'Final Val Loss': f\"{history['val_loss'][-1]:.4f}\",\n",
    "        'Converged at Epoch': np.argmin(history['val_loss']) + 1\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# ‡∏´‡∏≤‡∏ú‡∏π‡πâ‡∏ä‡∏ô‡∏∞\n",
    "best_optimizer = results_df.loc[results_df['Final Val Acc'].idxmax(), 'Optimizer']\n",
    "print(f\"\\nüèÜ Winner: {best_optimizer}\")\n",
    "print(f\"   Best validation accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 4: ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Optimizer ‡πÑ‡∏´‡∏ô?\n",
    "\n",
    "### Decision Guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ When to Use Which Optimizer?\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Adam - ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (Default Choice) ‚úÖ\")\n",
    "print(\"\"\"\n",
    "   Use when:\n",
    "   ‚Ä¢ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡πÉ‡∏´‡∏°‡πà\n",
    "   ‚Ä¢ ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏ô‡πÑ‡∏´‡∏ô\n",
    "   ‚Ä¢ Deep Learning ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n",
    "   ‚Ä¢ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ converge ‡πÄ‡∏£‡πá‡∏ß\n",
    "   \n",
    "   Settings:\n",
    "   ‚Ä¢ learning_rate=0.001 (default)\n",
    "   ‚Ä¢ beta_1=0.9, beta_2=0.999\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ SGD + Momentum - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CNNs ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ generalize ‡∏î‡∏µ\")\n",
    "print(\"\"\"\n",
    "   Use when:\n",
    "   ‚Ä¢ Image Classification\n",
    "   ‚Ä¢ Computer Vision tasks\n",
    "   ‚Ä¢ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà generalize ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ (‡πÅ‡∏°‡πâ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤)\n",
    "   ‚Ä¢ ‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏≤‡∏ô\n",
    "   \n",
    "   Settings:\n",
    "   ‚Ä¢ learning_rate=0.01\n",
    "   ‚Ä¢ momentum=0.9\n",
    "   ‚Ä¢ ‡πÉ‡∏ä‡πâ Learning Rate Scheduling\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ RMSprop - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RNNs ‡πÅ‡∏•‡∏∞ Time Series\")\n",
    "print(\"\"\"\n",
    "   Use when:\n",
    "   ‚Ä¢ Recurrent Neural Networks (RNNs)\n",
    "   ‚Ä¢ LSTMs, GRUs\n",
    "   ‚Ä¢ Time Series Prediction\n",
    "   ‚Ä¢ ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Non-stationary\n",
    "   \n",
    "   Settings:\n",
    "   ‚Ä¢ learning_rate=0.001\n",
    "   ‚Ä¢ rho=0.9\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ AdaGrad - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sparse Data\")\n",
    "print(\"\"\"\n",
    "   Use when:\n",
    "   ‚Ä¢ Natural Language Processing\n",
    "   ‚Ä¢ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà sparse (‡πÄ‡∏ï‡πá‡∏°‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢ 0)\n",
    "   ‚Ä¢ Word embeddings\n",
    "   \n",
    "   Settings:\n",
    "   ‚Ä¢ learning_rate=0.01\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ Plain SGD - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\")\n",
    "print(\"\"\"\n",
    "   Use when:\n",
    "   ‚Ä¢ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏§‡∏©‡∏é‡∏µ\n",
    "   ‚Ä¢ ‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏á‡πà‡∏≤‡∏¢‡πÜ\n",
    "   ‚Ä¢ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à optimization\n",
    "   \n",
    "   ‚ùå ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Part 5: Tips & Tricks\n",
    "\n",
    "### Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "print(\"üìà Learning Rate Scheduling Strategies:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ ReduceLROnPlateau (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)\")\n",
    "print(\"\"\"\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,           # ‡∏•‡∏î LR ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 50%\n",
    "    patience=5,           # ‡∏£‡∏≠ 5 epochs\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "model.fit(..., callbacks=[reduce_lr])\n",
    "\n",
    "‚Üí ‡∏ñ‡πâ‡∏≤ val_loss ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô 5 epochs ‚Üí ‡∏•‡∏î LR\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Step Decay\")\n",
    "print(\"\"\"\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    lr = initial_lr * (drop ** (epoch // epochs_drop))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(step_decay)\n",
    "\n",
    "‚Üí ‡∏•‡∏î LR ‡∏ó‡∏∏‡∏Å‡πÜ 10 epochs\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Cosine Annealing\")\n",
    "print(\"\"\"\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "lr_schedule = CosineDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=1000\n",
    ")\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "‚Üí LR ‡∏•‡∏î‡∏•‡∏á‡∏ï‡∏≤‡∏° cosine curve\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Learning Rate Schedules\n",
    "epochs_range = np.arange(1, 51)\n",
    "\n",
    "# Constant\n",
    "lr_constant = np.ones(50) * 0.01\n",
    "\n",
    "# Step Decay\n",
    "lr_step = [0.01 * (0.5 ** (e // 10)) for e in epochs_range]\n",
    "\n",
    "# Exponential Decay\n",
    "lr_exp = [0.01 * (0.95 ** e) for e in epochs_range]\n",
    "\n",
    "# Cosine\n",
    "lr_cosine = [0.01 * (0.5 * (1 + np.cos(np.pi * e / 50))) for e in epochs_range]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_range, lr_constant, 'b-', linewidth=2, label='Constant')\n",
    "plt.plot(epochs_range, lr_step, 'r-', linewidth=2, label='Step Decay')\n",
    "plt.plot(epochs_range, lr_exp, 'g-', linewidth=2, label='Exponential Decay')\n",
    "plt.plot(epochs_range, lr_cosine, 'm-', linewidth=2, label='Cosine Annealing')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Learning Rate Scheduling Strategies', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡πÉ‡∏ä‡πâ ReduceLROnPlateau ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° performance ‡∏à‡∏£‡∏¥‡∏á!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô\n",
    "\n",
    "### ‚úÖ Key Takeaways:\n",
    "\n",
    "#### 1. **Optimizer Ranking (‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ):**\n",
    "```\n",
    "üèÜ Adam          ‚Üí ‡πÄ‡∏£‡πá‡∏ß, ‡πÉ‡∏ä‡πâ‡∏á‡πà‡∏≤‡∏¢, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "ü•à SGD+Momentum  ‚Üí Generalize ‡∏î‡∏µ, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö CV\n",
    "ü•â RMSprop       ‚Üí ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö RNN\n",
    "```\n",
    "\n",
    "#### 2. **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà:**\n",
    "- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ **Adam** ‡πÄ‡∏™‡∏°‡∏≠\n",
    "- Learning rate = 0.001\n",
    "- ‡πÉ‡∏ä‡πâ ReduceLROnPlateau\n",
    "\n",
    "#### 3. **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÇ‡∏õ‡∏£:**\n",
    "- CNNs: SGD + Momentum + LR Scheduling\n",
    "- RNNs: RMSprop ‡∏´‡∏£‡∏∑‡∏≠ Adam\n",
    "- NLP: Adam\n",
    "\n",
    "#### 4. **Common Mistakes:**\n",
    "- ‚ùå Learning rate ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô ‚Üí ‡πÑ‡∏°‡πà converge\n",
    "- ‚ùå Learning rate ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô ‚Üí ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å\n",
    "- ‚ùå ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ LR scheduling ‚Üí stuck ‡∏ó‡∏µ‡πà plateau\n",
    "- ‚ùå ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô optimizer ‡∏Å‡∏•‡∏≤‡∏á‡∏ó‡∏≤‡∏á ‚Üí weights ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô\n",
    "\n",
    "#### 5. **Best Practices:**\n",
    "- ‚úÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ Adam\n",
    "- ‚úÖ ‡πÉ‡∏ä‡πâ Learning Rate Finder (optional)\n",
    "- ‚úÖ Monitor training curves\n",
    "- ‚úÖ ‡πÉ‡∏ä‡πâ ReduceLROnPlateau\n",
    "- ‚úÖ Save best model\n",
    "\n",
    "### üéì ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏î‡πâ‡∏ß‡∏¢!\n",
    "\n",
    "‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏£‡∏ö **‡∏ö‡∏ó‡∏ó‡∏µ‡πà 2: Data Training** ‡πÅ‡∏•‡πâ‡∏ß! üéâ\n",
    "\n",
    "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏£‡∏π‡πâ:\n",
    "- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å CSV, Images, APIs ‚úÖ\n",
    "- ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Missing, Outliers, Inconsistencies) ‚úÖ\n",
    "- Transform ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Normalization, Standardization) ‚úÖ\n",
    "- Data Augmentation ‚úÖ\n",
    "- ‡πÅ‡∏ö‡πà‡∏á Train/Test + ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á Data Leakage ‚úÖ\n",
    "- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Optimizer ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‚úÖ\n",
    "\n",
    "### üöÄ Next Chapter:\n",
    "\n",
    "üëâ [‡∏ö‡∏ó‡∏ó‡∏µ‡πà 3: CNN ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û](../../03_CNN_Image_Processing/README.md)\n",
    "\n",
    "‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á Image Classifier ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á? üì∏"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
