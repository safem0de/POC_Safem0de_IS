{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ ‡∏ö‡∏ó‡∏ó‡∏µ‡πà 2.5: Train/Test Split - ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "\n",
    "## üéì ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö\n",
    "\n",
    "‡∏•‡∏≠‡∏á‡∏ô‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏≠‡∏ö:\n",
    "\n",
    "### üò± ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ú‡∏¥‡∏î (Data Leakage):\n",
    "- ‡∏Ñ‡∏£‡∏π‡πÉ‡∏´‡πâ‡∏î‡∏π **‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á** ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏≠‡∏ö\n",
    "- ‡∏Ñ‡∏∏‡∏ì‡∏ó‡πà‡∏≠‡∏á‡∏à‡∏≥‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "- ‡∏ß‡∏±‡∏ô‡∏™‡∏≠‡∏ö: ‡πÑ‡∏î‡πâ 100 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô! üéâ\n",
    "- ‡πÅ‡∏ï‡πà... **‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡∏à‡∏£‡∏¥‡∏á‡πÜ**\n",
    "\n",
    "### ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ñ‡∏π‡∏Å (Proper Split):\n",
    "- ‡∏Ñ‡∏£‡∏π‡πÉ‡∏´‡πâ **‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î** (Training Data)\n",
    "- ‡∏Ñ‡∏∏‡∏ì‡∏ù‡∏∂‡∏Å‡∏ó‡∏≥ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£\n",
    "- ‡∏ß‡∏±‡∏ô‡∏™‡∏≠‡∏ö: ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **‡πÉ‡∏´‡∏°‡πà** ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô (Test Data)\n",
    "- ‡πÑ‡∏î‡πâ 85 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‚Üí ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤**‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏£‡∏¥‡∏á**! ‚ú®\n",
    "\n",
    "**Machine Learning ‡∏Å‡πá‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô:**\n",
    "- **Train Data** = ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î (‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ)\n",
    "- **Test Data** = ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á (‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏£‡∏¥‡∏á)\n",
    "- **Data Leakage** = ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• test ‡∏£‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÉ‡∏ô train ‚Üí ‡πÇ‡∏°‡πÄ‡∏î‡∏•**‡πÇ‡∏Å‡∏á**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"‚úÖ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV\n",
    "\n",
    "### ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô:\n",
    "- **70% Training** - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "- **15% Validation** - Fine-tune\n",
    "- **15% Test** - ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "data = {\n",
    "    'Feature1': np.random.randn(n_samples),\n",
    "    'Feature2': np.random.randn(n_samples),\n",
    "    'Feature3': np.random.randn(n_samples),\n",
    "    'Target': np.random.choice([0, 1], n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Features: {df.columns.tolist()[:-1]}\")\n",
    "print(f\"   Target: {df['Target'].unique()}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÅ‡∏ö‡πà‡∏á 80-20 (Train-Test)\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,      # 20% test\n",
    "    random_state=42     # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏•‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    ")\n",
    "\n",
    "print(\"üì¶ 80-20 Split:\")\n",
    "print(f\"   Training: {len(X_train)} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(X_test)} samples ({len(X_test)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÅ‡∏ö‡πà‡∏á 70-15-15 (Train-Val-Test)\n",
    "# Step 1: ‡πÅ‡∏ö‡πà‡∏á 70-30\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: ‡πÅ‡∏ö‡πà‡∏á 30 ‡πÄ‡∏õ‡πá‡∏ô 15-15\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"üì¶ 70-15-15 Split:\")\n",
    "print(f\"   Training: {len(X_train)} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Validation: {len(X_val)} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(X_test)} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualization\n",
    "sizes = [len(X_train), len(X_val), len(X_test)]\n",
    "labels = ['Training\\n70%', 'Validation\\n15%', 'Test\\n15%']\n",
    "colors = ['#66b3ff', '#99ff99', '#ffcc99']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%',\n",
    "        startangle=90, explode=explode, textprops={'fontsize': 12, 'weight': 'bold'})\n",
    "plt.title('Train-Validation-Test Split', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üñºÔ∏è Part 2: ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û (Images)\n",
    "\n",
    "### ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å CSV:\n",
    "- ‡∏†‡∏≤‡∏û‡∏°‡∏±‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô **‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå** ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° class\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ö‡πà‡∏á‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏Å‡∏©‡∏≤ **‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û (file paths)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á mock image paths\n",
    "n_cats = 300\n",
    "n_dogs = 200\n",
    "\n",
    "cat_images = [f'images/cats/cat_{i}.jpg' for i in range(n_cats)]\n",
    "dog_images = [f'images/dogs/dog_{i}.jpg' for i in range(n_dogs)]\n",
    "\n",
    "all_images = cat_images + dog_images\n",
    "all_labels = [0] * n_cats + [1] * n_dogs  # 0=cat, 1=dog\n",
    "\n",
    "print(\"üì∏ Image Dataset:\")\n",
    "print(f\"   Total images: {len(all_images)}\")\n",
    "print(f\"   Cats: {n_cats}\")\n",
    "print(f\"   Dogs: {n_dogs}\")\n",
    "print(f\"   Ratio: {n_cats}:{n_dogs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û - ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ stratify!\n",
    "train_imgs, test_imgs, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels,\n",
    "    test_size=0.2,\n",
    "    stratify=all_labels,  # üîë ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç! ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô class\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"üì¶ Image Split (with stratify):\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"   Total: {len(train_imgs)}\")\n",
    "print(f\"   Cats: {train_labels.count(0)} ({train_labels.count(0)/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"   Dogs: {train_labels.count(1)} ({train_labels.count(1)/len(train_labels)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"   Total: {len(test_imgs)}\")\n",
    "print(f\"   Cats: {test_labels.count(0)} ({test_labels.count(0)/len(test_labels)*100:.1f}%)\")\n",
    "print(f\"   Dogs: {test_labels.count(1)} ({test_labels.count(1)/len(test_labels)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 60:40 ‡∏ñ‡∏π‡∏Å‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÑ‡∏ß‡πâ‡∏ó‡∏±‡πâ‡∏á train ‡πÅ‡∏•‡∏∞ test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üö® Part 3: Data Leakage - ‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á!\n",
    "\n",
    "### Data Leakage ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å **test set ‡∏£‡∏±‡πà‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô training process**\n",
    "\n",
    "### ‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏≤‡∏°‡∏°‡∏≤:\n",
    "- Training accuracy ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å 99%+\n",
    "- Test accuracy ‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å 50-60%\n",
    "- ‡πÇ‡∏°‡πÄ‡∏î‡∏•**‡πÇ‡∏Å‡∏á** ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ï‡∏±‡∏ß!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Data Leakage ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "print(\"üö® Common Data Leakage Scenarios:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Scaling ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á (‡∏ú‡∏¥‡∏î!)\")\n",
    "print(\"\"\"\n",
    "‚ùå WRONG:\n",
    "   scaler.fit(all_data)          # ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á train + test\n",
    "   scaled_data = scaler.transform(all_data)\n",
    "   train, test = split(scaled_data)\n",
    "\n",
    "   ‚Üí Test data ‡∏£‡∏±‡πà‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤ training (‡∏ú‡πà‡∏≤‡∏ô mean, std)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ CORRECT:\n",
    "   train, test = split(data)     # ‡πÅ‡∏ö‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô!\n",
    "   scaler.fit(train)              # Fit ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ train\n",
    "   train = scaler.transform(train)\n",
    "   test = scaler.transform(test)  # ‡πÉ‡∏ä‡πâ scaler ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Feature Selection ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á (‡∏ú‡∏¥‡∏î!)\")\n",
    "print(\"\"\"\n",
    "‚ùå WRONG:\n",
    "   best_features = select_features(all_data)  # ‡πÄ‡∏´‡πá‡∏ô test!\n",
    "   train, test = split(data[best_features])\n",
    "\n",
    "‚úÖ CORRECT:\n",
    "   train, test = split(data)\n",
    "   best_features = select_features(train)  # ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏Ñ‡πà train\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Duplicate Data (‡∏ú‡∏¥‡∏î!)\")\n",
    "print(\"\"\"\n",
    "‚ùå WRONG:\n",
    "   data ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥\n",
    "   ‚Üí ‡πÅ‡∏ö‡πà‡∏á train/test ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°\n",
    "   ‚Üí ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏±‡πâ‡∏á train ‡πÅ‡∏•‡∏∞ test!\n",
    "\n",
    "‚úÖ CORRECT:\n",
    "   ‡∏•‡∏ö duplicates ‡∏Å‡πà‡∏≠‡∏ô\n",
    "   ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÅ‡∏ö‡πà‡∏á\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Data Leakage vs No Leakage\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "print(\"üî¨ Experiment: Data Leakage Impact\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ‚ùå ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ú‡∏¥‡∏î: Scale ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á\n",
    "print(\"\\n‚ùå WITH Data Leakage:\")\n",
    "scaler_wrong = StandardScaler()\n",
    "X_scaled_wrong = scaler_wrong.fit_transform(X)  # Fit ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î!\n",
    "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
    "    X_scaled_wrong, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_wrong = LogisticRegression(max_iter=1000)\n",
    "model_wrong.fit(X_train_wrong, y_train_wrong)\n",
    "\n",
    "train_acc_wrong = model_wrong.score(X_train_wrong, y_train_wrong)\n",
    "test_acc_wrong = model_wrong.score(X_test_wrong, y_test_wrong)\n",
    "\n",
    "print(f\"   Train Accuracy: {train_acc_wrong:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_wrong:.4f}\")\n",
    "print(f\"   Gap: {abs(train_acc_wrong - test_acc_wrong):.4f}\")\n",
    "\n",
    "# ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ñ‡∏π‡∏Å: ‡πÅ‡∏ö‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ Scale\n",
    "print(\"\\n‚úÖ WITHOUT Data Leakage (Correct):\")\n",
    "X_train_correct, X_test_correct, y_train_correct, y_test_correct = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_correct = StandardScaler()\n",
    "X_train_correct = scaler_correct.fit_transform(X_train_correct)  # Fit ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ train!\n",
    "X_test_correct = scaler_correct.transform(X_test_correct)        # Transform test\n",
    "\n",
    "model_correct = LogisticRegression(max_iter=1000)\n",
    "model_correct.fit(X_train_correct, y_train_correct)\n",
    "\n",
    "train_acc_correct = model_correct.score(X_train_correct, y_train_correct)\n",
    "test_acc_correct = model_correct.score(X_test_correct, y_test_correct)\n",
    "\n",
    "print(f\"   Train Accuracy: {train_acc_correct:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_correct:.4f}\")\n",
    "print(f\"   Gap: {abs(train_acc_correct - test_acc_correct):.4f}\")\n",
    "\n",
    "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Gap ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ = ‡πÇ‡∏°‡πÄ‡∏î‡∏• generalize ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öñÔ∏è Part 4: Class Imbalance - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•\n",
    "\n",
    "### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤:\n",
    "- Class ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞‡∏Å‡∏ß‡πà‡∏≤‡∏≠‡∏µ‡∏Å class ‡∏°‡∏≤‡∏Å\n",
    "- ‡πÄ‡∏ä‡πà‡∏ô: ‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á (1%) vs ‡∏õ‡∏Å‡∏ï‡∏¥ (99%)\n",
    "\n",
    "### ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö:\n",
    "- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≤‡∏¢ \"‡∏õ‡∏Å‡∏ï‡∏¥\" ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‚Üí Accuracy 99%!\n",
    "- ‡πÅ‡∏ï‡πà**‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå** (‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏±‡∏ö‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á‡πÄ‡∏•‡∏¢)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Imbalance\n",
    "np.random.seed(42)\n",
    "\n",
    "# 90% Class 0, 10% Class 1\n",
    "n_class0 = 900\n",
    "n_class1 = 100\n",
    "\n",
    "X_imb = np.vstack([\n",
    "    np.random.randn(n_class0, 2),\n",
    "    np.random.randn(n_class1, 2) + 2\n",
    "])\n",
    "\n",
    "y_imb = np.array([0] * n_class0 + [1] * n_class1)\n",
    "\n",
    "# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô\n",
    "class_counts = Counter(y_imb)\n",
    "\n",
    "print(\"‚öñÔ∏è Imbalanced Dataset:\")\n",
    "print(f\"   Total samples: {len(y_imb)}\")\n",
    "print(f\"   Class 0 (Majority): {class_counts[0]} ({class_counts[0]/len(y_imb)*100:.1f}%)\")\n",
    "print(f\"   Class 1 (Minority): {class_counts[1]} ({class_counts[1]/len(y_imb)*100:.1f}%)\")\n",
    "print(f\"   Imbalance Ratio: {class_counts[0]/class_counts[1]:.1f}:1\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "axes[0].bar(['Class 0\\n(Negative)', 'Class 1\\n(Positive)'], \n",
    "            [class_counts[0], class_counts[1]], \n",
    "            color=['skyblue', 'coral'])\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Class Distribution (Imbalanced)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(X_imb[y_imb==0, 0], X_imb[y_imb==0, 1], \n",
    "               alpha=0.5, s=50, label=f'Class 0 (n={class_counts[0]})', c='skyblue')\n",
    "axes[1].scatter(X_imb[y_imb==1, 0], X_imb[y_imb==1, 1], \n",
    "               alpha=0.7, s=50, label=f'Class 1 (n={class_counts[1]})', c='coral')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title('Data Visualization', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ Imbalance\n",
    "print(\"üõ†Ô∏è Solutions for Class Imbalance:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Stratified Split (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô):\")\n",
    "X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(\n",
    "    X_imb, y_imb,\n",
    "    test_size=0.2,\n",
    "    stratify=y_imb,  # üîë ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_counts = Counter(y_train_strat)\n",
    "test_counts = Counter(y_test_strat)\n",
    "\n",
    "print(f\"   Training set:\")\n",
    "print(f\"      Class 0: {train_counts[0]} ({train_counts[0]/len(y_train_strat)*100:.1f}%)\")\n",
    "print(f\"      Class 1: {train_counts[1]} ({train_counts[1]/len(y_train_strat)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test set:\")\n",
    "print(f\"      Class 0: {test_counts[0]} ({test_counts[0]/len(y_test_strat)*100:.1f}%)\")\n",
    "print(f\"      Class 1: {test_counts[1]} ({test_counts[1]/len(y_test_strat)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n   ‚úÖ ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 90:10 ‡∏ñ‡∏π‡∏Å‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡πâ‡∏á train ‡πÅ‡∏•‡∏∞ test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2Ô∏è‚É£ Class Weights (‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö minority class):\")\n",
    "print(\"\"\"\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, class_weight=dict(enumerate(class_weights)))\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ SMOTE (Synthetic Minority Over-sampling):\")\n",
    "print(\"\"\"\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• synthetic ‡πÉ‡∏´‡πâ minority class\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Undersampling (‡∏•‡∏î majority class):\")\n",
    "print(\"\"\"\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô majority class ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤ minority\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ Use Better Metrics (‡∏≠‡∏¢‡πà‡∏≤‡∏î‡∏π‡πÅ‡∏Ñ‡πà Accuracy!):\")\n",
    "print(\"\"\"\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_curve\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ metrics ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:\n",
    "- Precision: ‡∏ó‡∏≤‡∏¢‡∏ö‡∏ß‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡∏π‡∏Å‡∏Å‡∏µ‡πà%\n",
    "- Recall: ‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏ö‡∏ß‡∏Å‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏Å‡∏µ‡πà%\n",
    "- F1-Score: ‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏≤‡∏á Precision ‡πÅ‡∏•‡∏∞ Recall\n",
    "- ROC-AUC: ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å class\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Part 5: Cross-Validation\n",
    "\n",
    "### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á Single Split:\n",
    "- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏ß‡πà‡∏≤ **‡πÅ‡∏ö‡πà‡∏á‡∏¢‡∏±‡∏á‡πÑ‡∏á**\n",
    "- ‡∏≠‡∏≤‡∏à‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ/‡πÇ‡∏ä‡∏Ñ‡∏£‡πâ‡∏≤‡∏¢\n",
    "\n",
    "### ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ: K-Fold Cross-Validation\n",
    "- ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô K ‡∏™‡πà‡∏ß‡∏ô\n",
    "- ‡πÄ‡∏ó‡∏£‡∏ô K ‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô test)\n",
    "- ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"üîÑ 5-Fold Cross-Validation:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize folds\n",
    "fig, axes = plt.subplots(5, 1, figsize=(12, 8))\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X)):\n",
    "    train_mask = np.zeros(len(X))\n",
    "    test_mask = np.zeros(len(X))\n",
    "    train_mask[train_idx] = 1\n",
    "    test_mask[test_idx] = 1\n",
    "    \n",
    "    axes[fold].barh([0], [len(train_idx)], left=[0], height=0.5, color='skyblue', label='Train')\n",
    "    axes[fold].barh([0], [len(test_idx)], left=[len(train_idx)], height=0.5, color='coral', label='Test')\n",
    "    axes[fold].set_xlim([0, len(X)])\n",
    "    axes[fold].set_yticks([])\n",
    "    axes[fold].set_title(f'Fold {fold+1}: Train={len(train_idx)}, Test={len(test_idx)}', \n",
    "                         fontsize=11, fontweight='bold')\n",
    "    if fold == 0:\n",
    "        axes[fold].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ ‡πÅ‡∏ï‡πà‡∏•‡∏∞ fold ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô test\")\n",
    "print(\"   ‚Üí ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ single split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô\n",
    "\n",
    "### ‚úÖ Key Takeaways:\n",
    "\n",
    "#### 1. **Train/Test Split**\n",
    "- CSV: `train_test_split()`\n",
    "- Images: ‡πÉ‡∏ä‡πâ `stratify` ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô class\n",
    "- ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: 70-15-15 ‡∏´‡∏£‡∏∑‡∏≠ 80-20\n",
    "\n",
    "#### 2. **Data Leakage (‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢!)**\n",
    "- ‚ùå **‡∏≠‡∏¢‡πà‡∏≤** scale/transform ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á\n",
    "- ‚úÖ **‡∏ï‡πâ‡∏≠‡∏á** ‡πÅ‡∏ö‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ fit scaler ‡∏ö‡∏ô train ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "- ‡∏•‡∏ö duplicates ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á\n",
    "\n",
    "#### 3. **Class Imbalance**\n",
    "- ‡πÉ‡∏ä‡πâ `stratify=y` ‡∏ï‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á\n",
    "- Class weights\n",
    "- SMOTE / Undersampling\n",
    "- ‡πÉ‡∏ä‡πâ metrics ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (F1, Precision, Recall)\n",
    "\n",
    "#### 4. **Cross-Validation**\n",
    "- ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏Å‡∏ß‡πà‡∏≤ single split\n",
    "- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ 5-fold ‡∏´‡∏£‡∏∑‡∏≠ 10-fold\n",
    "- ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤\n",
    "\n",
    "### üöÄ Next Step:\n",
    "\n",
    "üëâ [Optimizers Comparison - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Optimizers](06_optimizers_comparison.ipynb)\n",
    "\n",
    "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ optimizer ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
