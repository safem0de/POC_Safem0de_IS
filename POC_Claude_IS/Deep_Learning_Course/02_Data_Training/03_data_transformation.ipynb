{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ ‡∏ö‡∏ó‡∏ó‡∏µ‡πà 2.3: Data Transformation - ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "\n",
    "## üéØ ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏ß‡∏¥‡πà‡∏á\n",
    "\n",
    "‡∏à‡∏¥‡∏ô‡∏ï‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡πà‡∏á 3 ‡∏Ñ‡∏ô:\n",
    "\n",
    "- üë® **‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 1:** ‡∏ß‡∏¥‡πà‡∏á‡πÑ‡∏î‡πâ **5 km/hr** (‡πÄ‡∏î‡∏¥‡∏ô‡πÄ‡∏•‡πà‡∏ô‡πÜ)\n",
    "- üë® **‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 2:** ‡∏ß‡∏¥‡πà‡∏á‡πÑ‡∏î‡πâ **15 km/hr** (‡∏ß‡∏¥‡πà‡∏á‡∏õ‡∏Å‡∏ï‡∏¥)\n",
    "- üèÉ **‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 3:** ‡∏ß‡∏¥‡πà‡∏á‡πÑ‡∏î‡πâ **50,000 steps/hr** (‡∏ô‡∏±‡∏ö‡∏Å‡πâ‡∏≤‡∏ß)\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:** ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 3 ‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ï‡πà‡∏≤‡∏á ‚Üí ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏Å!\n",
    "\n",
    "‡∏ñ‡πâ‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ML:\n",
    "- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 3 **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î** (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÉ‡∏´‡∏ç‡πà)\n",
    "- ‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏ó‡∏±‡πâ‡∏á 3 ‡∏Ñ‡∏ô‡∏ß‡∏¥‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß‡∏û‡∏≠‡πÜ ‡∏Å‡∏±‡∏ô!\n",
    "\n",
    "**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:** **Normalization** ‡∏´‡∏£‡∏∑‡∏≠ **Standardization**\n",
    "\n",
    "‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏¢! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé≠ Part 1: ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Transform?\n",
    "\n",
    "### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Employee Data\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    'Age': np.random.randint(22, 60, 200),\n",
    "    'Salary': np.random.randint(25000, 150000, 200),\n",
    "    'YearsExperience': np.random.randint(0, 30, 200),\n",
    "    'ProjectsCompleted': np.random.randint(1, 100, 200)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"üë• Employee Data (200 employees)\")\n",
    "print(\"=\"*60)\n",
    "print(df.head(10))\n",
    "print(\"\\nüìä Statistical Summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: Scale ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å!\n",
    "print(\"‚ö†Ô∏è PROBLEM: Different Scales!\\n\")\n",
    "print(\"Range of each feature:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in df.columns:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    range_val = max_val - min_val\n",
    "    print(f\"{col:20s}: [{min_val:6.0f}, {max_val:6.0f}]  Range: {range_val:6.0f}\")\n",
    "\n",
    "print(\"\\nüí° Salary ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ Age ‡∏°‡∏≤‡∏Å!\")\n",
    "print(\"   ‚Üí ML model ‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö Salary ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤\")\n",
    "print(\"   ‚Üí ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•! ‚ùå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: ‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏±‡∏î‡∏ß‡πà‡∏≤ scale ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    row = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    \n",
    "    axes[row, col_idx].hist(df[col], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[row, col_idx].set_title(f'{col} Distribution (Original)', fontsize=12, fontweight='bold')\n",
    "    axes[row, col_idx].set_xlabel(col)\n",
    "    axes[row, col_idx].set_ylabel('Frequency')\n",
    "    axes[row, col_idx].axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[col].mean():.0f}')\n",
    "    axes[row, col_idx].legend()\n",
    "    axes[row, col_idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üëÄ ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: ‡πÅ‡∏Å‡∏ô X ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìè Part 2: Min-Max Normalization (Scaling to 0-1)\n",
    "\n",
    "### ‡∏™‡∏π‡∏ï‡∏£:\n",
    "```\n",
    "X_normalized = (X - X_min) / (X_max - X_min)\n",
    "```\n",
    "\n",
    "### ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\n",
    "- ‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á **0 ‡∏ñ‡∏∂‡∏á 1**\n",
    "- ‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡∏™‡∏∏‡∏î ‚Üí 0\n",
    "- ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î ‚Üí 1\n",
    "\n",
    "### ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö:\n",
    "‚úÖ Neural Networks  \n",
    "‚úÖ Image data (pixel 0-255 ‚Üí 0-1)  \n",
    "‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ bounded range\n",
    "\n",
    "### ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö:\n",
    "‚ùå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ outliers ‡πÄ‡∏¢‡∏≠‡∏∞ (outliers ‡∏à‡∏∞‡∏î‡∏∂‡∏á scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: Manual calculation\n",
    "print(\"üîß Method 1: Manual Min-Max Normalization\\n\")\n",
    "\n",
    "age_normalized_manual = (df['Age'] - df['Age'].min()) / (df['Age'].max() - df['Age'].min())\n",
    "\n",
    "print(f\"Age (Original):\")\n",
    "print(f\"   Min: {df['Age'].min()}\")\n",
    "print(f\"   Max: {df['Age'].max()}\")\n",
    "print(f\"   Sample values: {df['Age'].head().values}\")\n",
    "\n",
    "print(f\"\\nAge (Normalized):\")\n",
    "print(f\"   Min: {age_normalized_manual.min():.4f}\")\n",
    "print(f\"   Max: {age_normalized_manual.max():.4f}\")\n",
    "print(f\"   Sample values: {age_normalized_manual.head().values}\")\n",
    "\n",
    "print(\"\\n‚úÖ All values are now between 0 and 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: Using scikit-learn (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)\n",
    "print(\"üõ†Ô∏è Method 2: Using MinMaxScaler (Recommended)\\n\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(df),\n",
    "    columns=df.columns\n",
    ")\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nNormalized Data (0-1):\")\n",
    "print(df_normalized.head())\n",
    "\n",
    "print(\"\\nüìä Normalized Statistics:\")\n",
    "print(df_normalized.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Before vs After\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    # Before (top row)\n",
    "    axes[0, idx].hist(df[col], bins=30, color='red', alpha=0.6, edgecolor='black')\n",
    "    axes[0, idx].set_title(f'{col}\\nBEFORE Normalization', fontsize=11, fontweight='bold')\n",
    "    axes[0, idx].set_xlabel('Value')\n",
    "    axes[0, idx].set_ylabel('Frequency')\n",
    "    axes[0, idx].grid(alpha=0.3)\n",
    "    \n",
    "    # After (bottom row)\n",
    "    axes[1, idx].hist(df_normalized[col], bins=30, color='green', alpha=0.6, edgecolor='black')\n",
    "    axes[1, idx].set_title(f'{col}\\nAFTER Normalization', fontsize=11, fontweight='bold')\n",
    "    axes[1, idx].set_xlabel('Value (0-1)')\n",
    "    axes[1, idx].set_ylabel('Frequency')\n",
    "    axes[1, idx].set_xlim([-0.1, 1.1])\n",
    "    axes[1, idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ú® ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ó‡∏∏‡∏Å feature ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô scale ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (0-1)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 3: Standardization (Z-Score Normalization)\n",
    "\n",
    "### ‡∏™‡∏π‡∏ï‡∏£:\n",
    "```\n",
    "X_standardized = (X - mean) / std\n",
    "```\n",
    "\n",
    "### ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\n",
    "- Mean = **0**\n",
    "- Standard Deviation = **1**\n",
    "- ‡∏Ñ‡πà‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á **-3 ‡∏ñ‡∏∂‡∏á +3**\n",
    "\n",
    "### ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö:\n",
    "‚úÖ Linear Regression  \n",
    "‚úÖ SVM  \n",
    "‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ Gaussian distribution  \n",
    "‚úÖ ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ distance (KNN, K-Means)\n",
    "\n",
    "### ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:\n",
    "‚úÖ ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å bounded (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÅ‡∏Ñ‡πà 0-1)  \n",
    "‚úÖ Preserve outliers information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: Manual calculation\n",
    "print(\"üîß Method 1: Manual Standardization\\n\")\n",
    "\n",
    "age_standardized_manual = (df['Age'] - df['Age'].mean()) / df['Age'].std()\n",
    "\n",
    "print(f\"Age (Original):\")\n",
    "print(f\"   Mean: {df['Age'].mean():.2f}\")\n",
    "print(f\"   Std: {df['Age'].std():.2f}\")\n",
    "print(f\"   Sample: {df['Age'].head().values}\")\n",
    "\n",
    "print(f\"\\nAge (Standardized):\")\n",
    "print(f\"   Mean: {age_standardized_manual.mean():.10f} (‚âà 0)\")\n",
    "print(f\"   Std: {age_standardized_manual.std():.10f} (‚âà 1)\")\n",
    "print(f\"   Sample: {age_standardized_manual.head().values}\")\n",
    "\n",
    "print(\"\\n‚úÖ Mean ‚âà 0, Std ‚âà 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: Using StandardScaler (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)\n",
    "print(\"üõ†Ô∏è Method 2: Using StandardScaler\\n\")\n",
    "\n",
    "scaler_std = StandardScaler()\n",
    "df_standardized = pd.DataFrame(\n",
    "    scaler_std.fit_transform(df),\n",
    "    columns=df.columns\n",
    ")\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nStandardized Data (mean=0, std=1):\")\n",
    "print(df_standardized.head())\n",
    "\n",
    "print(\"\\nüìä Standardized Statistics:\")\n",
    "print(df_standardized.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Before vs After\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    # Before (top row)\n",
    "    axes[0, idx].hist(df[col], bins=30, color='orange', alpha=0.6, edgecolor='black')\n",
    "    axes[0, idx].set_title(f'{col}\\nBEFORE Standardization', fontsize=11, fontweight='bold')\n",
    "    axes[0, idx].axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[col].mean():.0f}')\n",
    "    axes[0, idx].set_xlabel('Value')\n",
    "    axes[0, idx].set_ylabel('Frequency')\n",
    "    axes[0, idx].legend()\n",
    "    axes[0, idx].grid(alpha=0.3)\n",
    "    \n",
    "    # After (bottom row)\n",
    "    axes[1, idx].hist(df_standardized[col], bins=30, color='blue', alpha=0.6, edgecolor='black')\n",
    "    axes[1, idx].set_title(f'{col}\\nAFTER Standardization', fontsize=11, fontweight='bold')\n",
    "    axes[1, idx].axvline(0, color='red', linestyle='--', linewidth=2, label='Mean: 0')\n",
    "    axes[1, idx].set_xlabel('Z-Score')\n",
    "    axes[1, idx].set_ylabel('Frequency')\n",
    "    axes[1, idx].legend()\n",
    "    axes[1, idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ú® ‡∏ó‡∏∏‡∏Å feature ‡∏°‡∏µ mean=0 ‡πÅ‡∏•‡∏∞ std=1 ‡πÅ‡∏•‡πâ‡∏ß!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 4: Min-Max vs Standardization - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö side-by-side\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original_Age': df['Age'].head(10).values,\n",
    "    'Normalized_Age': df_normalized['Age'].head(10).values,\n",
    "    'Standardized_Age': df_standardized['Age'].head(10).values,\n",
    "})\n",
    "\n",
    "print(\"üìä Comparison: Original vs Normalized vs Standardized\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"   - Normalized: ‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0-1\")\n",
    "print(\"   - Standardized: ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏ß‡∏Å‡πÑ‡∏î‡πâ, ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á -3 ‡∏ñ‡∏∂‡∏á +3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏±‡πâ‡∏á 3 ‡πÅ‡∏ö‡∏ö\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].hist(df['Salary'], bins=30, color='gray', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Original Salary\\n(25,000 - 150,000)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Salary ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Normalized\n",
    "axes[1].hist(df_normalized['Salary'], bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Min-Max Normalized\\n(0 - 1)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Normalized Salary')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_xlim([-0.1, 1.1])\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Standardized\n",
    "axes[2].hist(df_standardized['Salary'], bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title('Standardized\\n(mean=0, std=1)', fontsize=13, fontweight='bold')\n",
    "axes[2].set_xlabel('Z-Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].axvline(0, color='red', linestyle='--', linewidth=2, label='Mean=0')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüëÄ ‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÑ‡∏´‡∏°?\")\n",
    "print(\"   - Shape ‡∏Ç‡∏≠‡∏á distribution ‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\")\n",
    "print(\"   - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏Ñ‡πà scale (‡πÅ‡∏Å‡∏ô X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ°Ô∏è Part 5: Robust Scaler (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ Outliers)\n",
    "\n",
    "### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á Standardization:\n",
    "- ‡πÉ‡∏ä‡πâ **mean** ‡πÅ‡∏•‡∏∞ **std**\n",
    "- Outliers ‡∏ó‡∏≥‡πÉ‡∏´‡πâ mean ‡πÅ‡∏•‡∏∞ std ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô!\n",
    "\n",
    "### ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ: Robust Scaler\n",
    "- ‡πÉ‡∏ä‡πâ **median** ‡πÅ‡∏•‡∏∞ **IQR** ‡πÅ‡∏ó‡∏ô\n",
    "- ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏à‡∏≤‡∏Å outliers\n",
    "\n",
    "### ‡∏™‡∏π‡∏ï‡∏£:\n",
    "```\n",
    "X_robust = (X - median) / IQR\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ outliers\n",
    "np.random.seed(42)\n",
    "\n",
    "data_with_outliers = {\n",
    "    'Score': np.concatenate([\n",
    "        np.random.normal(50, 10, 180),  # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏Å‡∏ï‡∏¥\n",
    "        np.array([200, 250, 300, -50, -100] * 4)  # Outliers!\n",
    "    ])\n",
    "}\n",
    "\n",
    "df_outliers = pd.DataFrame(data_with_outliers)\n",
    "\n",
    "print(\"üìä Data with Outliers:\")\n",
    "print(f\"   Mean: {df_outliers['Score'].mean():.2f}\")\n",
    "print(f\"   Median: {df_outliers['Score'].median():.2f}\")\n",
    "print(f\"   Std: {df_outliers['Score'].std():.2f}\")\n",
    "print(f\"   Min: {df_outliers['Score'].min():.2f}\")\n",
    "print(f\"   Max: {df_outliers['Score'].max():.2f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_outliers['Score'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "plt.title('Score Distribution (With Outliers)', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(df_outliers['Score'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df_outliers[\"Score\"].mean():.1f}')\n",
    "plt.axvline(df_outliers['Score'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df_outliers[\"Score\"].median():.1f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df_outliers['Score'])\n",
    "plt.title('Box Plot (Outliers Visible)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Score')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö 3 ‡∏ß‡∏¥‡∏ò‡∏µ ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ outliers\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_robust = RobustScaler()\n",
    "\n",
    "score_normalized = scaler_minmax.fit_transform(df_outliers)\n",
    "score_standardized = scaler_standard.fit_transform(df_outliers)\n",
    "score_robust = scaler_robust.fit_transform(df_outliers)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].hist(df_outliers['Score'], bins=50, color='gray', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Original\\n(With Outliers)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Min-Max\n",
    "axes[1].hist(score_normalized, bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Min-Max Normalized\\n‚ùå Affected by outliers', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Normalized Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Standard\n",
    "axes[2].hist(score_standardized, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title('Standardized\\n‚ùå Affected by outliers', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Z-Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "# Robust\n",
    "axes[3].hist(score_robust, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[3].set_title('Robust Scaler\\n‚úÖ NOT affected by outliers!', fontsize=12, fontweight='bold')\n",
    "axes[3].set_xlabel('Robust Score')\n",
    "axes[3].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Robust Scaler is BEST for data with outliers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Part 6: ‡∏™‡∏£‡∏∏‡∏õ - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡πÑ‡∏´‡∏ô‡∏î‡∏µ?\n",
    "\n",
    "### Decision Tree: üå≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ\n",
    "summary = pd.DataFrame({\n",
    "    'Method': ['Min-Max Normalization', 'Standardization', 'Robust Scaler'],\n",
    "    'Formula': [\n",
    "        '(X - min) / (max - min)',\n",
    "        '(X - mean) / std',\n",
    "        '(X - median) / IQR'\n",
    "    ],\n",
    "    'Output Range': [\n",
    "        '0 to 1',\n",
    "        '-‚àû to +‚àû (mostly -3 to +3)',\n",
    "        '-‚àû to +‚àû'\n",
    "    ],\n",
    "    'Best For': [\n",
    "        'Neural Networks, Images, Bounded data',\n",
    "        'Linear models, SVM, Gaussian data',\n",
    "        'Data with many outliers'\n",
    "    ],\n",
    "    'Sensitive to Outliers': [\n",
    "        '‚ùå Yes (Very)',\n",
    "        '‚ùå Yes (Moderately)',\n",
    "        '‚úÖ No (Robust!)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"üìä Scaling Methods Comparison\")\n",
    "print(\"=\"*100)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n",
    "print(\"\\nüéØ How to Choose? Decision Guide:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ NEURAL NETWORKS ‡∏´‡∏£‡∏∑‡∏≠ DEEP LEARNING:\")\n",
    "print(\"   ‚Üí ‡πÉ‡∏ä‡πâ Min-Max Normalization (0-1) ‚úÖ\")\n",
    "print(\"   ‚Üí ‡∏´‡∏£‡∏∑‡∏≠ Standardization ‡∏Å‡πá‡πÑ‡∏î‡πâ\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ OUTLIERS ‡πÄ‡∏¢‡∏≠‡∏∞:\")\n",
    "print(\"   ‚Üí ‡πÉ‡∏ä‡πâ Robust Scaler ‚úÖ\")\n",
    "print(\"   ‚Üí ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏ö outliers ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏≠‡∏∑‡πà‡∏ô\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ LINEAR MODELS (Linear Regression, SVM):\")\n",
    "print(\"   ‚Üí ‡πÉ‡∏ä‡πâ Standardization ‚úÖ\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô IMAGE (pixels 0-255):\")\n",
    "print(\"   ‚Üí ‡πÅ‡∏ö‡πà‡∏á 255.0 (‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö normalize to 0-1) ‚úÖ\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ TREE-BASED models (Random Forest, XGBoost):\")\n",
    "print(\"   ‚Üí ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á scale! Trees ‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à scale üå≤\")\n",
    "\n",
    "print(\"\\nüí° Golden Rule:\")\n",
    "print(\"   ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Part 7: Save & Load Scalers\n",
    "\n",
    "**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!** ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö scaler ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df)\n",
    "\n",
    "# Save\n",
    "joblib.dump(scaler, 'minmax_scaler.pkl')\n",
    "print(\"üíæ Scaler saved to 'minmax_scaler.pkl'\")\n",
    "\n",
    "# Load\n",
    "loaded_scaler = joblib.load('minmax_scaler.pkl')\n",
    "print(\"üìÇ Scaler loaded successfully!\")\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà\n",
    "new_data = np.array([[35, 75000, 8, 25]])  # ‡∏≠‡∏≤‡∏¢‡∏∏ 35, ‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô 75000, ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå 8 ‡∏õ‡∏µ, 25 ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå\n",
    "new_data_scaled = loaded_scaler.transform(new_data)\n",
    "\n",
    "print(\"\\n‚úÖ Transform new data:\")\n",
    "print(f\"   Original: {new_data[0]}\")\n",
    "print(f\"   Scaled: {new_data_scaled[0]}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Important: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ scaler ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà fit ‡∏ö‡∏ô training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô\n",
    "\n",
    "### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "1. **‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Transform?**\n",
    "   - Features ‡∏°‡∏µ scale ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô ‚Üí ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•\n",
    "   - Scaling ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å feature ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô\n",
    "\n",
    "2. **Min-Max Normalization**\n",
    "   - Scale to [0, 1]\n",
    "   - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Neural Networks\n",
    "   - ‡πÑ‡∏ß‡∏ï‡πà‡∏≠ outliers\n",
    "\n",
    "3. **Standardization**\n",
    "   - Mean = 0, Std = 1\n",
    "   - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Linear models\n",
    "   - Preserve outlier information\n",
    "\n",
    "4. **Robust Scaler**\n",
    "   - ‡πÉ‡∏ä‡πâ median ‡πÅ‡∏•‡∏∞ IQR\n",
    "   - ‡∏ó‡∏ô‡∏ï‡πà‡∏≠ outliers\n",
    "   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ outliers ‡πÄ‡∏¢‡∏≠‡∏∞\n",
    "\n",
    "5. **Best Practices**\n",
    "   - Fit scaler ‡∏ö‡∏ô training data ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "   - Transform ‡∏ó‡∏±‡πâ‡∏á train ‡πÅ‡∏•‡∏∞ test ‡∏î‡πâ‡∏ß‡∏¢ scaler ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
    "   - ‡πÄ‡∏Å‡πá‡∏ö scaler ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö production data\n",
    "\n",
    "### üöÄ Next Step:\n",
    "\n",
    "‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "üëâ [Data Augmentation - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û](04_data_augmentation.ipynb)\n",
    "\n",
    "‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deep Learning ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
