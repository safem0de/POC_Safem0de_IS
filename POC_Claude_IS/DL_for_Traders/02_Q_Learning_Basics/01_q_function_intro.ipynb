{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Q-Function: ‡∏à‡∏≤‡∏Å V(s) ‡∏™‡∏π‡πà Q(s,a)\n",
    "## ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á Q-Function\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á Value Function V(s)\n",
    "\n",
    "‡∏à‡∏≤‡∏Å Notebook ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ **Value Function: V(s)**\n",
    "\n",
    "```\n",
    "V(s) = ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á return ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà state s\n",
    "```\n",
    "\n",
    "### üéÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Grid World\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ V=9  ‚îÇ V=10 ‚îÇ Goal ‚îÇ\n",
    "‚îÇ  A   ‚îÇ  B   ‚îÇ V=10 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** ‡∏£‡∏π‡πâ‡πÅ‡∏Ñ‡πà V(s) ‡∏û‡∏≠‡πÑ‡∏´‡∏°?\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:**\n",
    "- ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ state A ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ 9\n",
    "- ‡πÅ‡∏ï‡πà**‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥ action ‡∏≠‡∏∞‡πÑ‡∏£!** ü§∑\n",
    "\n",
    "---\n",
    "\n",
    "## üí° ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î: ‡∏ñ‡πâ‡∏≤‡∏£‡∏π‡πâ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Action ‡∏•‡πà‡∏∞?\n",
    "\n",
    "### üéØ Q-Function: Q(s, a)\n",
    "\n",
    "**Q(s, a) = ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á return ‡πÄ‡∏°‡∏∑‡πà‡∏≠:**\n",
    "- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà state s\n",
    "- **‡∏ó‡∏≥ action a**\n",
    "- ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡πà‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ optimal\n",
    "\n",
    "---\n",
    "\n",
    "## üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö V(s) vs Q(s,a)\n",
    "\n",
    "### ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà state S:\n",
    "\n",
    "```\n",
    "         ‚Üë Up\n",
    "         ‚îÇ\n",
    "    ‚Üê‚îÄ‚îÄ‚îÄ‚îÄS‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Right\n",
    "         ‚îÇ\n",
    "         ‚Üì Down\n",
    "```\n",
    "\n",
    "### Value Function V(s):\n",
    "```\n",
    "V(S) = 5.0\n",
    "```\n",
    "**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:** ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà S ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡πà‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ optimal ‚Üí ‡πÑ‡∏î‡πâ return ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ 5\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:** ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏¥‡∏®‡πÑ‡∏´‡∏ô!\n",
    "\n",
    "### Q-Function Q(s,a):\n",
    "```\n",
    "Q(S, Up)    = 2.0\n",
    "Q(S, Down)  = 8.0  ‚Üê ‡∏î‡∏µ‡∏™‡∏∏‡∏î!\n",
    "Q(S, Left)  = 1.0\n",
    "Q(S, Right) = 5.0\n",
    "```\n",
    "**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:** ‡∏£‡∏π‡πâ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ action ‚Üí **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Down ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î!**\n",
    "\n",
    "---\n",
    "\n",
    "## üîë ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á V(s) ‡πÅ‡∏•‡∏∞ Q(s,a)\n",
    "\n",
    "### üìê ‡∏™‡∏π‡∏ï‡∏£:\n",
    "\n",
    "```\n",
    "V(s) = max_a Q(s, a)\n",
    "```\n",
    "\n",
    "**‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏Ñ‡∏ô:**\n",
    "- V(s) = ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á state s\n",
    "- = ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á action ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà state s\n",
    "- = max ‡∏Ç‡∏≠‡∏á Q(s, a) ‡∏ó‡∏∏‡∏Å action\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**\n",
    "```\n",
    "V(S) = max(2.0, 8.0, 1.0, 5.0) = 8.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡πÜ: ‡πÄ‡∏Å‡∏°‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡∏≠‡∏≠‡∏Å\n",
    "\n",
    "‡∏°‡∏µ 2 ‡∏ó‡∏≤‡∏á:\n",
    "\n",
    "```\n",
    "              ‚îå‚îÄ‚Üí Safe Path ‚Üí Goal (+10)\n",
    "              ‚îÇ    ‡∏£‡∏∞‡∏¢‡∏∞ 3 ‡∏Å‡πâ‡∏≤‡∏ß\n",
    "    Start ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "              ‚îÇ\n",
    "              ‚îî‚îÄ‚Üí Shortcut ‚Üí Goal (+10)\n",
    "                   ‡∏£‡∏∞‡∏¢‡∏∞ 1 ‡∏Å‡πâ‡∏≤‡∏ß ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏´‡∏•‡∏∏‡∏° 50%\n",
    "```\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥ Œ≥ = 0.9:\n",
    "\n",
    "### Path 1: Safe (3 ‡∏Å‡πâ‡∏≤‡∏ß)\n",
    "```\n",
    "Return = 0.9¬≥ √ó 10 = 7.29\n",
    "```\n",
    "\n",
    "### Path 2: Shortcut (1 ‡∏Å‡πâ‡∏≤‡∏ß ‡πÅ‡∏ï‡πà‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ 50%)\n",
    "```\n",
    "Expected Return = 0.5 √ó (0.9 √ó 10) + 0.5 √ó (-10) = 4.5 - 5 = -0.5\n",
    "```\n",
    "\n",
    "### Q-Values:\n",
    "```\n",
    "Q(Start, Safe)     = 7.29  ‚Üê ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤!\n",
    "Q(Start, Shortcut) = -0.5\n",
    "```\n",
    "\n",
    "**‡∏™‡∏£‡∏∏‡∏õ:** ‡πÅ‡∏°‡πâ Shortcut ‡∏à‡∏∞‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà Q-value ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Safe!\n",
    "\n",
    "---\n",
    "\n",
    "## üíπ Q-Function ‡πÉ‡∏ô Trading\n",
    "\n",
    "### State: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô = 100 ‡∏ö‡∏≤‡∏ó, ‡∏ñ‡∏∑‡∏≠ 0 ‡∏´‡∏∏‡πâ‡∏ô\n",
    "\n",
    "```\n",
    "Q(State, Buy)  = 50   ‚Üê ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£ 50 ‡∏ö‡∏≤‡∏ó\n",
    "Q(State, Hold) = 0    ‚Üê ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏∞‡πÑ‡∏£\n",
    "Q(State, Sell) = -10  ‚Üê ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏∏‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏Ç‡∏≤‡∏¢ ‚Üí ‡πÇ‡∏ó‡∏©\n",
    "```\n",
    "\n",
    "**Action ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:** Buy (‡∏°‡∏µ Q-value ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)\n",
    "\n",
    "### State: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô = 105 ‡∏ö‡∏≤‡∏ó, ‡∏ñ‡∏∑‡∏≠ 100 ‡∏´‡∏∏‡πâ‡∏ô (‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≤‡∏ó‡∏µ‡πà 100)\n",
    "\n",
    "```\n",
    "Q(State, Buy)  = 20   ‚Üê ‡πÄ‡∏û‡∏¥‡πà‡∏° position ‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°\n",
    "Q(State, Hold) = 80   ‚Üê ‡∏£‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πà‡∏≠\n",
    "Q(State, Sell) = 150  ‚Üê ‡∏Ç‡∏≤‡∏¢‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£ 500 ‡∏ö‡∏≤‡∏ó!\n",
    "```\n",
    "\n",
    "**Action ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:** Sell (‡∏°‡∏µ Q-value ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Bellman Equation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Q(s,a)\n",
    "\n",
    "### üìê ‡∏™‡∏π‡∏ï‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:\n",
    "\n",
    "```\n",
    "Q(s, a) = E[R + Œ≥ √ó max_a' Q(s', a')]\n",
    "```\n",
    "\n",
    "**‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏Ñ‡∏ô:**\n",
    "\n",
    "```\n",
    "Q(s, a) = ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ action a ‡∏ó‡∏µ‡πà state s\n",
    "        = Immediate Reward (R)\n",
    "        + Discounted Value ‡∏Ç‡∏≠‡∏á action ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà state ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n",
    "```\n",
    "\n",
    "### üîç ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö V(s):\n",
    "\n",
    "| | V(s) | Q(s, a) |\n",
    "|---|------|--------|\n",
    "| **Input** | State (s) | State (s) + Action (a) |\n",
    "| **Output** | Value ‡∏Ç‡∏≠‡∏á state | Value ‡∏Ç‡∏≠‡∏á state-action pair |\n",
    "| **Bellman** | V(s) = E[R + Œ≥√óV(s')] | Q(s,a) = E[R + Œ≥√ómax Q(s',a')] |\n",
    "| **‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à** | ‚ùå ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ | ‚úÖ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ó‡∏µ‡πà max Q |\n",
    "| **‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ Model** | ‚úÖ ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ P(s'\\|s,a) | ‚ùå ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ! (Model-free) |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ‡∏ó‡∏≥‡πÑ‡∏° Q-Function ‡∏ñ‡∏∂‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç?\n",
    "\n",
    "### ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ 3 ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Model-Free Learning**\n",
    "- V(s) ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ transition probability P(s'|s,a)\n",
    "- Q(s,a) ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ! ‡πÅ‡∏Ñ‡πà‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "\n",
    "```\n",
    "V(s) approach:\n",
    "\"‡∏ñ‡πâ‡∏≤‡πÄ‡∏î‡∏¥‡∏ô‡∏Ç‡∏ß‡∏≤ ‚Üí 80% ‡πÑ‡∏õ (1,2), 20% ‡πÑ‡∏õ (1,1)\"\n",
    "‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ probability!\n",
    "\n",
    "Q(s,a) approach:\n",
    "\"‡∏•‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏Ç‡∏ß‡∏≤‡∏î‡∏π ‚Üí ‡πÑ‡∏î‡πâ reward +5 ‡πÑ‡∏õ state (1,2)\"\n",
    "‚Üí ‡πÅ‡∏Ñ‡πà‡∏•‡∏≠‡∏á‡∏ó‡∏≥!\n",
    "```\n",
    "\n",
    "#### 2Ô∏è‚É£ **Direct Policy Extraction**\n",
    "- ‡∏°‡∏µ Q-table ‚Üí ‡∏´‡∏≤ policy ‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å!\n",
    "```\n",
    "œÄ(s) = argmax_a Q(s, a)\n",
    "```\n",
    "\n",
    "#### 3Ô∏è‚É£ **‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Real-World**\n",
    "- Trading: ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ probability ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á\n",
    "- Robotics: ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ dynamics ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô\n",
    "- Games: ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏Å‡∏•‡πÑ‡∏Å internal ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏°\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Q-Table: ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö Q-values\n",
    "\n",
    "### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Q-Table ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Grid World 3√ó3:\n",
    "\n",
    "| State | ‚Üë Up | ‚Üì Down | ‚Üê Left | ‚Üí Right |\n",
    "|-------|------|--------|--------|----------|\n",
    "| (0,0) | 0.0  | 2.5    | 0.0    | 3.2      |\n",
    "| (0,1) | 1.5  | 3.8    | 2.5    | 4.1      |\n",
    "| (0,2) | 2.0  | 5.0    | 3.8    | **8.5**  |\n",
    "| (1,0) | 2.5  | 1.0    | 0.0    | 5.2      |\n",
    "| (1,1) | **HOLE** | **HOLE** | **HOLE** | **HOLE** |\n",
    "| (1,2) | 3.8  | **9.0** | 5.0   | 7.5      |\n",
    "| (2,0) | 5.0  | 0.0    | 0.0    | 6.5      |\n",
    "| (2,1) | 8.0  | 1.5    | 6.5    | 8.8      |\n",
    "| (2,2) | **GOAL** | **GOAL** | **GOAL** | **GOAL** |\n",
    "\n",
    "**‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:**\n",
    "1. ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà (0,0) ‚Üí ‡∏î‡∏π Q-values ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Right (3.2)\n",
    "2. ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà (0,1) ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Right (4.1)\n",
    "3. ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà (0,2) ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Right (8.5)\n",
    "4. ‡∏ñ‡∏∂‡∏á Goal!\n",
    "\n",
    "---\n",
    "\n",
    "## üîú ‡∏ï‡πà‡∏≠‡πÑ‡∏õ: Q-Learning Algorithm\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏´‡∏≤ Q-values ‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á?\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:** ‡πÉ‡∏ä‡πâ **Q-Learning Algorithm!**\n",
    "\n",
    "‡πÉ‡∏ô‡∏ö‡∏ó‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "- Q-Learning Update Rule\n",
    "- Learning Rate (Œ±)\n",
    "- Exploration vs Exploitation (Œµ-greedy)\n",
    "- Implementation ‡∏à‡∏£‡∏¥‡∏á!\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î\n",
    "\n",
    "### Exercise 1: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì V(s) ‡∏à‡∏≤‡∏Å Q(s,a)\n",
    "```\n",
    "Q(s, a1) = 5.0\n",
    "Q(s, a2) = 8.0\n",
    "Q(s, a3) = 3.0\n",
    "Q(s, a4) = 7.0\n",
    "\n",
    "V(s) = ?\n",
    "Best action = ?\n",
    "```\n",
    "\n",
    "### Exercise 2: Trading Scenario\n",
    "```\n",
    "State: ‡∏£‡∏≤‡∏Ñ‡∏≤ = 100, ‡∏ñ‡∏∑‡∏≠ 50 ‡∏´‡∏∏‡πâ‡∏ô (‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≤‡∏ó‡∏µ‡πà 95)\n",
    "\n",
    "Q(state, Buy)  = 30\n",
    "Q(state, Hold) = 80\n",
    "Q(state, Sell) = 100\n",
    "\n",
    "‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£? ‡∏ó‡∏≥‡πÑ‡∏°?\n",
    "```\n",
    "\n",
    "### Exercise 3: Bellman Equation\n",
    "```\n",
    "State s, Action a\n",
    "‚Üí 70% ‡πÑ‡∏õ s1 (reward = +5)\n",
    "‚Üí 30% ‡πÑ‡∏õ s2 (reward = -2)\n",
    "\n",
    "Q(s1, best_action) = 10\n",
    "Q(s2, best_action) = 3\n",
    "Œ≥ = 0.9\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Q(s, a) = ?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ\n",
    "\n",
    "### üìå Key Takeaways:\n",
    "\n",
    "1. **Q(s,a) ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ V(s):**\n",
    "   - ‡∏ö‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ action\n",
    "   - ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ: argmax Q\n",
    "\n",
    "2. **Model-Free:**\n",
    "   - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ P(s'|s,a)\n",
    "   - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö real-world\n",
    "\n",
    "3. **Bellman Equation:**\n",
    "   ```\n",
    "   Q(s,a) = E[R + Œ≥ √ó max Q(s',a')]\n",
    "   ```\n",
    "\n",
    "4. **Q-Table:**\n",
    "   - ‡πÄ‡∏Å‡πá‡∏ö Q-value ‡∏ó‡∏∏‡∏Å (state, action) pair\n",
    "   - ‡πÉ‡∏ä‡πâ lookup ‡∏´‡∏≤ action ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Notebook:\n",
    "\n",
    "üëâ **[02_q_learning_algorithm.ipynb](02_q_learning_algorithm.ipynb)**\n",
    "\n",
    "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "- Q-Learning Update Rule\n",
    "- Temporal Difference (TD) Learning\n",
    "- Learning Rate ‡πÅ‡∏•‡∏∞ Hyperparameters\n",
    "- ‡∏ó‡∏î‡∏•‡∏≠‡∏á implement ‡∏à‡∏£‡∏¥‡∏á!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏°‡∏≤‡∏î‡∏π Q-values ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏Å‡∏±‡∏ô‡πÄ‡∏ñ‡∏≠‡∏∞!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üì¶ Libraries loaded!\")\n",
    "print(\"‚úÖ Ready to explore Q-Functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Visualization: Q-Table\n",
    "\n",
    "‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á Q-Table ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á Q-Table ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Simple Grid World\n",
    "\n",
    "Grid Layout (3x3):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ S   ‚îÇ     ‚îÇ     ‚îÇ  S = Start\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ     ‚îÇ  X  ‚îÇ     ‚îÇ  X = Hole\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ     ‚îÇ     ‚îÇ  G  ‚îÇ  G = Goal\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\"\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Q-Table (9 states √ó 4 actions)\n",
    "# States: (0,0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0), (2,1), (2,2)\n",
    "# Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "\n",
    "# Initialize Q-table\n",
    "n_states = 9\n",
    "n_actions = 4\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Q-values (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß)\n",
    "Q_example = np.array([\n",
    "    # Up,  Down, Left, Right\n",
    "    [0.0,  2.5,  0.0,  3.2],   # (0,0) - Start\n",
    "    [1.5,  3.8,  2.5,  4.1],   # (0,1)\n",
    "    [2.0,  5.0,  3.8,  5.5],   # (0,2)\n",
    "    [2.5,  1.0,  0.0,  5.2],   # (1,0)\n",
    "    [-10, -10, -10,  -10],     # (1,1) - Hole (‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤)\n",
    "    [3.8,  9.0,  5.0,  7.5],   # (1,2)\n",
    "    [5.0,  0.0,  0.0,  6.5],   # (2,0)\n",
    "    [8.0,  1.5,  6.5,  8.8],   # (2,1)\n",
    "    [0.0,  0.0,  0.0,  0.0],   # (2,2) - Goal (terminal)\n",
    "])\n",
    "\n",
    "print(\"üìä Q-Table Example:\\n\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'State':<12} {'‚Üë Up':<8} {'‚Üì Down':<8} {'‚Üê Left':<8} {'‚Üí Right':<8} {'Best Action'}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "action_names = ['‚Üë', '‚Üì', '‚Üê', '‚Üí']\n",
    "state_names = [\n",
    "    '(0,0) Start', '(0,1)', '(0,2)',\n",
    "    '(1,0)', '(1,1) Hole', '(1,2)',\n",
    "    '(2,0)', '(2,1)', '(2,2) Goal'\n",
    "]\n",
    "\n",
    "for i, state_name in enumerate(state_names):\n",
    "    q_values = Q_example[i]\n",
    "    \n",
    "    # ‡∏´‡∏≤ best action (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà hole/goal)\n",
    "    if 'Hole' not in state_name and 'Goal' not in state_name:\n",
    "        best_action_idx = np.argmax(q_values)\n",
    "        best_action = action_names[best_action_idx]\n",
    "    else:\n",
    "        best_action = '-'\n",
    "    \n",
    "    print(f\"{state_name:<12} {q_values[0]:<8.1f} {q_values[1]:<8.1f} {q_values[2]:<8.1f} {q_values[3]:<8.1f} {best_action}\")\n",
    "\n",
    "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"  - Q-values ‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏Å‡∏•‡πâ Goal ‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á\")\n",
    "print(\"  - Best action ‡∏Ñ‡∏∑‡∏≠ action ‡∏ó‡∏µ‡πà‡∏°‡∏µ Q-value ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\")\n",
    "print(\"  - Hole ‡∏°‡∏µ Q-values ‡∏ï‡∏¥‡∏î‡∏•‡∏ö‡∏ó‡∏∏‡∏Å action (penalty!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualize Q-Table ‡πÅ‡∏ö‡∏ö Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q-Table as heatmap\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "action_titles = ['‚Üë Up', '‚Üì Down', '‚Üê Left', '‚Üí Right']\n",
    "\n",
    "for idx, (ax, title) in enumerate(zip(axes.flat, action_titles)):\n",
    "    # Reshape Q-values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö action ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô 3x3 grid\n",
    "    q_grid = Q_example[:, idx].reshape(3, 3)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(q_grid, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "                center=0, ax=ax, cbar_kws={'label': 'Q-value'},\n",
    "                vmin=-10, vmax=10, linewidths=2)\n",
    "    \n",
    "    ax.set_title(f'Q-values for Action: {title}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Column', fontsize=11)\n",
    "    ax.set_ylabel('Row', fontsize=11)\n",
    "    \n",
    "    # Mark special states\n",
    "    ax.text(0.5, 0.5, 'S', ha='center', va='center', fontsize=20, color='blue', fontweight='bold')\n",
    "    ax.text(1.5, 1.5, 'X', ha='center', va='center', fontsize=20, color='red', fontweight='bold')\n",
    "    ax.text(2.5, 2.5, 'G', ha='center', va='center', fontsize=20, color='gold', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Q-Table Visualization: Q-values for Each Action', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® Heatmap Analysis:\")\n",
    "print(\"  - ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß = Q-value ‡∏™‡∏π‡∏á (‡∏î‡∏µ)\")\n",
    "print(\"  - ‡∏™‡∏µ‡πÅ‡∏î‡∏á = Q-value ‡∏ï‡πà‡∏≥ (‡πÅ‡∏¢‡πà)\")\n",
    "print(\"  - Hole ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏•‡∏ö‡∏ó‡∏∏‡∏Å action\")\n",
    "print(\"  - ‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏Å‡∏•‡πâ Goal ‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏µ Q-value ‡∏™‡∏π‡∏á (‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ ‡πÅ‡∏™‡∏î‡∏á Optimal Policy ‡∏à‡∏≤‡∏Å Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract Optimal Policy ‡∏à‡∏≤‡∏Å Q-Table\n",
    "Policy œÄ(s) = argmax_a Q(s, a)\n",
    "\"\"\"\n",
    "\n",
    "# ‡∏´‡∏≤ optimal policy\n",
    "optimal_actions = np.argmax(Q_example, axis=1)\n",
    "policy_grid = optimal_actions.reshape(3, 3)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot grid\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        # ‡∏™‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á\n",
    "        if i == 0 and j == 0:\n",
    "            color = 'lightgreen'  # Start\n",
    "        elif i == 1 and j == 1:\n",
    "            color = 'red'  # Hole\n",
    "        elif i == 2 and j == 2:\n",
    "            color = 'gold'  # Goal\n",
    "        else:\n",
    "            color = 'white'\n",
    "        \n",
    "        rect = plt.Rectangle((j, 2-i), 1, 1, facecolor=color, edgecolor='black', linewidth=3)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á optimal action (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô hole ‡πÅ‡∏•‡∏∞ goal)\n",
    "        if not ((i == 1 and j == 1) or (i == 2 and j == 2)):\n",
    "            action_idx = policy_grid[i, j]\n",
    "            action_symbol = action_names[action_idx]\n",
    "            \n",
    "            # ‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏π‡∏Å‡∏®‡∏£\n",
    "            ax.text(j + 0.5, 2 - i + 0.5, action_symbol, \n",
    "                   ha='center', va='center', fontsize=40, fontweight='bold')\n",
    "        \n",
    "        # Label special states\n",
    "        if i == 0 and j == 0:\n",
    "            ax.text(j + 0.1, 2 - i + 0.9, 'START', ha='left', va='top', \n",
    "                   fontsize=10, fontweight='bold', color='darkgreen')\n",
    "        elif i == 1 and j == 1:\n",
    "            ax.text(j + 0.5, 2 - i + 0.5, 'HOLE', ha='center', va='center', \n",
    "                   fontsize=16, fontweight='bold', color='white')\n",
    "        elif i == 2 and j == 2:\n",
    "            ax.text(j + 0.5, 2 - i + 0.5, 'GOAL', ha='center', va='center', \n",
    "                   fontsize=16, fontweight='bold', color='darkred')\n",
    "\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 3)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "plt.title('Optimal Policy from Q-Table\\n(Follow the arrows!)', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüß≠ Optimal Policy:\")\n",
    "print(\"  - ‡∏•‡∏π‡∏Å‡∏®‡∏£‡πÅ‡∏™‡∏î‡∏á action ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ state\")\n",
    "print(\"  - ‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡∏•‡∏π‡∏Å‡∏®‡∏£ ‚Üí ‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á Goal ‡πÇ‡∏î‡∏¢‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á Hole\")\n",
    "print(\"  - Policy ‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å argmax Q(s, a)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à: Interactive Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quiz: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Q-Function\n",
    "\"\"\"\n",
    "\n",
    "print(\"üéØ Q-Function Quiz!\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Question 1\n",
    "print(\"\\n‚ùì Question 1:\")\n",
    "print(\"   Q(s, a1) = 5, Q(s, a2) = 8, Q(s, a3) = 3\")\n",
    "print(\"   V(s) = ?\")\n",
    "print(\"\\n   A) 5\")\n",
    "print(\"   B) 8  ‚úÖ Correct! V(s) = max Q(s,a)\")\n",
    "print(\"   C) 16\")\n",
    "print(\"   D) 5.33\")\n",
    "\n",
    "# Question 2\n",
    "print(\"\\n‚ùì Question 2:\")\n",
    "print(\"   ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Q-function ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö V-function ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\")\n",
    "print(\"\\n   A) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤\")\n",
    "print(\"   B) ‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤\")\n",
    "print(\"   C) Model-free ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ  ‚úÖ Correct!\")\n",
    "print(\"   D) ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏ß‡πà‡∏≤\")\n",
    "\n",
    "# Question 3\n",
    "print(\"\\n‚ùì Question 3:\")\n",
    "print(\"   Trading State: ‡∏£‡∏≤‡∏Ñ‡∏≤=100, ‡∏ñ‡∏∑‡∏≠ 50 ‡∏´‡∏∏‡πâ‡∏ô (‡∏ã‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà 90)\")\n",
    "print(\"   Q(s, Buy)=20, Q(s, Hold)=80, Q(s, Sell)=100\")\n",
    "print(\"   ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£?\")\n",
    "print(\"\\n   A) Buy\")\n",
    "print(\"   B) Hold\")\n",
    "print(\"   C) Sell  ‚úÖ Correct! Max Q-value\")\n",
    "print(\"   D) Random\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° ‡∏™‡∏£‡∏∏‡∏õ:\")\n",
    "print(\"   - V(s) = max_a Q(s,a)\")\n",
    "print(\"   - Q-function ‡πÉ‡∏´‡πâ Model-free learning\")\n",
    "print(\"   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ó‡∏µ‡πà max Q ‚Üí Optimal policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Ready for Q-Learning!\n",
    "\n",
    "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Q-Function ‡πÅ‡∏•‡πâ‡∏ß!\n",
    "\n",
    "### üéì ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "- ‚úÖ Q(s,a) vs V(s)\n",
    "- ‚úÖ Bellman Equation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Q\n",
    "- ‚úÖ Q-Table representation\n",
    "- ‚úÖ Policy extraction: œÄ(s) = argmax Q(s,a)\n",
    "- ‚úÖ Model-free learning\n",
    "\n",
    "### üîú Next Step:\n",
    "üëâ **[02_q_learning_algorithm.ipynb](02_q_learning_algorithm.ipynb)**\n",
    "\n",
    "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ **update Q-values** ‡∏î‡πâ‡∏ß‡∏¢ Q-Learning Algorithm!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
