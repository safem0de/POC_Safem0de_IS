{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Q-Learning Algorithm: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå\n",
    "## ‡∏à‡∏≤‡∏Å Theory ‡∏™‡∏π‡πà Implementation (‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á DQN)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏ô‡∏µ‡πâ\n",
    "\n",
    "‡∏à‡∏≤‡∏Å Notebook ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤ **Q(s,a) ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£**\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** ü§î\n",
    "> ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏´‡∏≤ Q-values ‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á?\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:** ‡πÉ‡∏ä‡πâ **Q-Learning Algorithm!**\n",
    "\n",
    "### üöÄ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "1. **Temporal Difference (TD) Learning** - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á\n",
    "2. **Q-Learning Update Rule** - ‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡πÄ‡∏®‡∏©\n",
    "3. **Learning Rate (Œ±)** - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß/‡∏ä‡πâ‡∏≤‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô\n",
    "4. **Off-Policy Learning** - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡πÉ‡∏î‡πÜ ‡∏Å‡πá‡πÑ‡∏î‡πâ\n",
    "5. **Implementation** - ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏£‡∏¥‡∏á!\n",
    "\n",
    "### üéì ‡∏ó‡∏≥‡πÑ‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DQN?\n",
    "\n",
    "```\n",
    "Q-Learning ‚Üí Deep Q-Network (DQN) ‚Üí Trading Bot\n",
    "    ‚Üì              ‚Üì                      ‚Üì\n",
    " Q-Table      Neural Net          Portfolio Management\n",
    " (discrete)   (continuous)        (real-time trading)\n",
    "```\n",
    "\n",
    "**Q-Learning = ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á DQN!**\n",
    "- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Q-Learning ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à DQN ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "- Update rule ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÅ‡∏Ñ‡πà‡πÉ‡∏ä‡πâ Neural Network ‡πÅ‡∏ó‡∏ô Table\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Temporal Difference (TD) Learning\n",
    "\n",
    "### üí≠ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô:\n",
    "\n",
    "**‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î (Error)**\n",
    "\n",
    "‡∏•‡∏≠‡∏á‡∏ô‡∏∂‡∏Å‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:\n",
    "1. ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏∑‡∏≠ 10 (‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå)\n",
    "2. ‡∏Ñ‡∏£‡∏π‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏Ñ‡∏∑‡∏≠ 15 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á)\n",
    "3. ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î -5 (TD Error)\n",
    "4. ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå\n",
    "\n",
    "### üìê TD Error:\n",
    "\n",
    "```\n",
    "TD Error = (Actual - Predicted)\n",
    "         = (Target - Q_current)\n",
    "```\n",
    "\n",
    "**‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó RL:**\n",
    "```\n",
    "TD Error (Œ¥) = R + Œ≥ √ó max Q(s',a') - Q(s,a)\n",
    "                ‚îî‚îÄ‚îÄ‚îÄ Target ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ Current ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üéØ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡πÜ:\n",
    "\n",
    "```\n",
    "Agent ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà state A, ‡∏ó‡∏≥ action \"Right\"\n",
    "‚Üí ‡πÑ‡∏î‡πâ reward = 5\n",
    "‚Üí ‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á state B\n",
    "\n",
    "Current:  Q(A, Right) = 10\n",
    "Target:   R + Œ≥ √ó max Q(B, all actions)\n",
    "        = 5 + 0.9 √ó 20 = 23\n",
    "\n",
    "TD Error = 23 - 10 = +13  ‚Üê ‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡πà‡∏≥‡πÑ‡∏õ!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Q-Learning Update Rule\n",
    "\n",
    "### üìê ‡∏™‡∏π‡∏ï‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:\n",
    "\n",
    "```python\n",
    "Q(s, a) ‚Üê Q(s, a) + Œ± √ó [R + Œ≥ √ó max Q(s', a') - Q(s, a)]\n",
    "          ‚îî‚îÄ old ‚îÄ‚îò   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TD Error ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îî‚îÄ Learning Rate\n",
    "```\n",
    "\n",
    "### üîç ‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö:\n",
    "\n",
    "| ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ | ‡∏Ñ‡πà‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ |\n",
    "|-----------|----------|----------|\n",
    "| **Q(s,a)** | Q-value ‡πÄ‡∏î‡∏¥‡∏° | - |\n",
    "| **Œ±** | Learning rate (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ) | 0.1 - 0.5 |\n",
    "| **R** | Immediate reward | ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö environment |\n",
    "| **Œ≥** | Discount factor | 0.9 - 0.99 |\n",
    "| **max Q(s',a')** | Q-value ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á state ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ | - |\n",
    "\n",
    "### üéÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô:\n",
    "\n",
    "```\n",
    "‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå:\n",
    "- ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà state (0,0), ‡∏ó‡∏≥ action \"Right\"\n",
    "- ‡πÑ‡∏î‡πâ reward = -1 (penalty ‡πÄ‡∏î‡∏¥‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡πâ‡∏≤‡∏ß)\n",
    "- ‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á state (0,1)\n",
    "- Œ± = 0.1, Œ≥ = 0.9\n",
    "\n",
    "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:\n",
    "1. Q_current = Q(0,0, Right) = 3.0\n",
    "\n",
    "2. ‡∏´‡∏≤ max Q ‡∏Ç‡∏≠‡∏á state ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ:\n",
    "   Q((0,1), Up)    = 2.0\n",
    "   Q((0,1), Down)  = 4.0  ‚Üê max\n",
    "   Q((0,1), Left)  = 1.0\n",
    "   Q((0,1), Right) = 5.0  ‚Üê max!\n",
    "   max Q(s', a') = 5.0\n",
    "\n",
    "3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Target:\n",
    "   Target = R + Œ≥ √ó max Q(s', a')\n",
    "          = -1 + 0.9 √ó 5.0\n",
    "          = -1 + 4.5\n",
    "          = 3.5\n",
    "\n",
    "4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì TD Error:\n",
    "   TD Error = Target - Q_current\n",
    "            = 3.5 - 3.0\n",
    "            = 0.5\n",
    "\n",
    "5. Update Q-value:\n",
    "   Q_new = Q_old + Œ± √ó TD Error\n",
    "         = 3.0 + 0.1 √ó 0.5\n",
    "         = 3.0 + 0.05\n",
    "         = 3.05\n",
    "\n",
    "‚úÖ Q((0,0), Right) = 3.0 ‚Üí 3.05\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéõÔ∏è Learning Rate (Œ±): ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "\n",
    "### üìä ‡∏ú‡∏•‡∏Ç‡∏≠‡∏á Œ± ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "```\n",
    "Q_new = Q_old + Œ± √ó TD_Error\n",
    "```\n",
    "\n",
    "### Œ± = 0.0 (‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏•‡∏¢)\n",
    "```\n",
    "Q_new = Q_old + 0 √ó TD_Error = Q_old\n",
    "‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á!\n",
    "```\n",
    "\n",
    "### Œ± = 1.0 (‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà 100%)\n",
    "```\n",
    "Q_new = Q_old + 1.0 √ó TD_Error = Target\n",
    "‚ö†Ô∏è ‡∏•‡∏∑‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏±‡∏ô‡∏ó‡∏µ! (unstable)\n",
    "```\n",
    "\n",
    "### Œ± = 0.1 (‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏ä‡πâ‡∏≤, ‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á)\n",
    "```\n",
    "Q_new = Q_old + 0.1 √ó TD_Error\n",
    "‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡πÑ‡∏õ (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö stochastic environment)\n",
    "```\n",
    "\n",
    "### Œ± = 0.5 (‡∏Å‡∏•‡∏≤‡∏á‡πÜ)\n",
    "```\n",
    "Q_new = Q_old + 0.5 √ó TD_Error\n",
    "‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö deterministic environment)\n",
    "```\n",
    "\n",
    "### üìà ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Œ± ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Trading:\n",
    "\n",
    "| ‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå | Œ± ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ | ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• |\n",
    "|-----------|-----------|--------|\n",
    "| **Backtest (historical data)** | 0.3 - 0.5 | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏á‡∏ó‡∏µ‡πà, ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß |\n",
    "| **Live Trading (real-time)** | 0.05 - 0.1 | ‡∏ï‡∏•‡∏≤‡∏î‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô, ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤‡πÜ |\n",
    "| **High volatility markets** | 0.01 - 0.05 | ‡∏•‡∏î noise, stable |\n",
    "| **Low volatility markets** | 0.1 - 0.3 | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏î‡πâ |\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Off-Policy Learning: ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á Q-Learning\n",
    "\n",
    "### ü§î On-Policy vs Off-Policy?\n",
    "\n",
    "#### **On-Policy (‡πÄ‡∏ä‡πà‡∏ô SARSA):**\n",
    "```\n",
    "\"‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å policy ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà\"\n",
    "\n",
    "Agent ‡∏ó‡∏≥ action a ‡∏ó‡∏µ‡πà state s (‡∏ï‡∏≤‡∏° policy Œµ-greedy)\n",
    "‚Üí Update Q ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ action ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á (‡∏ï‡∏≤‡∏° policy)\n",
    "```\n",
    "\n",
    "#### **Off-Policy (Q-Learning):**\n",
    "```\n",
    "\"‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å policy ‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ ‚Üí ‡∏°‡∏∏‡πà‡∏á‡∏™‡∏π‡πà optimal policy\"\n",
    "\n",
    "Agent ‡∏ó‡∏≥ action a ‡πÉ‡∏î‡πÜ ‡∏ó‡∏µ‡πà state s (random/explore/exploit)\n",
    "‚Üí Update Q ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ max Q(s',a') (optimal action)\n",
    "```\n",
    "\n",
    "### üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:\n",
    "\n",
    "```\n",
    "Scenario: ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà state S, ‡∏°‡∏µ 2 actions:\n",
    "- Action A (safe): reward = 1, ‡πÑ‡∏õ state S1\n",
    "- Action B (risky): reward = 10 ‡∏ñ‡πâ‡∏≤‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ, -10 ‡∏ñ‡πâ‡∏≤‡πÇ‡∏ä‡∏Ñ‡∏£‡πâ‡∏≤‡∏¢\n",
    "\n",
    "Agent explore ‡πÇ‡∏î‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å B (random)\n",
    "‚Üí ‡πÇ‡∏ä‡∏Ñ‡∏£‡πâ‡∏≤‡∏¢! ‡πÑ‡∏î‡πâ reward = -10\n",
    "\n",
    "üî¥ On-Policy (SARSA):\n",
    "Update Q(S,B) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ action ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ (‡∏≠‡∏≤‡∏à random ‡∏≠‡∏µ‡∏Å)\n",
    "‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤, ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á\n",
    "\n",
    "üü¢ Off-Policy (Q-Learning):\n",
    "Update Q(S,B) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ max Q (optimal action)\n",
    "‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß, ‡∏°‡∏∏‡πà‡∏á‡∏™‡∏π‡πà optimal ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ!\n",
    "```\n",
    "\n",
    "### üéØ ‡∏ó‡∏≥‡πÑ‡∏° Off-Policy ‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Trading?\n",
    "\n",
    "1. **Exploration ‡∏°‡∏≤‡∏Å‡πÑ‡∏î‡πâ:** ‡∏•‡∏≠‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "2. **Reuse old data:** ‡πÉ‡∏ä‡πâ historical trades ‡πÑ‡∏î‡πâ (Experience Replay)\n",
    "3. **Safe exploration:** ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö bad policy\n",
    "\n",
    "**‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á DQN Experience Replay!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Q-Learning Algorithm (Pseudocode)\n",
    "\n",
    "```python\n",
    "# Initialization\n",
    "Initialize Q(s,a) = 0 for all s, a\n",
    "Set hyperparameters: Œ±, Œ≥, Œµ\n",
    "\n",
    "# Main Loop\n",
    "for episode in range(num_episodes):\n",
    "    s = env.reset()  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô episode\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action (Œµ-greedy)\n",
    "        if random() < Œµ:\n",
    "            a = random_action()      # Explore\n",
    "        else:\n",
    "            a = argmax_a Q(s, a)     # Exploit\n",
    "        \n",
    "        # 2. ‡∏ó‡∏≥ action ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï result\n",
    "        s', r, done = env.step(a)\n",
    "        \n",
    "        # 3. Update Q-value (Q-Learning Update Rule)\n",
    "        Q(s,a) ‚Üê Q(s,a) + Œ± √ó [r + Œ≥ √ó max_a' Q(s',a') - Q(s,a)]\n",
    "        \n",
    "        # 4. ‡πÄ‡∏î‡∏¥‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ\n",
    "        s = s'\n",
    "    \n",
    "    # (Optional) Decay Œµ\n",
    "    Œµ = Œµ √ó decay_rate\n",
    "```\n",
    "\n",
    "### üîë Key Steps:\n",
    "\n",
    "1. **Initialize Q-table:** ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 0 ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏™‡∏∏‡πà‡∏°\n",
    "2. **Œµ-greedy action selection:** ‡∏™‡∏°‡∏î‡∏∏‡∏• explore/exploit\n",
    "3. **Execute action:** ‡∏ó‡∏≥‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏à‡∏£‡∏¥‡∏á\n",
    "4. **Update Q:** ‡πÉ‡∏ä‡πâ Q-Learning rule\n",
    "5. **Repeat:** ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ converge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"üì¶ Libraries loaded!\")\n",
    "print(\"‚úÖ Ready to implement Q-Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Implementation 1: Q-Learning Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q-Learning Update Function\n",
    "\n",
    "Core function ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô DQN ‡∏î‡πâ‡∏ß‡∏¢!\n",
    "\"\"\"\n",
    "\n",
    "def q_learning_update(Q, state, action, reward, next_state, alpha, gamma, n_actions):\n",
    "    \"\"\"\n",
    "    Q-Learning Update Rule\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q : dict or array\n",
    "        Q-table {(state, action): value}\n",
    "    state : tuple\n",
    "        Current state\n",
    "    action : int\n",
    "        Action taken\n",
    "    reward : float\n",
    "        Reward received\n",
    "    next_state : tuple\n",
    "        Next state\n",
    "    alpha : float\n",
    "        Learning rate\n",
    "    gamma : float\n",
    "        Discount factor\n",
    "    n_actions : int\n",
    "        Number of possible actions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    td_error : float\n",
    "        TD Error (for monitoring)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get current Q-value\n",
    "    q_current = Q.get((state, action), 0.0)\n",
    "    \n",
    "    # 2. Find max Q-value of next state\n",
    "    max_q_next = max([Q.get((next_state, a), 0.0) for a in range(n_actions)])\n",
    "    \n",
    "    # 3. Calculate target\n",
    "    target = reward + gamma * max_q_next\n",
    "    \n",
    "    # 4. Calculate TD error\n",
    "    td_error = target - q_current\n",
    "    \n",
    "    # 5. Update Q-value\n",
    "    Q[(state, action)] = q_current + alpha * td_error\n",
    "    \n",
    "    return td_error\n",
    "\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö function\n",
    "print(\"üß™ Testing Q-Learning Update Function\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Q-table\n",
    "Q = {}\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ initial Q-values\n",
    "state = (0, 0)\n",
    "next_state = (0, 1)\n",
    "Q[(state, 3)] = 3.0  # action 3 (Right) ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô 3.0\n",
    "Q[(next_state, 0)] = 2.0\n",
    "Q[(next_state, 1)] = 4.0\n",
    "Q[(next_state, 2)] = 1.0\n",
    "Q[(next_state, 3)] = 5.0  # max!\n",
    "\n",
    "print(f\"Before Update:\")\n",
    "print(f\"  Q({state}, Right) = {Q[(state, 3)]:.2f}\")\n",
    "print(f\"\\nNext State Q-values:\")\n",
    "print(f\"  Q({next_state}, Up)    = {Q[(next_state, 0)]:.2f}\")\n",
    "print(f\"  Q({next_state}, Down)  = {Q[(next_state, 1)]:.2f}\")\n",
    "print(f\"  Q({next_state}, Left)  = {Q[(next_state, 2)]:.2f}\")\n",
    "print(f\"  Q({next_state}, Right) = {Q[(next_state, 3)]:.2f} ‚Üê max!\")\n",
    "\n",
    "# ‡∏ó‡∏≥ update\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "reward = -1.0\n",
    "action = 3  # Right\n",
    "n_actions = 4\n",
    "\n",
    "td_error = q_learning_update(Q, state, action, reward, next_state, alpha, gamma, n_actions)\n",
    "\n",
    "print(f\"\\nUpdate Process:\")\n",
    "print(f\"  Reward (R) = {reward}\")\n",
    "print(f\"  max Q(s',a') = {max([Q[(next_state, a)] for a in range(n_actions)]):.2f}\")\n",
    "print(f\"  Target = R + Œ≥ √ó max Q(s',a') = {reward} + {gamma} √ó 5.0 = {reward + gamma * 5.0:.2f}\")\n",
    "print(f\"  TD Error = {td_error:.2f}\")\n",
    "print(f\"  New Q = Old Q + Œ± √ó TD Error = 3.0 + {alpha} √ó {td_error:.2f} = {Q[(state, action)]:.2f}\")\n",
    "\n",
    "print(f\"\\nAfter Update:\")\n",
    "print(f\"  Q({state}, Right) = {Q[(state, action)]:.2f}\")\n",
    "print(f\"\\n‚úÖ Q-value updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualization: Learning Rate Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ç‡∏≠‡∏á Learning Rate (Œ±) ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ Convergence\n",
    "\"\"\"\n",
    "\n",
    "def simulate_learning(alpha, num_updates=100):\n",
    "    \"\"\"\n",
    "    Simulate Q-value updates with different alpha\n",
    "    \"\"\"\n",
    "    Q_current = 0.0\n",
    "    true_value = 10.0  # ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á\n",
    "    history = [Q_current]\n",
    "    \n",
    "    for _ in range(num_updates):\n",
    "        # TD Error = True Value - Current Estimate\n",
    "        td_error = true_value - Q_current\n",
    "        \n",
    "        # Update\n",
    "        Q_current = Q_current + alpha * td_error\n",
    "        history.append(Q_current)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö alpha ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "alphas = [0.01, 0.1, 0.3, 0.5, 1.0]\n",
    "num_updates = 50\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for alpha in alphas:\n",
    "    history = simulate_learning(alpha, num_updates)\n",
    "    plt.plot(history, label=f'Œ± = {alpha}', linewidth=2, marker='o', markersize=3, alpha=0.7)\n",
    "\n",
    "plt.axhline(y=10.0, color='red', linestyle='--', linewidth=2, label='True Value', alpha=0.5)\n",
    "plt.xlabel('Number of Updates', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Q-value', fontsize=12, fontweight='bold')\n",
    "plt.title('Effect of Learning Rate (Œ±) on Convergence', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-1, 12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Analysis:\")\n",
    "print(\"  - Œ± = 0.01: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å, ‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á\")\n",
    "print(\"  - Œ± = 0.1:  ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏ä‡πâ‡∏≤, ‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Trading)\")\n",
    "print(\"  - Œ± = 0.3:  ‡∏Å‡∏•‡∏≤‡∏á‡πÜ, balance ‡∏î‡∏µ\")\n",
    "print(\"  - Œ± = 0.5:  ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß\")\n",
    "print(\"  - Œ± = 1.0:  ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ averaging (‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)\")\n",
    "print(\"\\nüí° ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Trading: ‡πÉ‡∏ä‡πâ Œ± = 0.05-0.1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Implementation 2: Simple Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q-Learning Agent Class\n",
    "\n",
    "Template ‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô DQN ‡∏î‡πâ‡∏ß‡∏¢!\n",
    "\"\"\"\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning Agent\n",
    "    \n",
    "    ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö DQN Agent ‡∏°‡∏≤‡∏Å!\n",
    "    ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏Ñ‡πà:\n",
    "    - Q-Learning ‡πÉ‡∏ä‡πâ Q-table (dict)\n",
    "    - DQN ‡πÉ‡∏ä‡πâ Neural Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Agent\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_actions : int\n",
    "            Number of possible actions\n",
    "        alpha : float\n",
    "            Learning rate (0-1)\n",
    "        gamma : float\n",
    "            Discount factor (0-1)\n",
    "        epsilon : float\n",
    "            Exploration rate for Œµ-greedy (0-1)\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Q-table: {(state, action): value}\n",
    "        self.Q = defaultdict(float)  # default = 0.0\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'td_errors': [],\n",
    "            'q_values': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Get Q-value for (state, action)\"\"\"\n",
    "        return self.Q[(state, action)]\n",
    "    \n",
    "    def get_max_q(self, state):\n",
    "        \"\"\"Get maximum Q-value for state\"\"\"\n",
    "        q_values = [self.get_q_value(state, a) for a in range(self.n_actions)]\n",
    "        return max(q_values)\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"Get action with highest Q-value (greedy)\"\"\"\n",
    "        q_values = [self.get_q_value(state, a) for a in range(self.n_actions)]\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose action using Œµ-greedy policy\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        action : int\n",
    "        \"\"\"\n",
    "        # Œµ-greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            # Exploit: best action\n",
    "            return self.get_best_action(state)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Q-Learning Update\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        state : tuple\n",
    "            Current state\n",
    "        action : int\n",
    "            Action taken\n",
    "        reward : float\n",
    "            Reward received\n",
    "        next_state : tuple\n",
    "            Next state\n",
    "        done : bool\n",
    "            Episode terminated?\n",
    "        \"\"\"\n",
    "        # Current Q-value\n",
    "        q_current = self.get_q_value(state, action)\n",
    "        \n",
    "        # Target Q-value\n",
    "        if done:\n",
    "            # Terminal state: no future reward\n",
    "            target = reward\n",
    "        else:\n",
    "            # Non-terminal: R + Œ≥ √ó max Q(s',a')\n",
    "            max_q_next = self.get_max_q(next_state)\n",
    "            target = reward + self.gamma * max_q_next\n",
    "        \n",
    "        # TD Error\n",
    "        td_error = target - q_current\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.Q[(state, action)] = q_current + self.alpha * td_error\n",
    "        \n",
    "        # Save statistics\n",
    "        self.stats['td_errors'].append(td_error)\n",
    "        self.stats['q_values'].append(q_current)\n",
    "        self.stats['rewards'].append(reward)\n",
    "    \n",
    "    def decay_epsilon(self, decay_rate=0.995, min_epsilon=0.01):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon * decay_rate, min_epsilon)\n",
    "\n",
    "\n",
    "# Test Agent\n",
    "print(\"ü§ñ Q-Learning Agent Created!\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "agent = QLearningAgent(n_actions=4, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "print(f\"Agent Configuration:\")\n",
    "print(f\"  Number of actions: {agent.n_actions}\")\n",
    "print(f\"  Learning rate (Œ±): {agent.alpha}\")\n",
    "print(f\"  Discount factor (Œ≥): {agent.gamma}\")\n",
    "print(f\"  Exploration rate (Œµ): {agent.epsilon}\")\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö action selection\n",
    "test_state = (0, 0)\n",
    "print(f\"\\nüéØ Test Action Selection at state {test_state}:\")\n",
    "\n",
    "actions_selected = []\n",
    "for _ in range(100):\n",
    "    action = agent.choose_action(test_state)\n",
    "    actions_selected.append(action)\n",
    "\n",
    "print(f\"\\n  Action distribution (100 trials):\")\n",
    "for a in range(agent.n_actions):\n",
    "    count = actions_selected.count(a)\n",
    "    print(f\"    Action {a}: {count}% ({'‚Üë' if a==0 else '‚Üì' if a==1 else '‚Üê' if a==2 else '‚Üí'})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Agent ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Mini Experiment: 2-State MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "‡∏ó‡∏î‡∏•‡∏≠‡∏á Q-Learning ‡∏Å‡∏±‡∏ö MDP ‡∏á‡πà‡∏≤‡∏¢‡πÜ\n",
    "\n",
    "MDP:\n",
    "  State A --[action: go]--> State B (reward = 0)\n",
    "  State B = Terminal (reward = 10)\n",
    "\"\"\"\n",
    "\n",
    "class TwoStateMDP:\n",
    "    \"\"\"Simple 2-state MDP for testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state = 'A'\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 'A'\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action = 0: go to B\n",
    "        \"\"\"\n",
    "        if self.state == 'A' and action == 0:\n",
    "            self.state = 'B'\n",
    "            reward = 0.0\n",
    "            done = False\n",
    "        elif self.state == 'B':\n",
    "            reward = 10.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1.0\n",
    "            done = False\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "\n",
    "# Train Agent\n",
    "print(\"üß™ Training Q-Learning on 2-State MDP\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = TwoStateMDP()\n",
    "agent = QLearningAgent(n_actions=1, alpha=0.1, gamma=0.9, epsilon=0.0)  # no exploration needed\n",
    "\n",
    "num_episodes = 50\n",
    "q_history = []\n",
    "\n",
    "print(f\"Training for {num_episodes} episodes...\\n\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = 0  # only one action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "    \n",
    "    # Record Q-value\n",
    "    q_a = agent.get_q_value('A', 0)\n",
    "    q_history.append(q_a)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"  Episode {episode:3d}: Q(A, go) = {q_a:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"\\nFinal Q-values:\")\n",
    "print(f\"  Q(A, go) = {agent.get_q_value('A', 0):.4f}\")\n",
    "print(f\"  Q(B, -) = {agent.get_q_value('B', 0):.4f}\")\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì theoretical value\n",
    "theoretical = 0 + 0.9 * 10  # R(A‚ÜíB) + Œ≥ √ó R(B)\n",
    "print(f\"\\nüéØ Theoretical Q(A, go) = 0 + 0.9 √ó 10 = {theoretical:.4f}\")\n",
    "print(f\"üìä Learned Q(A, go) = {agent.get_q_value('A', 0):.4f}\")\n",
    "print(f\"‚úÖ Difference = {abs(theoretical - agent.get_q_value('A', 0)):.4f}\")\n",
    "\n",
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(q_history, linewidth=2, label='Learned Q(A, go)')\n",
    "plt.axhline(y=theoretical, color='red', linestyle='--', linewidth=2, label='Theoretical Value')\n",
    "plt.xlabel('Episode', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Q-value', fontsize=12, fontweight='bold')\n",
    "plt.title('Q-Learning Convergence on 2-State MDP', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation:\")\n",
    "print(\"  - Q-value converges to theoretical value!\")\n",
    "print(\"  - Learning is stable with Œ± = 0.1\")\n",
    "print(\"  - This proves Q-Learning works! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡πÑ‡∏õ‡∏™‡∏π‡πà DQN\n",
    "\n",
    "### üìå Q-Learning ‚Üí Deep Q-Network (DQN)\n",
    "\n",
    "| Component | Q-Learning | DQN |\n",
    "|-----------|------------|-----|\n",
    "| **Q-value Storage** | Q-table (dict) | Neural Network |\n",
    "| **State Space** | Discrete (small) | Continuous (large) |\n",
    "| **Update Rule** | Q(s,a) ‚Üê Q(s,a) + Œ± √ó TD_error | Same! Just neural net |\n",
    "| **Action Selection** | argmax_a Q(s,a) | Same! |\n",
    "| **Experience** | Online (forget old) | Replay Buffer (reuse) |\n",
    "| **Target** | max Q(s',a') | Target Network (stable) |\n",
    "\n",
    "### üîÑ Evolution Path:\n",
    "\n",
    "```\n",
    "1. Q-Learning (1989)\n",
    "   ‚Üì\n",
    "   Q-table ‚Üí Neural Network\n",
    "   ‚Üì\n",
    "2. Neural Fitted Q (2005)\n",
    "   ‚Üì\n",
    "   + Experience Replay\n",
    "   ‚Üì\n",
    "3. DQN (2013)\n",
    "   ‚Üì\n",
    "   + Target Network\n",
    "   ‚Üì\n",
    "4. Double DQN (2015)\n",
    "   ‚Üì\n",
    "   + Reduce overestimation\n",
    "   ‚Üì\n",
    "5. Trading DQN (Now!)\n",
    "   ‚Üì\n",
    "   + Portfolio management\n",
    "   + Risk control\n",
    "```\n",
    "\n",
    "### üöÄ Next Steps to DQN:\n",
    "\n",
    "**‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß (Notebook ‡∏ô‡∏µ‡πâ):**\n",
    "- ‚úÖ Q-Learning update rule\n",
    "- ‚úÖ TD Learning\n",
    "- ‚úÖ Off-policy learning\n",
    "- ‚úÖ Hyperparameters (Œ±, Œ≥, Œµ)\n",
    "\n",
    "**‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠:**\n",
    "- üìä Grid World implementation (Notebook 03)\n",
    "- üé≤ Exploration strategies (Notebook 04)\n",
    "- üß† Neural Networks (Module 03)\n",
    "- üéÆ DQN architecture (Module 04)\n",
    "- üíπ Trading DQN (Module 05)\n",
    "\n",
    "---\n",
    "\n",
    "## üíπ Q-Learning for Trading (Preview)\n",
    "\n",
    "### üéØ Trading as MDP:\n",
    "\n",
    "```python\n",
    "# State\n",
    "state = {\n",
    "    'price': current_price,\n",
    "    'position': current_position,\n",
    "    'cash': available_cash,\n",
    "    'indicators': [RSI, MACD, ...]  # Technical indicators\n",
    "}\n",
    "\n",
    "# Actions\n",
    "actions = [0, 1, 2]  # [Buy, Hold, Sell]\n",
    "\n",
    "# Reward\n",
    "reward = PnL + penalty_for_risk\n",
    "\n",
    "# Q-Learning Update (same!)\n",
    "Q(state, action) ‚Üê Q(state, action) + Œ± √ó [reward + Œ≥ √ó max Q(next_state, a') - Q(state, action)]\n",
    "```\n",
    "\n",
    "### üîß Adaptations for Trading:\n",
    "\n",
    "1. **State Representation:**\n",
    "   - Price history (OHLCV)\n",
    "   - Technical indicators\n",
    "   - Portfolio state\n",
    "   - Market regime\n",
    "\n",
    "2. **Action Space:**\n",
    "   - Discrete: {Buy, Hold, Sell}\n",
    "   - Continuous: {position_size: -1 to +1}\n",
    "\n",
    "3. **Reward Design:**\n",
    "   ```python\n",
    "   reward = realized_pnl \n",
    "          - transaction_cost\n",
    "          - risk_penalty (e.g., volatility)\n",
    "          + diversification_bonus\n",
    "   ```\n",
    "\n",
    "4. **Hyperparameters:**\n",
    "   - Œ± = 0.001 (very small for stability)\n",
    "   - Œ≥ = 0.95 - 0.99 (long-term view)\n",
    "   - Œµ = decay from 1.0 to 0.01\n",
    "\n",
    "---\n",
    "\n",
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô\n",
    "\n",
    "### üìö ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Temporal Difference Learning**\n",
    "```\n",
    "TD Error = (Target - Current)\n",
    "         = (R + Œ≥ √ó max Q(s',a')) - Q(s,a)\n",
    "```\n",
    "\n",
    "#### 2Ô∏è‚É£ **Q-Learning Update Rule**\n",
    "```\n",
    "Q(s,a) ‚Üê Q(s,a) + Œ± √ó TD_Error\n",
    "```\n",
    "\n",
    "#### 3Ô∏è‚É£ **Learning Rate (Œ±)**\n",
    "- Œ± ‡∏ï‡πà‡∏≥ (0.01-0.1): ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤, ‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á ‚Üí Trading\n",
    "- Œ± ‡∏™‡∏π‡∏á (0.3-0.5): ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß ‚Üí Simulation\n",
    "\n",
    "#### 4Ô∏è‚É£ **Off-Policy Learning**\n",
    "- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å policy ‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ\n",
    "- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ reuse old experiences\n",
    "- ‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Experience Replay\n",
    "\n",
    "#### 5Ô∏è‚É£ **Implementation**\n",
    "- Q-table with defaultdict\n",
    "- Œµ-greedy action selection\n",
    "- Update function\n",
    "\n",
    "### üîë Key Insights:\n",
    "\n",
    "1. **Q-Learning = Base of DQN**\n",
    "   - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Q-Learning ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à DQN\n",
    "   - Update rule ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô\n",
    "\n",
    "2. **Model-Free**\n",
    "   - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ P(s'|s,a)\n",
    "   - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö real-world\n",
    "\n",
    "3. **Hyperparameters Matter**\n",
    "   - Œ±, Œ≥, Œµ ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
    "   - Trading ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏≤‡∏û‡∏¥‡πÄ‡∏®‡∏©\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î\n",
    "\n",
    "### Exercise 1: Manual Update\n",
    "```\n",
    "Q(s,a) = 5.0\n",
    "Reward = 2.0\n",
    "max Q(s',a') = 8.0\n",
    "Œ± = 0.1, Œ≥ = 0.9\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Q_new = ?\n",
    "```\n",
    "\n",
    "### Exercise 2: Learning Rate\n",
    "```\n",
    "‡∏ó‡∏≥‡πÑ‡∏° Trading ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Œ± ‡∏ï‡πà‡∏≥ (0.05-0.1)?\n",
    "‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "```\n",
    "\n",
    "### Exercise 3: Off-Policy\n",
    "```\n",
    "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏° Off-Policy learning\n",
    "‡∏ñ‡∏∂‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Experience Replay?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Notebook\n",
    "\n",
    "üëâ **[03_q_learning_gridworld.ipynb](03_q_learning_gridworld.ipynb)**\n",
    "\n",
    "**‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:**\n",
    "- Implement Q-Learning ‡πÉ‡∏ô Grid World\n",
    "- Training loop ‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\n",
    "- Visualize learning progress\n",
    "- Compare with Value Iteration\n",
    "- Hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Reference\n",
    "\n",
    "### Papers:\n",
    "- [Watkins (1989) - Q-Learning](https://link.springer.com/article/10.1007/BF00992698)\n",
    "- [Mnih et al. (2013) - DQN (Atari)](https://arxiv.org/abs/1312.5602)\n",
    "- [Mnih et al. (2015) - DQN (Nature)](https://www.nature.com/articles/nature14236)\n",
    "\n",
    "### Videos:\n",
    "- [David Silver RL Course - Lecture 5: Model-Free Control](https://www.youtube.com/watch?v=0g4j2k_Ggc4)\n",
    "- [DeepMind x UCL - Q-Learning](https://www.youtube.com/watch?v=ISk80iLhdfU)\n",
    "\n",
    "### Books:\n",
    "- Sutton & Barto - Reinforcement Learning (Chapter 6)\n",
    "- [Spinning Up in Deep RL - Q-Learning](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
