{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíé Value Functions: V(s) ‡πÅ‡∏•‡∏∞ Q(s,a)\n",
    "## ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß: ‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡∏£‡∏î‡∏™‡∏≠‡∏á‡∏Ñ‡∏ô\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏°‡∏µ‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡∏£‡∏î 2 ‡∏Ñ‡∏ô:\n",
    "\n",
    "### üë§ ‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡∏£‡∏î A (State Value Thinker)\n",
    "‡πÄ‡∏Ç‡∏≤‡∏Ñ‡∏¥‡∏î‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ:\n",
    "- \"‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà $100 ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£?\"\n",
    "- ‡∏°‡∏≠‡∏á‡πÅ‡∏Ñ‡πà‡∏ß‡πà‡∏≤ **‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô**\n",
    "- ‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ó‡∏≥ action ‡∏≠‡∏∞‡πÑ‡∏£\n",
    "\n",
    "**‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ State Value Function: V(s)**\n",
    "\n",
    "---\n",
    "\n",
    "### üë• ‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡∏£‡∏î B (Action-Value Thinker)\n",
    "‡πÄ‡∏Ç‡∏≤‡∏Ñ‡∏¥‡∏î‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ:\n",
    "- \"‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤ $100, ‡∏ñ‡πâ‡∏≤ **‡∏ã‡∏∑‡πâ‡∏≠** ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£?\"\n",
    "- \"‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤ $100, ‡∏ñ‡πâ‡∏≤ **‡∏Ç‡∏≤‡∏¢** ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£?\"\n",
    "- \"‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤ $100, ‡∏ñ‡πâ‡∏≤ **‡∏£‡∏≠** ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£?\"\n",
    "- ‡∏°‡∏≠‡∏á‡∏ß‡πà‡∏≤ **‡πÅ‡∏ï‡πà‡∏•‡∏∞ action ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£**\n",
    "\n",
    "**‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ Action-Value Function: Q(s,a)**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á V(s) ‡πÅ‡∏•‡∏∞ Q(s,a)\n",
    "\n",
    "### 1Ô∏è‚É£ State Value Function: V(s)\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°:**\n",
    "```\n",
    "V(s) = Expected return ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà state s ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ï‡∏≤‡∏° policy œÄ\n",
    "```\n",
    "\n",
    "**Mathematical Formula:**\n",
    "```\n",
    "V^œÄ(s) = E_œÄ[G_t | S_t = s]\n",
    "        = E_œÄ[R_{t+1} + Œ≥*V^œÄ(S_{t+1}) | S_t = s]\n",
    "```\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**\n",
    "- V(‡∏£‡∏≤‡∏Ñ‡∏≤ $100) = 5.2 ‚Üí ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ return ‡∏£‡∏ß‡∏° $5.2\n",
    "- V(‡∏°‡∏µ position) = -2.1 ‚Üí ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏î‡∏µ ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏Ç‡∏≤‡∏î‡∏ó‡∏∏‡∏ô\n",
    "\n",
    "> ‡πÅ‡∏ï‡πà! ‡πÑ‡∏°‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ action ‡πÑ‡∏´‡∏ô‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Action-Value Function: Q(s,a)\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°:**\n",
    "```\n",
    "Q(s,a) = Expected return ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà state s, ‡∏ó‡∏≥ action a, ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏ï‡∏≤‡∏° policy œÄ\n",
    "```\n",
    "\n",
    "**Mathematical Formula:**\n",
    "```\n",
    "Q^œÄ(s,a) = E_œÄ[G_t | S_t = s, A_t = a]\n",
    "          = E[R_{t+1} + Œ≥*V^œÄ(S_{t+1}) | S_t = s, A_t = a]\n",
    "```\n",
    "$Q^\\pi(s,a) = \\mathbb{E}_\\pi \\big[\\, R_{t+1} + \\gamma V^\\pi(S_{t+1}) \\,\\big|\\, S_t = s, A_t = a \\big]$\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**\n",
    "- Q(‡∏£‡∏≤‡∏Ñ‡∏≤ $100, Buy) = 8.5 ‚Üí ‡∏ã‡∏∑‡πâ‡∏≠‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ $8.5\n",
    "- Q(‡∏£‡∏≤‡∏Ñ‡∏≤ $100, Sell) = -3.2 ‚Üí ‡∏Ç‡∏≤‡∏¢‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏Ç‡∏≤‡∏î‡∏ó‡∏∏‡∏ô\n",
    "- Q(‡∏£‡∏≤‡∏Ñ‡∏≤ $100, Hold) = 2.1 ‚Üí ‡∏£‡∏≠‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ $2.1\n",
    "\n",
    "> Q(s,a) ‡∏Ñ‡∏∑‡∏≠ ‚Äú‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action a ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• + ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Œ≥‚Äù\n",
    "---\n",
    "\n",
    "## üîó ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á V(s) ‡πÅ‡∏•‡∏∞ Q(s,a)\n",
    "\n",
    "$\n",
    "V^\\pi(s) = \\sum_a \\pi(a|s) \\, Q^\\pi(s,a)\n",
    "$\n",
    "\n",
    "\n",
    "**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:**\n",
    "- V(s) = ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Q(s,a) ‡∏ó‡∏∏‡∏Å action\n",
    "- ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠ probability ‡∏ó‡∏µ‡πà policy ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ô‡∏±‡πâ‡∏ô\n",
    "\n",
    "**‡∏ñ‡πâ‡∏≤ policy ‡πÄ‡∏õ‡πá‡∏ô Deterministic (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ô‡πà‡πÜ):**\n",
    "\n",
    "$\n",
    "V(s) = Q(s, a^*)\n",
    "$\n",
    "‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà $a^*$ ‡∏Ñ‡∏∑‡∏≠ action ‡∏ó‡∏µ‡πà policy ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏≥‡πÉ‡∏ô state ‡∏ô‡∏±‡πâ‡∏ô\n",
    "\n",
    "**‡∏ñ‡πâ‡∏≤ policy ‡πÄ‡∏õ‡πá‡∏ô Greedy (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î):**\n",
    "\n",
    "$\n",
    "V^*(s) = \\max_a Q^*(s,a)\n",
    "$ ‚Üê Optimal value\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô state Start ‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ 2 ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:\n",
    "| Action  | Next state | Reward | Probability |\n",
    "| :------ | :--------- | :----: | :---------: |\n",
    "| GoLeft  | Bad        |   -3   |     1.0     |\n",
    "| GoRight | Good       |   +5   |     1.0     |\n",
    "\n",
    "‡πÅ‡∏•‡∏∞ Good / Bad ‡πÄ‡∏õ‡πá‡∏ô terminal ‚Üí $V(Good)=V(Bad)=0, Œ≥=0.9$\n",
    "\n",
    "$Q(Start,GoLeft)=-3+0.9√ó0=-3$\n",
    "\n",
    "$Q(Start,GoRight)=+5+0.9√ó0=5$\n",
    "\n",
    "‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô\n",
    "$V(Start)= \\max‚Å°_a Q(Start,a)=5$\n",
    "\n",
    "‚Üí Agent ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å GoRight ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Q ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ <br>\n",
    "(‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ ‚Äú‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‚Äù)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"üì¶ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Grid World with V(s) and Q(s,a)\n",
    "\n",
    "‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á simple grid ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏±‡πâ‡∏á V(s) ‡πÅ‡∏•‡∏∞ Q(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldValues:\n",
    "    def __init__(self, size=4, gamma=0.9):\n",
    "        self.size = size\n",
    "        self.gamma = gamma\n",
    "        self.actions = ['‚Üë', '‚Üì', '‚Üê', '‚Üí']\n",
    "        self.action_effects = {\n",
    "            '‚Üë': (-1, 0),\n",
    "            '‚Üì': (1, 0),\n",
    "            '‚Üê': (0, -1),\n",
    "            '‚Üí': (0, 1)\n",
    "        }\n",
    "        \n",
    "        # Goal ‡πÅ‡∏•‡∏∞ Holes\n",
    "        self.goal = (size-1, size-1)\n",
    "        self.holes = [(1, 1), (2, 2)]\n",
    "        \n",
    "        # Initialize V(s) and Q(s,a)\n",
    "        self.V = np.zeros((size, size))\n",
    "        self.Q = {a: np.zeros((size, size)) for a in self.actions}\n",
    "    \n",
    "    def is_valid(self, pos):\n",
    "        \"\"\"Check if position is valid\"\"\"\n",
    "        r, c = pos\n",
    "        return 0 <= r < self.size and 0 <= c < self.size\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"Get next state after action\"\"\"\n",
    "        dr, dc = self.action_effects[action]\n",
    "        next_state = (state[0] + dr, state[1] + dc)\n",
    "        \n",
    "        if not self.is_valid(next_state):\n",
    "            return state  # ‡∏ä‡∏ô‡∏ù‡∏≤, ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏¥‡∏°\n",
    "        return next_state\n",
    "    \n",
    "    def get_reward(self, state, next_state):\n",
    "        \"\"\"Get reward for transition\"\"\"\n",
    "        if next_state == self.goal:\n",
    "            return 10\n",
    "        elif next_state in self.holes:\n",
    "            return -10\n",
    "        else:\n",
    "            return -1  # Step penalty\n",
    "    \n",
    "    def update_q_values(self):\n",
    "        \"\"\"Update Q(s,a) using Bellman equation\"\"\"\n",
    "        new_Q = {a: np.zeros((self.size, self.size)) for a in self.actions}\n",
    "        \n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                state = (r, c)\n",
    "                \n",
    "                # Terminal states\n",
    "                if state == self.goal or state in self.holes:\n",
    "                    continue\n",
    "                \n",
    "                # Update Q for each action\n",
    "                for action in self.actions:\n",
    "                    next_state = self.get_next_state(state, action)\n",
    "                    reward = self.get_reward(state, next_state)\n",
    "                    \n",
    "                    # Bellman equation for Q\n",
    "                    new_Q[action][r, c] = reward + self.gamma * self.V[next_state]\n",
    "        \n",
    "        self.Q = new_Q\n",
    "    \n",
    "    def update_v_values(self, policy='greedy'):\n",
    "        \"\"\"Update V(s) from Q(s,a)\"\"\"\n",
    "        new_V = np.zeros((self.size, self.size))\n",
    "        \n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                state = (r, c)\n",
    "                \n",
    "                # Terminal states\n",
    "                if state == self.goal:\n",
    "                    new_V[r, c] = 10\n",
    "                elif state in self.holes:\n",
    "                    new_V[r, c] = -10\n",
    "                else:\n",
    "                    # Get Q values for all actions\n",
    "                    q_values = [self.Q[a][r, c] for a in self.actions]\n",
    "                    \n",
    "                    if policy == 'greedy':\n",
    "                        # V(s) = max Q(s,a)\n",
    "                        new_V[r, c] = max(q_values)\n",
    "                    elif policy == 'uniform':\n",
    "                        # V(s) = average Q(s,a)\n",
    "                        new_V[r, c] = np.mean(q_values)\n",
    "        \n",
    "        self.V = new_V\n",
    "    \n",
    "    def value_iteration(self, num_iterations=20, policy='greedy'):\n",
    "        \"\"\"Run value iteration\"\"\"\n",
    "        print(f\"üîÑ Running Value Iteration with {policy} policy...\\n\")\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            self.update_q_values()\n",
    "            self.update_v_values(policy=policy)\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                print(f\"Iteration {i}: V(0,0) = {self.V[0,0]:.2f}, V(0,1) = {self.V[0,1]:.2f}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Converged!\")\n",
    "    \n",
    "    def get_optimal_policy(self):\n",
    "        \"\"\"Get optimal policy from Q values\"\"\"\n",
    "        policy = np.empty((self.size, self.size), dtype=str)\n",
    "        \n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                if (r, c) == self.goal:\n",
    "                    policy[r, c] = '‚òÖ'\n",
    "                elif (r, c) in self.holes:\n",
    "                    policy[r, c] = '‚úñ'\n",
    "                else:\n",
    "                    q_values = [self.Q[a][r, c] for a in self.actions]\n",
    "                    best_action_idx = np.argmax(q_values)\n",
    "                    policy[r, c] = self.actions[best_action_idx]\n",
    "        \n",
    "        return policy\n",
    "\n",
    "# Create and train\n",
    "grid = GridWorldValues(size=4, gamma=0.9)\n",
    "grid.value_iteration(num_iterations=20, policy='greedy')\n",
    "\n",
    "print(f\"\\nüìä Final V(0,0) = {grid.V[0,0]:.2f}\")\n",
    "print(f\"üìä Final V(goal) = {grid.V[grid.goal]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualize V(s): State Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize V(s)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot heatmap\n",
    "im = ax.imshow(grid.V, cmap='RdYlGn', interpolation='nearest')\n",
    "\n",
    "# Add values\n",
    "for r in range(grid.size):\n",
    "    for c in range(grid.size):\n",
    "        if (r, c) == grid.goal:\n",
    "            text = ax.text(c, r, f'GOAL\\n{grid.V[r, c]:.1f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=11)\n",
    "        elif (r, c) in grid.holes:\n",
    "            text = ax.text(c, r, f'HOLE\\n{grid.V[r, c]:.1f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"white\", fontweight='bold', fontsize=11)\n",
    "        else:\n",
    "            text = ax.text(c, r, f'{grid.V[r, c]:.1f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "ax.set_title('State Value Function: V(s)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xticks(range(grid.size))\n",
    "ax.set_yticks(range(grid.size))\n",
    "ax.set_xlabel('Column', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Row', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Value V(s)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"- ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß = State ‡∏ó‡∏µ‡πà‡∏î‡∏µ (‡πÉ‡∏Å‡∏•‡πâ Goal, V ‡∏™‡∏π‡∏á)\")\n",
    "print(\"- ‡∏™‡∏µ‡πÅ‡∏î‡∏á = State ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡πà (‡πÉ‡∏Å‡∏•‡πâ Hole, V ‡∏ï‡πà‡∏≥)\")\n",
    "print(\"- V(s) ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ expected return ‡∏à‡∏≤‡∏Å state ‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Visualize Q(s,a): Action Values\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞ action ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q(s,a) for each action\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, action in enumerate(grid.actions):\n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(grid.Q[action], cmap='RdYlGn', interpolation='nearest')\n",
    "    \n",
    "    # Add values\n",
    "    for r in range(grid.size):\n",
    "        for c in range(grid.size):\n",
    "            value = grid.Q[action][r, c]\n",
    "            color = 'white' if value < 0 else 'black'\n",
    "            ax.text(c, r, f'{value:.1f}', ha=\"center\", va=\"center\", \n",
    "                   color=color, fontsize=9)\n",
    "    \n",
    "    ax.set_title(f'Q(s, {action})', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(range(grid.size))\n",
    "    ax.set_yticks(range(grid.size))\n",
    "    \n",
    "    # Colorbar\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('Action-Value Function: Q(s,a) for Each Action', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"- Q(s,a) ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ ‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥ action ‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà state ‡∏ô‡∏±‡πâ‡∏ô ‡∏à‡∏∞‡πÑ‡∏î‡πâ return ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£\")\n",
    "print(\"- ‡πÅ‡∏ï‡πà‡∏•‡∏∞ state ‡∏°‡∏µ 4 Q values (4 actions)\")\n",
    "print(\"- ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß = action ‡∏ó‡∏µ‡πà‡∏î‡∏µ, ‡∏™‡∏µ‡πÅ‡∏î‡∏á = action ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡πà\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ Optimal Policy from Q(s,a)\n",
    "\n",
    "‡∏à‡∏≤‡∏Å Q values ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏≤ optimal policy ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢:\n",
    "```\n",
    "œÄ*(s) = argmax_a Q(s,a)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimal policy\n",
    "optimal_policy = grid.get_optimal_policy()\n",
    "\n",
    "print(\"üß≠ Optimal Policy:\")\n",
    "print(\"=\"*40)\n",
    "print(optimal_policy)\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Visualize policy\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Background\n",
    "ax.imshow(grid.V, cmap='RdYlGn', interpolation='nearest', alpha=0.3)\n",
    "\n",
    "# Add arrows and labels\n",
    "for r in range(grid.size):\n",
    "    for c in range(grid.size):\n",
    "        action = optimal_policy[r, c]\n",
    "        \n",
    "        if action == '‚òÖ':\n",
    "            ax.text(c, r, '‚òÖ\\nGOAL', ha=\"center\", va=\"center\", \n",
    "                   fontsize=16, fontweight='bold', color='green')\n",
    "        elif action == '‚úñ':\n",
    "            ax.text(c, r, '‚úñ\\nHOLE', ha=\"center\", va=\"center\", \n",
    "                   fontsize=16, fontweight='bold', color='red')\n",
    "        else:\n",
    "            # Show arrow and Q value\n",
    "            q_value = grid.Q[action][r, c]\n",
    "            ax.text(c, r, f'{action}\\n{q_value:.1f}', ha=\"center\", va=\"center\", \n",
    "                   fontsize=14, fontweight='bold', color='blue')\n",
    "\n",
    "ax.set_title('Optimal Policy: Best Action at Each State', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xticks(range(grid.size))\n",
    "ax.set_yticks(range(grid.size))\n",
    "ax.set_xlabel('Column', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Row', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"- ‡∏•‡∏π‡∏Å‡∏®‡∏£‡πÅ‡∏™‡∏î‡∏á action ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ state\")\n",
    "print(\"- Agent ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡∏ï‡∏≤‡∏° policy ‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠ maximize return\")\n",
    "print(\"- ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ greedy policy ‡∏à‡∏≤‡∏Å Q values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: Q(s,a) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 1 State\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π Q values ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å actions ‡∏ó‡∏µ‡πà state ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å state ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n",
    "example_state = (1, 2)\n",
    "\n",
    "# Get Q values for all actions at this state\n",
    "q_values_at_state = {action: grid.Q[action][example_state] for action in grid.actions}\n",
    "\n",
    "print(f\"üìç State: {example_state}\")\n",
    "print(f\"üìä V(s) = {grid.V[example_state]:.2f}\")\n",
    "print(\"\\nQ(s,a) for each action:\")\n",
    "print(\"=\"*40)\n",
    "for action, q_val in q_values_at_state.items():\n",
    "    marker = \" ‚Üê BEST\" if q_val == max(q_values_at_state.values()) else \"\"\n",
    "    print(f\"Q(s, {action}) = {q_val:6.2f}{marker}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "actions = list(q_values_at_state.keys())\n",
    "values = list(q_values_at_state.values())\n",
    "colors = ['green' if v == max(values) else 'gray' for v in values]\n",
    "\n",
    "bars = ax.bar(actions, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add V(s) line\n",
    "v_value = grid.V[example_state]\n",
    "ax.axhline(y=v_value, color='red', linestyle='--', linewidth=2, label=f'V(s) = {v_value:.2f}')\n",
    "\n",
    "ax.set_xlabel('Action', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Q(s,a)', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Q Values at State {example_state}', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(f\"- Best action: {max(q_values_at_state, key=q_values_at_state.get)}\")\n",
    "print(f\"- V(s) = max Q(s,a) = {max(values):.2f} (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö greedy policy)\")\n",
    "print(\"- ‡πÅ‡∏ï‡πà‡∏•‡∏∞ action ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Relationship: V(s) = max Q(s,a)\n",
    "\n",
    "‡∏°‡∏≤‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå‡∏ß‡πà‡∏≤ V(s) = max Q(s,a) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify relationship V(s) = max Q(s,a)\n",
    "print(\"üîç Verifying: V(s) = max Q(s,a)\\n\" + \"=\"*60)\n",
    "\n",
    "errors = []\n",
    "for r in range(grid.size):\n",
    "    for c in range(grid.size):\n",
    "        if (r, c) == grid.goal or (r, c) in grid.holes:\n",
    "            continue\n",
    "        \n",
    "        v_value = grid.V[r, c]\n",
    "        q_values = [grid.Q[a][r, c] for a in grid.actions]\n",
    "        max_q = max(q_values)\n",
    "        \n",
    "        error = abs(v_value - max_q)\n",
    "        errors.append(error)\n",
    "        \n",
    "        if r == 0 and c <= 2:  # Show first few\n",
    "            print(f\"State ({r},{c}): V(s)={v_value:.3f}, max Q(s,a)={max_q:.3f}, Error={error:.6f}\")\n",
    "\n",
    "avg_error = np.mean(errors)\n",
    "max_error = np.max(errors)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Average Error: {avg_error:.8f}\")\n",
    "print(f\"Max Error: {max_error:.8f}\")\n",
    "print(\"\\n‚úÖ Relationship verified! V(s) ‚âà max Q(s,a) for greedy policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Q-values ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Trading\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏Ç‡∏≠‡∏á Q values ‡πÉ‡∏ô trading scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Q-values Example\n",
    "class SimpleTradingExample:\n",
    "    def __init__(self):\n",
    "        # States: (price_trend, has_position)\n",
    "        # price_trend: 'rising', 'falling', 'flat'\n",
    "        # has_position: True/False\n",
    "        \n",
    "        # Actions: 'buy', 'sell', 'hold'\n",
    "        \n",
    "        # Mock Q-values (‡∏à‡∏≤‡∏Å training)\n",
    "        self.Q_table = {\n",
    "            ('rising', False): {'buy': 8.5, 'sell': -2.0, 'hold': 3.2},\n",
    "            ('rising', True):  {'buy': -1.0, 'sell': 5.5, 'hold': 6.8},\n",
    "            ('falling', False): {'buy': -5.2, 'sell': 1.0, 'hold': 2.1},\n",
    "            ('falling', True):  {'buy': -8.0, 'sell': 7.5, 'hold': -2.3},\n",
    "            ('flat', False):    {'buy': 1.5, 'sell': -0.5, 'hold': 2.8},\n",
    "            ('flat', True):     {'buy': -0.8, 'sell': 2.1, 'hold': 3.5}\n",
    "        }\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        q_values = self.Q_table[state]\n",
    "        return max(q_values, key=q_values.get)\n",
    "    \n",
    "    def visualize_q_values(self):\n",
    "        \"\"\"Visualize Q-values for all state-action pairs\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        states = list(self.Q_table.keys())\n",
    "        \n",
    "        for idx, state in enumerate(states):\n",
    "            ax = axes[idx]\n",
    "            q_values = self.Q_table[state]\n",
    "            \n",
    "            actions = list(q_values.keys())\n",
    "            values = list(q_values.values())\n",
    "            colors = ['green' if v == max(values) else 'lightcoral' if v < 0 else 'lightblue' \n",
    "                     for v in values]\n",
    "            \n",
    "            bars = ax.bar(actions, values, color=colors, edgecolor='black', linewidth=2)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.1f}',\n",
    "                       ha='center', va='bottom' if height > 0 else 'top', \n",
    "                       fontsize=11, fontweight='bold')\n",
    "            \n",
    "            ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "            ax.set_title(f'State: {state}', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('Q(s,a)', fontsize=10, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            ax.set_ylim(-10, 10)\n",
    "        \n",
    "        plt.suptitle('Trading Q-Values: Different Market States', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and visualize\n",
    "trading_example = SimpleTradingExample()\n",
    "trading_example.visualize_q_values()\n",
    "\n",
    "print(\"\\nüéØ Optimal Actions:\")\n",
    "print(\"=\"*60)\n",
    "for state in trading_example.Q_table.keys():\n",
    "    best_action = trading_example.get_best_action(state)\n",
    "    q_value = trading_example.Q_table[state][best_action]\n",
    "    print(f\"State {state:25s} ‚Üí Best: {best_action:4s} (Q={q_value:.1f})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüí° Trading Insights:\")\n",
    "print(\"- rising + no position ‚Üí BUY (Q=8.5)\")\n",
    "print(\"- falling + has position ‚Üí SELL (Q=7.5)\")\n",
    "print(\"- rising + has position ‚Üí HOLD (Q=6.8) ‡∏´‡∏£‡∏∑‡∏≠ SELL (Q=5.5)\")\n",
    "print(\"- Q-values ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ action ‡πÑ‡∏´‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ: V(s) vs Q(s,a)\n",
    "\n",
    "### üìå State Value Function: V(s)\n",
    "- **‡∏Ñ‡∏≥‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°:** Expected return ‡∏à‡∏≤‡∏Å state s\n",
    "- **‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠:** ‡∏£‡∏π‡πâ policy ‡πÅ‡∏•‡πâ‡∏ß, ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô state\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ (1 value ‡∏ï‡πà‡∏≠ state)\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:** ‡πÑ‡∏°‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥ action ‡∏≠‡∏∞‡πÑ‡∏£\n",
    "\n",
    "### üìå Action-Value Function: Q(s,a)\n",
    "- **‡∏Ñ‡∏≥‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°:** Expected return ‡∏à‡∏≤‡∏Å state s ‡πÅ‡∏•‡∏∞ action a\n",
    "- **‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠:** ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏≤ optimal policy\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡∏´‡∏≤ policy ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á: œÄ(s) = argmax Q(s,a)\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:** ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ (|A| values ‡∏ï‡πà‡∏≠ state)\n",
    "\n",
    "### üîó Relationship\n",
    "```\n",
    "V(s) = Œ£ œÄ(a|s) * Q(s,a)           (General)\n",
    "V*(s) = max_a Q*(s,a)               (Optimal)\n",
    "Q(s,a) = E[R + Œ≥*V(s')]             (Bellman)\n",
    "```\n",
    "\n",
    "### üíº ‡πÉ‡∏ô Trading\n",
    "- **Q-Network ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°** ‡πÄ‡∏û‡∏£‡∏≤‡∏∞:\n",
    "  - ‡∏´‡∏≤ action ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n",
    "  - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö discrete actions (Buy/Sell/Hold)\n",
    "  - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ transition dynamics\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "1. **Notebook 04:** Policies (Deterministic, Stochastic, Epsilon-Greedy)\n",
    "2. **Notebook 05:** Exploration vs Exploitation\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î\n",
    "\n",
    "### Exercise 1: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì V(s) ‡∏à‡∏≤‡∏Å Q(s,a)\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥:\n",
    "- Q(s, up) = 5\n",
    "- Q(s, down) = 3\n",
    "- Q(s, left) = -2\n",
    "- Q(s, right) = 7\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì V(s) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:\n",
    "1. Greedy policy\n",
    "2. Uniform random policy\n",
    "\n",
    "### Exercise 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Q-table ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Crypto Trading\n",
    "States: (trend, volatility, position)\n",
    "- trend: 'up', 'down'\n",
    "- volatility: 'high', 'low'\n",
    "- position: 'long', 'short', 'none'\n",
    "\n",
    "Actions: 'buy', 'sell', 'hold'\n",
    "\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á mock Q-table ‡πÅ‡∏•‡∏∞‡∏´‡∏≤ optimal policy\n",
    "\n",
    "### Exercise 3: Value Iteration\n",
    "‡πÉ‡∏ä‡πâ GridWorldValues class:\n",
    "1. ‡πÄ‡∏û‡∏¥‡πà‡∏° holes ‡∏ó‡∏µ‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏∑‡πà‡∏ô\n",
    "2. Run value iteration\n",
    "3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå V(s) ‡πÅ‡∏•‡∏∞ optimal policy\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Ready to learn about Policies?**\n",
    "\n",
    "üëâ [Next: 04_policies.ipynb](04_policies.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
