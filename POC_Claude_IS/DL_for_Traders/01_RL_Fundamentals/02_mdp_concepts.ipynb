{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≤ Markov Decision Process (MDP)\n",
    "## ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß: ‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏™‡∏±‡πâ‡∏ô\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤... **‡∏°‡∏±‡∏ô‡∏à‡∏≥‡∏≠‡∏î‡∏µ‡∏ï‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ!** ü§ñüí≠\n",
    "\n",
    "‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡∏à‡∏≥‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà:\n",
    "- ‚úÖ **‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏£‡∏á‡πÑ‡∏´‡∏ô** (State ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)\n",
    "- ‚ùå **‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏´‡∏ô** (‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à history)\n",
    "\n",
    "‡πÅ‡∏ï‡πà‡∏ô‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢! ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ **Markov Property** üéØ\n",
    "\n",
    "### üß† Markov Property ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "\n",
    "**\"‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏î‡∏µ‡∏ï\"**\n",
    "\n",
    "```\n",
    "P(S_{t+1} | S_t, S_{t-1}, S_{t-2}, ..., S_0) = P(S_{t+1} | S_t)\n",
    "```\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á:**\n",
    "- ‚ôüÔ∏è **‡∏´‡∏°‡∏≤‡∏Å‡∏£‡∏∏‡∏Å:** ‡∏ó‡πà‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏ß‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\n",
    "- üìà **‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô (‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á):** ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ß‡∏±‡∏ô‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ (Efficient Market Hypothesis)\n",
    "- üöó **‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥:** ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏à‡∏≤‡∏Å‡∏™‡∏†‡∏≤‡∏û‡∏ñ‡∏ô‡∏ô/‡∏£‡∏ñ ‡∏ì ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Markov Decision Process (MDP)\n",
    "\n",
    "MDP ‡∏Ñ‡∏∑‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Reinforcement Learning\n",
    "\n",
    "### ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á MDP:\n",
    "\n",
    "1. **S** = Set of States (‡πÄ‡∏ã‡∏ï‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)\n",
    "2. **A** = Set of Actions (‡πÄ‡∏ã‡∏ï‡∏Ç‡∏≠‡∏á action ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)\n",
    "3. **P** = Transition Probability (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô state)\n",
    "   - P(s'|s,a) = ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á s' ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà s ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ action a\n",
    "4. **R** = Reward Function (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô reward)\n",
    "   - R(s,a,s') = reward ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ó‡∏≥ a ‡∏ó‡∏µ‡πà s ‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á s'\n",
    "5. **Œ≥** (gamma) = Discount Factor (0 ‚â§ Œ≥ ‚â§ 1)\n",
    "   - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤ reward ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£\n",
    "\n",
    "---\n",
    "\n",
    "## üí∞ Return ‡πÅ‡∏•‡∏∞ Discount Factor\n",
    "\n",
    "### Return (G_t) ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "\n",
    "**Return = ‡∏£‡∏ß‡∏° reward ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏ß‡∏•‡∏≤ t ‡∏à‡∏ô‡∏à‡∏ö**\n",
    "\n",
    "```\n",
    "G_t = R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T\n",
    "```\n",
    "\n",
    "‡πÅ‡∏ï‡πà‡πÉ‡∏ô Trading/Real-world:\n",
    "- üíµ **$100 ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ** ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ **$100 ‡∏õ‡∏µ‡∏´‡∏ô‡πâ‡∏≤** (time value of money)\n",
    "- üéØ **‡∏Å‡∏≥‡πÑ‡∏£‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ** ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Å‡∏ß‡πà‡∏≤ **‡∏Å‡∏≥‡πÑ‡∏£‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï** (uncertainty)\n",
    "\n",
    "‡πÄ‡∏•‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ **Discount Factor (Œ≥)**:\n",
    "\n",
    "```\n",
    "G_t = R_{t+1} + Œ≥*R_{t+2} + Œ≥¬≤*R_{t+3} + ... = Œ£ Œ≥·µè * R_{t+k+1}\n",
    "```\n",
    "\n",
    "### üéõÔ∏è ‡∏ú‡∏•‡∏Ç‡∏≠‡∏á Œ≥:\n",
    "\n",
    "- **Œ≥ = 0:** ‡∏°‡∏≠‡∏á‡πÅ‡∏Ñ‡πà immediate reward (‡∏™‡∏≤‡∏¢‡∏ï‡∏≤‡∏™‡∏±‡πâ‡∏ô)\n",
    "- **Œ≥ = 0.9:** ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á\n",
    "- **Œ≥ = 0.99:** ‡∏°‡∏≠‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å\n",
    "- **Œ≥ = 1:** reward ‡∏ó‡∏∏‡∏Å‡∏Å‡πâ‡∏≤‡∏ß‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô (infinite horizon)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"üì¶ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ ‡∏ó‡∏î‡∏•‡∏≠‡∏á: ‡∏ú‡∏•‡∏Ç‡∏≠‡∏á Discount Factor (Œ≥)\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ reward stream:\n",
    "```\n",
    "t=1: +1\n",
    "t=2: +2\n",
    "t=3: +3\n",
    "t=4: +4\n",
    "t=5: +5\n",
    "```\n",
    "\n",
    "Return ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Œ≥?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward stream\n",
    "rewards = [1, 2, 3, 4, 5]\n",
    "\n",
    "def calculate_return(rewards, gamma):\n",
    "    \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì discounted return\"\"\"\n",
    "    G = 0\n",
    "    for t, r in enumerate(rewards):\n",
    "        G += (gamma ** t) * r\n",
    "    return G\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö gamma ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "gammas = [0.0, 0.5, 0.9, 0.95, 0.99, 1.0]\n",
    "returns = []\n",
    "\n",
    "print(\"üé≤ Discount Factor Experiment\\n\" + \"=\"*50)\n",
    "for gamma in gammas:\n",
    "    G = calculate_return(rewards, gamma)\n",
    "    returns.append(G)\n",
    "    print(f\"Œ≥ = {gamma:.2f} ‚Üí Return = {G:.2f}\")\n",
    "\n",
    "print(\"\\nüìä ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(f\"- Œ≥=0.0: Return = {returns[0]:.2f} (‡∏°‡∏≠‡∏á‡πÅ‡∏Ñ‡πà‡∏Ç‡∏±‡πâ‡∏ô‡πÅ‡∏£‡∏Å)\")\n",
    "print(f\"- Œ≥=1.0: Return = {returns[-1]:.2f} (‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô)\")\n",
    "print(f\"- Difference: {returns[-1] - returns[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Gamma vs Return\n",
    "ax1.plot(gammas, returns, marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
    "ax1.set_xlabel('Discount Factor (gamma)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Total Return', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Effect of Discount Factor on Return', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=sum(rewards), color='red', linestyle='--', alpha=0.5, label='Undiscounted Sum')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Discounted Rewards over Time\n",
    "time_steps = range(1, len(rewards) + 1)\n",
    "for gamma in [0.5, 0.9, 0.99]:\n",
    "    discounted = [(gamma ** t) * r for t, r in enumerate(rewards)]\n",
    "    ax2.plot(time_steps, discounted, marker='o', label=f'Œ≥={gamma}')\n",
    "\n",
    "ax2.set_xlabel('Time Step', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Discounted Reward', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Discounted Rewards Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"- Œ≥ ‡∏ï‡πà‡∏≥: Reward ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß (agent ‡∏°‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô)\")\n",
    "print(\"- Œ≥ ‡∏™‡∏π‡∏á: Reward ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å (agent ‡∏°‡∏≠‡∏á‡∏¢‡∏≤‡∏ß)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Bellman Equation: ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á RL\n",
    "\n",
    "### üß† Value Function: V(s)\n",
    "\n",
    "**V(s) = Expected Return ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà state s**\n",
    "\n",
    "```\n",
    "V(s) = E[G_t | S_t = s]\n",
    "     = E[R_{t+1} + Œ≥*G_{t+1} | S_t = s]\n",
    "     = E[R_{t+1} + Œ≥*V(S_{t+1}) | S_t = s]  ‚Üê Bellman Equation!\n",
    "```\n",
    "\n",
    "### üîë ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:\n",
    "\n",
    "**Value ‡∏Ç‡∏≠‡∏á state ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô = Immediate Reward + Discounted Value ‡∏Ç‡∏≠‡∏á state ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ**\n",
    "\n",
    "```\n",
    "V(s) = Œ£ P(s'|s,a) * [R(s,a,s') + Œ≥ * V(s')]\n",
    "       ^^^^^^^^^^^^^^^^  ^^^^^^^^^^^^^^^^^^^^^^\n",
    "       ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô      Immediate + Future Value\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Simple MDP\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏£‡∏≤‡∏°‡∏µ 3 states:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  action  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  action  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  A  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  B  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Goal ‚îÇ\n",
    "‚îÇ r=0 ‚îÇ          ‚îÇ r=1 ‚îÇ          ‚îÇ r=10 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "- State A ‚Üí B: reward = 0\n",
    "- State B ‚Üí Goal: reward = 1\n",
    "- State Goal: reward = 10 (terminal)\n",
    "\n",
    "**‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì V(s) ‡∏î‡πâ‡∏ß‡∏¢ Bellman Equation (Œ≥ = 0.9):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMDP:\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        self.states = ['A', 'B', 'Goal']\n",
    "        \n",
    "        # Transition: {current_state: {action: (next_state, reward)}}\n",
    "        self.transitions = {\n",
    "            'A': {'right': ('B', 0)},\n",
    "            'B': {'right': ('Goal', 1)},\n",
    "            'Goal': {}  # Terminal state\n",
    "        }\n",
    "        \n",
    "        # Terminal reward\n",
    "        self.terminal_reward = 10\n",
    "    \n",
    "    def bellman_update(self, V):\n",
    "        \"\"\"Update value function ‡∏î‡πâ‡∏ß‡∏¢ Bellman Equation\"\"\"\n",
    "        new_V = V.copy()\n",
    "        \n",
    "        # V(Goal) = terminal reward\n",
    "        new_V['Goal'] = self.terminal_reward\n",
    "        \n",
    "        # V(B) = R(B‚ÜíGoal) + Œ≥ * V(Goal)\n",
    "        next_state, reward = self.transitions['B']['right']\n",
    "        new_V['B'] = reward + self.gamma * V[next_state]\n",
    "        \n",
    "        # V(A) = R(A‚ÜíB) + Œ≥ * V(B)\n",
    "        next_state, reward = self.transitions['A']['right']\n",
    "        new_V['A'] = reward + self.gamma * V[next_state]\n",
    "        \n",
    "        return new_V\n",
    "    \n",
    "    def value_iteration(self, num_iterations=10):\n",
    "        \"\"\"Iterative ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤ V(s)\"\"\"\n",
    "        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ V(s) = 0 ‡∏ó‡∏∏‡∏Å state\n",
    "        V = {s: 0 for s in self.states}\n",
    "        history = [V.copy()]\n",
    "        \n",
    "        print(\"üîÑ Value Iteration Process:\\n\" + \"=\"*60)\n",
    "        print(f\"Iteration 0: V(A)={V['A']:.2f}, V(B)={V['B']:.2f}, V(Goal)={V['Goal']:.2f}\")\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            V = self.bellman_update(V)\n",
    "            history.append(V.copy())\n",
    "            print(f\"Iteration {i+1}: V(A)={V['A']:.2f}, V(B)={V['B']:.2f}, V(Goal)={V['Goal']:.2f}\")\n",
    "        \n",
    "        return V, history\n",
    "\n",
    "# Run Value Iteration\n",
    "mdp = SimpleMDP(gamma=0.9)\n",
    "final_V, history = mdp.value_iteration(num_iterations=10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Converged Values:\")\n",
    "print(f\"V(A) = {final_V['A']:.2f}\")\n",
    "print(f\"V(B) = {final_V['B']:.2f}\")\n",
    "print(f\"V(Goal) = {final_V['Goal']:.2f}\")\n",
    "\n",
    "print(\"\\nüí° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:\")\n",
    "print(f\"- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà A ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ return ‡∏£‡∏ß‡∏° {final_V['A']:.2f}\")\n",
    "print(f\"- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà B ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ return ‡∏£‡∏ß‡∏° {final_V['B']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Value Convergence\n",
    "iterations = range(len(history))\n",
    "V_A = [h['A'] for h in history]\n",
    "V_B = [h['B'] for h in history]\n",
    "V_Goal = [h['Goal'] for h in history]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(iterations, V_A, marker='o', label='V(A)', linewidth=2, markersize=6)\n",
    "plt.plot(iterations, V_B, marker='s', label='V(B)', linewidth=2, markersize=6)\n",
    "plt.plot(iterations, V_Goal, marker='^', label='V(Goal)', linewidth=2, markersize=6)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Value Function V(s)', fontsize=12, fontweight='bold')\n",
    "plt.title('Bellman Equation: Value Function Convergence', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"- V(Goal) ‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á 10 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (terminal reward)\")\n",
    "print(\"- V(B) ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏à‡∏≤‡∏Å 0 ‚Üí ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö reward ‡∏à‡∏≤‡∏Å Goal\")\n",
    "print(\"- V(A) ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ä‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏Å‡∏• Goal\")\n",
    "print(\"- ‡∏Ñ‡πà‡∏≤ converge ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å ~5-7 iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé≤ MDP ‡∏Å‡∏±‡∏ö Stochastic Transitions\n",
    "\n",
    "‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏à‡∏£‡∏¥‡∏á action ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ deterministic ‡πÄ‡∏™‡∏°‡∏≠‡πÑ‡∏õ!\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Slippery Grid**\n",
    "\n",
    "```\n",
    "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏±‡πà‡∏á \"‡πÑ‡∏õ‡∏Ç‡∏ß‡∏≤\":\n",
    "- 80% ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ ‚Üí ‡πÑ‡∏õ‡∏Ç‡∏ß‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÜ ‚úÖ\n",
    "- 10% ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ ‚Üí ‡πÑ‡∏õ‡∏ö‡∏ô ‚¨ÜÔ∏è\n",
    "- 10% ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ ‚Üí ‡πÑ‡∏õ‡∏•‡πà‡∏≤‡∏á ‚¨áÔ∏è\n",
    "```\n",
    "\n",
    "Bellman Equation ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô:\n",
    "\n",
    "```\n",
    "V(s) = Œ£ P(s'|s,a) * [R(s,a,s') + Œ≥ * V(s')]\n",
    "     = 0.8 * [R + Œ≥*V(right)] + 0.1 * [R + Œ≥*V(up)] + 0.1 * [R + Œ≥*V(down)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMDP:\n",
    "    \"\"\"MDP ‡∏ó‡∏µ‡πà‡∏°‡∏µ stochastic transitions\"\"\"\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        self.states = ['Start', 'Middle', 'Good', 'Bad']\n",
    "        \n",
    "        # Transition probabilities: {state: {action: [(prob, next_state, reward)]}}\n",
    "        self.transitions = {\n",
    "            'Start': {\n",
    "                'forward': [\n",
    "                    (0.8, 'Middle', 0),   # 80% ‡πÑ‡∏õ‡∏ï‡∏£‡∏á\n",
    "                    (0.1, 'Good', 5),     # 10% ‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ!\n",
    "                    (0.1, 'Bad', -5)      # 10% ‡πÇ‡∏ä‡∏Ñ‡∏£‡πâ‡∏≤‡∏¢\n",
    "                ]\n",
    "            },\n",
    "            'Middle': {\n",
    "                'forward': [\n",
    "                    (0.7, 'Good', 5),\n",
    "                    (0.3, 'Bad', -5)\n",
    "                ]\n",
    "            },\n",
    "            'Good': {},   # Terminal\n",
    "            'Bad': {}     # Terminal\n",
    "        }\n",
    "        \n",
    "        self.terminal_values = {'Good': 10, 'Bad': -10}\n",
    "    \n",
    "    def bellman_update(self, V):\n",
    "        \"\"\"Update with stochastic Bellman equation\"\"\"\n",
    "        new_V = V.copy()\n",
    "        \n",
    "        # Terminal states\n",
    "        new_V['Good'] = self.terminal_values['Good']\n",
    "        new_V['Bad'] = self.terminal_values['Bad']\n",
    "        \n",
    "        # Non-terminal states\n",
    "        for state in ['Start', 'Middle']:\n",
    "            if state in self.transitions:\n",
    "                action = 'forward'\n",
    "                expected_value = 0\n",
    "                \n",
    "                for prob, next_state, reward in self.transitions[state][action]:\n",
    "                    expected_value += prob * (reward + self.gamma * V[next_state])\n",
    "                \n",
    "                new_V[state] = expected_value\n",
    "        \n",
    "        return new_V\n",
    "    \n",
    "    def value_iteration(self, num_iterations=15):\n",
    "        V = {s: 0 for s in self.states}\n",
    "        history = [V.copy()]\n",
    "        \n",
    "        print(\"üé≤ Stochastic MDP Value Iteration:\\n\" + \"=\"*70)\n",
    "        print(f\"Iter 0: V(Start)={V['Start']:.2f}, V(Middle)={V['Middle']:.2f}, V(Good)={V['Good']:.2f}, V(Bad)={V['Bad']:.2f}\")\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            V = self.bellman_update(V)\n",
    "            history.append(V.copy())\n",
    "            print(f\"Iter {i+1}: V(Start)={V['Start']:.2f}, V(Middle)={V['Middle']:.2f}, V(Good)={V['Good']:.2f}, V(Bad)={V['Bad']:.2f}\")\n",
    "        \n",
    "        return V, history\n",
    "\n",
    "# Run Stochastic MDP\n",
    "stoch_mdp = StochasticMDP(gamma=0.9)\n",
    "final_V_stoch, history_stoch = stoch_mdp.value_iteration(num_iterations=15)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Final Values:\")\n",
    "print(f\"V(Start) = {final_V_stoch['Start']:.2f}\")\n",
    "print(f\"V(Middle) = {final_V_stoch['Middle']:.2f}\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(f\"- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà Start ‚Üí Expected return = {final_V_stoch['Start']:.2f}\")\n",
    "print(\"- ‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ß‡∏Å ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£\")\n",
    "print(\"- ‡πÅ‡∏°‡πâ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ 10-30% ‡πÑ‡∏õ Bad ‡πÅ‡∏ï‡πà probability ‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÑ‡∏õ‡∏ó‡∏≤‡∏á Good ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Stochastic MDP\n",
    "iterations = range(len(history_stoch))\n",
    "V_Start = [h['Start'] for h in history_stoch]\n",
    "V_Middle = [h['Middle'] for h in history_stoch]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ax.plot(iterations, V_Start, marker='o', label='V(Start)', linewidth=2, markersize=7, color='#2E86AB')\n",
    "ax.plot(iterations, V_Middle, marker='s', label='V(Middle)', linewidth=2, markersize=7, color='#A23B72')\n",
    "ax.axhline(y=10, color='green', linestyle='--', alpha=0.5, label='V(Good)')\n",
    "ax.axhline(y=-10, color='red', linestyle='--', alpha=0.5, label='V(Bad)')\n",
    "\n",
    "ax.set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Value Function V(s)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Stochastic MDP: Value Convergence', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(\"- Value functions converge ‡∏î‡πâ‡∏ß‡∏¢ expected value ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å possible outcomes\")\n",
    "print(\"- Stochastic transitions ‡∏ó‡∏≥‡πÉ‡∏´‡πâ MDP realistic ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\")\n",
    "print(\"- ‡πÉ‡∏ô Trading: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡πÅ‡∏ö‡∏ö stochastic ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏ô‡∏µ‡πâ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: Effect of Gamma (Œ≥)\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ß‡πà‡∏≤ discount factor ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ value function ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different gamma values\n",
    "gammas_to_test = [0.5, 0.9, 0.95, 0.99]\n",
    "results = {}\n",
    "\n",
    "print(\"üéõÔ∏è Comparing Different Gamma Values:\\n\" + \"=\"*60)\n",
    "\n",
    "for gamma in gammas_to_test:\n",
    "    mdp_test = SimpleMDP(gamma=gamma)\n",
    "    final_V, _ = mdp_test.value_iteration(num_iterations=10)\n",
    "    results[gamma] = final_V\n",
    "    print(f\"\\nŒ≥ = {gamma}:\")\n",
    "    print(f\"  V(A) = {final_V['A']:.2f}\")\n",
    "    print(f\"  V(B) = {final_V['B']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gamma comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "states_to_plot = ['A', 'B']\n",
    "x = np.arange(len(gammas_to_test))\n",
    "width = 0.35\n",
    "\n",
    "V_A_values = [results[g]['A'] for g in gammas_to_test]\n",
    "V_B_values = [results[g]['B'] for g in gammas_to_test]\n",
    "\n",
    "ax.bar(x - width/2, V_A_values, width, label='V(A)', color='#2E86AB')\n",
    "ax.bar(x + width/2, V_B_values, width, label='V(B)', color='#A23B72')\n",
    "\n",
    "ax.set_xlabel('Discount Factor (gamma)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Value Function', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Effect of Gamma on Value Functions', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{g}' for g in gammas_to_test])\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"- Œ≥ ‡∏™‡∏π‡∏á ‚Üí Value functions ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô (‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)\")\n",
    "print(\"- Œ≥ ‡∏ï‡πà‡∏≥ ‚Üí Value functions ‡∏ï‡πà‡∏≥‡∏•‡∏á (‡∏°‡∏≠‡∏á‡πÅ‡∏Ñ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÜ)\")\n",
    "print(\"- ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Œ≥ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö problem:\")\n",
    "print(\"  * Day Trading ‚Üí Œ≥ ‡∏ï‡πà‡∏≥ (‡∏°‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô)\")\n",
    "print(\"  * Long-term Investment ‚Üí Œ≥ ‡∏™‡∏π‡∏á (‡∏°‡∏≠‡∏á‡∏¢‡∏≤‡∏ß)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ: MDP Concepts\n",
    "\n",
    "### 1Ô∏è‚É£ Markov Property\n",
    "- ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "- P(S_{t+1} | S_t, history) = P(S_{t+1} | S_t)\n",
    "\n",
    "### 2Ô∏è‚É£ MDP Components\n",
    "- **S**: States\n",
    "- **A**: Actions\n",
    "- **P**: Transition probabilities\n",
    "- **R**: Reward function\n",
    "- **Œ≥**: Discount factor\n",
    "\n",
    "### 3Ô∏è‚É£ Return & Discount\n",
    "- Return = Œ£ Œ≥·µè * R_{t+k+1}\n",
    "- Œ≥ ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏ß‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô\n",
    "\n",
    "### 4Ô∏è‚É£ Bellman Equation\n",
    "- V(s) = E[R + Œ≥ * V(s')]\n",
    "- ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á Value Function calculation\n",
    "- Recursive relationship\n",
    "\n",
    "### 5Ô∏è‚É£ Value Iteration\n",
    "- Update V(s) iteratively ‡∏î‡πâ‡∏ß‡∏¢ Bellman equation\n",
    "- Converge ‡πÑ‡∏õ‡∏™‡∏π‡πà true value function\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "1. **Notebook 03:** Value Functions (V ‡πÅ‡∏•‡∏∞ Q)\n",
    "2. **Notebook 04:** Policies (Deterministic vs Stochastic)\n",
    "3. **Notebook 05:** Exploration vs Exploitation\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î\n",
    "\n",
    "### Exercise 1: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Return\n",
    "Reward sequence: [2, 3, 4, 5, 6]\n",
    "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì return ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Œ≥ = 0.8\n",
    "\n",
    "### Exercise 2: Bellman Update\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á MDP 4 states: Start ‚Üí A ‚Üí B ‚Üí Goal\n",
    "- Rewards: 0, 1, 2, 10\n",
    "- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì value function ‡∏î‡πâ‡∏ß‡∏¢ value iteration (Œ≥=0.9)\n",
    "\n",
    "### Exercise 3: Stochastic Transitions\n",
    "‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö MDP ‡∏ó‡∏µ‡πà‡∏°‡∏µ stochastic transitions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö trading scenario:\n",
    "- States: No Position, Long Position, Short Position\n",
    "- Actions: Buy, Sell, Hold\n",
    "- Probability: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô 60%, ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏•‡∏á 40%\n",
    "\n",
    "---\n",
    "\n",
    "## üìö ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
    "\n",
    "- [Sutton & Barto: Reinforcement Learning Book](http://incompleteideas.net/book/the-book.html)\n",
    "- [David Silver RL Course Lecture 2: MDPs](https://www.youtube.com/watch?v=lfHX2hHRMVQ)\n",
    "- [UC Berkeley CS188: MDPs](https://inst.eecs.berkeley.edu/~cs188/sp21/)\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Ready to learn about Value Functions?**\n",
    "\n",
    "üëâ [Next: 03_value_functions.ipynb](03_value_functions.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
