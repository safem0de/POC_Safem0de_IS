{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üé≤ Markov Decision Process (MDP)\n## ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Reinforcement Learning ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Å‡πâ‡∏≤‡∏ß\n\n---\n\n## üö∂ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÜ: ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n\n‡∏•‡∏≠‡∏á‡∏ô‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏∑‡πà‡∏ô‡∏ô‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏ä‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á**‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®** üè¢\n\n### üó∫Ô∏è ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:\n\n```\nüè† ‡∏ö‡πâ‡∏≤‡∏ô ‚Üí üö∂ ‡πÄ‡∏î‡∏¥‡∏ô ‚Üí üöá ‡∏£‡∏ñ‡πÑ‡∏ü ‚Üí üö∂ ‡πÄ‡∏î‡∏¥‡∏ô ‚Üí üè¢ ‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®\n```\n\n**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?\n\n### ü§î ‡∏™‡∏≠‡∏á‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏¥‡∏î:\n\n#### ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 1: ‡∏à‡∏î‡∏à‡∏≥‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á (Non-Markov)\n```\n\"‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô‡∏ù‡∏ô‡∏ï‡∏Å ‚Üí ‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏£‡∏ñ‡∏ï‡∏¥‡∏î ‚Üí ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏•‡πà‡∏≤‡∏ä‡πâ‡∏≤...\"\n‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≥‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï ‚Üí ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ!\n```\n\n#### ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 2: ‡∏î‡∏π‡πÅ‡∏Ñ‡πà‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (Markov)\n```\n\"‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô + ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£ + ‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ\"\n‚úÖ ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≠‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÑ‡∏î‡πâ!\n```\n\n### üéØ ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\n- ‡∏Ñ‡∏∏‡∏ì**‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ** ‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô/‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£\n- ‡πÅ‡∏Ñ‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ **\"‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ\"** ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô ‡∏Å‡πá‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß\n- ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ **Markov Property!** üéâ\n\n---\n\n## üß† Markov Property ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£? (‡∏á‡πà‡∏≤‡∏¢‡πÜ)\n\n### üìå ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°:\n**\"‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\"**\n\n### üéÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡πÜ:\n\n#### ‚úÖ ‡∏°‡∏µ Markov Property:\n\n1. **‡∏´‡∏°‡∏≤‡∏Å‡∏£‡∏∏‡∏Å ‚ôüÔ∏è**\n   - ‡∏°‡∏≠‡∏á‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‚Üí ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á\n   - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ 10 ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏∞‡πÑ‡∏£\n\n2. **‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á GPS üìç**\n   - ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‚Üí ‡πÑ‡∏õ‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ\n   - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏´‡∏ô\n\n3. **‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡πÉ‡∏ô‡∏ö‡∏±‡∏ç‡∏ä‡∏µ üí∞**\n   - ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£ ‚Üí ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á\n   - ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏¥‡∏ô‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏´‡∏ô\n\n#### ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ Markov Property:\n\n1. **‡πÇ‡∏£‡∏Ñ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß üè•**\n   - ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á\n   - ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÑ‡∏°‡πà‡∏û‡∏≠\n\n2. **‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß üåç**\n   - ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏µ\n   - ‡πÅ‡∏Ñ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠\n\n---\n\n## üìà Markov Property ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î\n\n### üíπ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô\n\n‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏£‡∏î‡πÄ‡∏î‡∏≠‡∏£‡πå:\n\n```\nüìä ‡∏Å‡∏£‡∏≤‡∏ü‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô:\n100 ‚Üí 105 ‚Üí 110 ‚Üí 108 ‚Üí ??? (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ)\n```\n\n**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô/‡∏•‡∏á ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡∏≠‡∏∞‡πÑ‡∏£?\n\n#### üîµ Efficient Market Hypothesis (EMH) - ‡πÅ‡∏ö‡∏ö Markov:\n```\n\"‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ = 108 ‡∏ö‡∏≤‡∏ó\"\n‚Üí ‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß\n‚Üí ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n```\n\n#### üî¥ Technical Analysis - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Markov:\n```\n\"‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≤‡∏Å 100‚Üí110 ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡∏°‡∏≤ 108\"\n‚Üí ‡∏≠‡∏≤‡∏à‡∏°‡∏µ pattern (Head & Shoulders, etc.)\n‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á\n```\n\n### üí° ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RL:\n‡πÄ‡∏£‡∏≤‡∏à‡∏∞**‡∏™‡∏°‡∏°‡∏ï‡∏¥** ‡∏ß‡πà‡∏≤ environment ‡∏°‡∏µ Markov Property\n‚Üí ‡∏ó‡∏≥‡πÉ‡∏´‡πâ model ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Å‡∏£‡∏ì‡∏µ\n\n---\n\n## üéØ ‡∏™‡∏£‡∏∏‡∏õ Markov Property ‡∏á‡πà‡∏≤‡∏¢‡πÜ:\n\n| ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á |\n|--------|----------|----------|\n| **Markov** | ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô | ‡∏´‡∏°‡∏≤‡∏Å‡∏£‡∏∏‡∏Å, GPS, ‡πÄ‡∏Å‡∏° |\n| **Non-Markov** | ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á | ‡πÇ‡∏£‡∏Ñ, ‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß, ‡∏†‡∏≤‡∏©‡∏≤ |\n\n### üìê ‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:\n```\nP(S_{t+1} | S_t, S_{t-1}, ..., S_0) = P(S_{t+1} | S_t)\n        ‚îî‚îÄ‚îÄ ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ ‡∏≠‡∏î‡∏µ‡∏ï‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ ‡πÅ‡∏Ñ‡πà‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‚îÄ‚îÄ‚îò\n```\n\n**‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏Ñ‡∏ô:**\n\"‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ï‡πà‡∏≠‡πÑ‡∏õ = ‡∏°‡∏≠‡∏á‡πÅ‡∏Ñ‡πà‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡πá‡∏û‡∏≠\"\n\n---\n\n## üîú ‡∏ï‡πà‡∏≠‡πÑ‡∏õ: MDP ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n\n‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Markov Property ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°:\n- **A** = Actions (‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥)\n- **R** = Rewards (‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•)\n- **P** = Probabilities (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô)\n\n‚Üí ‡πÑ‡∏î‡πâ **Markov Decision Process (MDP)** ‚¨áÔ∏è"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import FancyBboxPatch, FancyArrowPatch, Circle\nfrom matplotlib.patches import Rectangle, FancyArrow\nfrom IPython.display import display, HTML\n\nplt.rcParams['font.family'] = 'DejaVu Sans'\nsns.set_style('whitegrid')\n\nprint(\"üì¶ Libraries loaded successfully!\")\nprint(\"‚úÖ Ready to visualize MDP concepts!\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üí∞ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Discount Factor (Œ≥) ‡πÅ‡∏ö‡∏ö‡∏•‡∏∂‡∏Å\n\n### ü§î ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Discount Factor?\n\n**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• 100 ‡∏ö‡∏≤‡∏ó‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ vs 100 ‡∏ö‡∏≤‡∏ó 1 ‡∏õ‡∏µ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‡∏≠‡∏±‡∏ô‡πÑ‡∏´‡∏ô‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤?\n\n**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:** ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô! ‡πÄ‡∏û‡∏£‡∏≤‡∏∞:\n1. üíµ **Time Value of Money** - ‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢\n2. üé≤ **Uncertainty** - ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢\n3. üèÉ **Opportunity Cost** - ‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏≠‡∏∑‡πà‡∏ô‡πÜ\n\n### üìä Discount Factor ‡πÉ‡∏ô 3 ‡πÇ‡∏•‡∏Å:\n\n#### 1Ô∏è‚É£ **‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô (Finance)**\n```\nPresent Value = Future Value / (1 + r)^t\n\n‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:\n100 ‡∏ö‡∏≤‡∏ó 1 ‡∏õ‡∏µ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ (‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢ 10%)\n‚Üí ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô = 100 / (1.1) = 90.91 ‡∏ö‡∏≤‡∏ó\n```\n\n#### 2Ô∏è‚É£ **Reinforcement Learning**\n```\nDiscounted Reward = Œ≥^t √ó Reward\n\n‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (Œ≥ = 0.9):\n100 ‡∏ö‡∏≤‡∏ó 1 ‡∏Å‡πâ‡∏≤‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤\n‚Üí ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô = 0.9 √ó 100 = 90 ‡∏ö‡∏≤‡∏ó\n```\n\n#### 3Ô∏è‚É£ **Trading**\n```\n‡∏Å‡∏≥‡πÑ‡∏£ 1,000 ‡∏ö‡∏≤‡∏ó‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ vs 1,000 ‡∏ö‡∏≤‡∏ó 1 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤\n\nDay Trader (Œ≥ = 0.5):\n‚Üí ‡∏Å‡∏≥‡πÑ‡∏£‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ = 1,000 ‡∏ö‡∏≤‡∏ó\n‚Üí ‡∏Å‡∏≥‡πÑ‡∏£ 30 ‡∏ß‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ = 0.5^30 √ó 1,000 ‚âà 0 ‡∏ö‡∏≤‡∏ó (‡∏°‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô!)\n\nLong-term Investor (Œ≥ = 0.99):\n‚Üí ‡∏Å‡∏≥‡πÑ‡∏£‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ = 1,000 ‡∏ö‡∏≤‡∏ó\n‚Üí ‡∏Å‡∏≥‡πÑ‡∏£ 30 ‡∏ß‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ = 0.99^30 √ó 1,000 ‚âà 740 ‡∏ö‡∏≤‡∏ó (‡∏°‡∏≠‡∏á‡∏¢‡∏≤‡∏ß!)\n```\n\n---\n\n## üéØ Return (G) ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n\n**Return = ‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ**\n\n### üìê ‡∏™‡∏π‡∏ï‡∏£:\n\n```\nG_t = R_{t+1} + Œ≥*R_{t+2} + Œ≥¬≤*R_{t+3} + Œ≥¬≥*R_{t+4} + ...\n    = Œ£ (k=0 to ‚àû) Œ≥^k √ó R_{t+k+1}\n```\n\n**‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏Ñ‡∏ô:**\n- R_{t+1} = ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡πÉ‡∏ô‡∏Å‡πâ‡∏≤‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏°‡πá‡∏î‡πÄ‡∏ï‡πá‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢)\n- Œ≥*R_{t+2} = ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡πÉ‡∏ô 2 ‡∏Å‡πâ‡∏≤‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ (‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏•‡∏á)\n- Œ≥¬≤*R_{t+3} = ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡πÉ‡∏ô 3 ‡∏Å‡πâ‡∏≤‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ (‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)\n\n### üßÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:\n\n‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÑ‡∏î‡πâ reward stream:\n```\nt=1: +10\nt=2: +20\nt=3: +30\nt=4: +40\nt=5: +50\n```\n\n#### ‡∏Å‡∏£‡∏ì‡∏µ Œ≥ = 1.0 (‡πÑ‡∏°‡πà‡∏°‡∏µ discount):\n```\nG = 10 + 20 + 30 + 40 + 50 = 150\n```\n\n#### ‡∏Å‡∏£‡∏ì‡∏µ Œ≥ = 0.9:\n```\nG = 10 + (0.9√ó20) + (0.9¬≤√ó30) + (0.9¬≥√ó40) + (0.9‚Å¥√ó50)\n  = 10 + 18 + 24.3 + 29.16 + 32.81\n  = 114.27\n```\n\n#### ‡∏Å‡∏£‡∏ì‡∏µ Œ≥ = 0.5 (‡∏°‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô):\n```\nG = 10 + (0.5√ó20) + (0.5¬≤√ó30) + (0.5¬≥√ó40) + (0.5‚Å¥√ó50)\n  = 10 + 10 + 7.5 + 5 + 3.125\n  = 35.625\n```\n\n**‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:** Œ≥ ‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥ ‚Üí Return ‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)\n\n---\n\n## üéÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÄ‡∏Å‡∏° Treasure Hunt\n\n‡∏°‡∏µ 2 ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á:\n\n### üõ§Ô∏è ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á A: ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (Greedy)\n```\n‡∏Å‡πâ‡∏≤‡∏ß‡∏ó‡∏µ‡πà 1: +100 ‡∏ö‡∏≤‡∏ó\n‡∏Å‡πâ‡∏≤‡∏ß‡∏ó‡∏µ‡πà 2: +0\n‡∏Å‡πâ‡∏≤‡∏ß‡∏ó‡∏µ‡πà 3: +0\nTotal = 100\n```\n\n### üõ§Ô∏è ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á B: ‡∏£‡∏≠‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å (Patient)\n```\n‡∏Å‡πâ‡∏≤‡∏ß‡∏ó‡∏µ‡πà 1: +0\n‡∏Å‡πâ‡∏≤‡∏ß‡∏ó‡∏µ‡πà 2: +0\n‡∏Å‡πâ‡∏≤‡∏ß‡∏ó‡∏µ‡πà 3: +200 ‡∏ö‡∏≤‡∏ó\nTotal = 200\n```\n\n**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏´‡∏ô?\n\n#### ‡∏ñ‡πâ‡∏≤ Œ≥ = 0.5 (‡∏°‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô):\n```\nReturn_A = 100 + 0 + 0 = 100\nReturn_B = 0 + 0 + (0.5¬≤ √ó 200) = 0 + 0 + 50 = 50\n‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å A! ‚úÖ\n```\n\n#### ‡∏ñ‡πâ‡∏≤ Œ≥ = 0.95 (‡∏°‡∏≠‡∏á‡∏¢‡∏≤‡∏ß):\n```\nReturn_A = 100 + 0 + 0 = 100\nReturn_B = 0 + 0 + (0.95¬≤ √ó 200) = 0 + 0 + 180.5 = 180.5\n‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å B! ‚úÖ\n```\n\n**‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô:**\n- Œ≥ ‡∏ï‡πà‡∏≥ ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (impatient agent)\n- Œ≥ ‡∏™‡∏π‡∏á ‚Üí ‡∏¢‡∏≠‡∏°‡∏£‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡πÉ‡∏´‡∏ç‡πà (patient agent)\n\n---\n\n## üìà ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Œ≥ ‡πÉ‡∏ô Trading\n\n| Trading Style | Œ≥ ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° | ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• |\n|--------------|-------------|--------|\n| **Scalping** (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ-‡∏ô‡∏≤‡∏ó‡∏µ) | 0.1 - 0.5 | ‡∏°‡∏≠‡∏á‡∏Å‡∏≥‡πÑ‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ, ‡πÑ‡∏°‡πà‡∏™‡∏ô trend ‡∏¢‡∏≤‡∏ß |\n| **Day Trading** | 0.5 - 0.7 | ‡∏°‡∏≠‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ß‡∏±‡∏ô, ‡∏õ‡∏¥‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏¥‡∏î‡∏ï‡∏•‡∏≤‡∏î |\n| **Swing Trading** (‡∏ß‡∏±‡∏ô-‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå) | 0.8 - 0.9 | ‡∏ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏° overnight, ‡∏°‡∏≠‡∏á pattern |\n| **Position Trading** (‡πÄ‡∏î‡∏∑‡∏≠‡∏ô) | 0.95 - 0.99 | ‡∏°‡∏≠‡∏á trend ‡∏¢‡∏≤‡∏ß, ‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à noise |\n| **Long-term Investment** | 0.99 - 0.999 | ‡∏ñ‡∏∑‡∏≠‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏µ, ‡∏°‡∏≠‡∏á fundamental |\n\n### üí° ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á:\n\n**Œ≥ ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ:**\n- ‡∏°‡∏≠‡∏á‡πÅ‡∏Ñ‡πà‡∏Å‡∏≥‡πÑ‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n- ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ big move\n- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏™‡∏π‡∏á\n\n**Œ≥ ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ:**\n- ‡∏°‡∏≠‡∏á‡πÑ‡∏Å‡∏•‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n- ‡∏≠‡∏≤‡∏à‡∏Ç‡∏≤‡∏î‡∏ó‡∏∏‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏≤‡∏á\n- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á\n\n---\n\n## üî¨ ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î!\n\n‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤ Œ≥ ‡πÅ‡∏•‡∏∞‡∏î‡∏π‡∏ú‡∏•‡∏ï‡πà‡∏≠ Return ‚¨áÔ∏è",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ ‡∏ó‡∏î‡∏•‡∏≠‡∏á: ‡∏ú‡∏•‡∏Ç‡∏≠‡∏á Discount Factor (Œ≥)\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ reward stream:\n",
    "```\n",
    "t=1: +1\n",
    "t=2: +2\n",
    "t=3: +3\n",
    "t=4: +4\n",
    "t=5: +5\n",
    "```\n",
    "\n",
    "Return ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Œ≥?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward stream\n",
    "rewards = [1, 2, 3, 4, 5]\n",
    "\n",
    "def calculate_return(rewards, gamma):\n",
    "    \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì discounted return\"\"\"\n",
    "    G = 0\n",
    "    for t, r in enumerate(rewards):\n",
    "        G += (gamma ** t) * r\n",
    "    return G\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö gamma ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "gammas = [0.0, 0.5, 0.9, 0.95, 0.99, 1.0]\n",
    "returns = []\n",
    "\n",
    "print(\"üé≤ Discount Factor Experiment\\n\" + \"=\"*50)\n",
    "for gamma in gammas:\n",
    "    G = calculate_return(rewards, gamma)\n",
    "    returns.append(G)\n",
    "    print(f\"Œ≥ = {gamma:.2f} ‚Üí Return = {G:.2f}\")\n",
    "\n",
    "print(\"\\nüìä ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(f\"- Œ≥=0.0: Return = {returns[0]:.2f} (‡∏°‡∏≠‡∏á‡πÅ‡∏Ñ‡πà‡∏Ç‡∏±‡πâ‡∏ô‡πÅ‡∏£‡∏Å)\")\n",
    "print(f\"- Œ≥=1.0: Return = {returns[-1]:.2f} (‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô)\")\n",
    "print(f\"- Difference: {returns[-1] - returns[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## üèÜ Bellman Equation: ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏®‡∏©‡∏Ç‡∏≠‡∏á RL\n\n### üéØ ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ:\n\n**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** ‡∏à‡∏∞‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ß‡πà‡∏≤ state ‡∏ô‡∏µ‡πâ \"‡∏î‡∏µ\" ‡∏´‡∏£‡∏∑‡∏≠ \"‡πÅ‡∏¢‡πà\"?\n\n**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**\n```\nü§ñ ‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏≠‡∏á A\nü§î ‡∏ä‡πà‡∏≠‡∏á A ‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?\n‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏à‡∏≤‡∏Å A ‡πÑ‡∏õ‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ reward ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£\n```\n\n### üí° ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:\n\n**‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á state = ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ + ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á state ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ**\n\n‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ **Bellman Equation!**\n\n---\n\n## üìä Value Function V(s)\n\n### üìå ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°:\n\n**V(s) = ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á Return ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà state s**\n\n### üßÆ ‡∏™‡∏π‡∏ï‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô:\n\n```\nV(s) = E[G_t | S_t = s]\n     = E[R_{t+1} + Œ≥*R_{t+2} + Œ≥¬≤*R_{t+3} + ... | S_t = s]\n```\n\n**‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏Ñ‡∏ô:**\n\"‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà state s ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡πà‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡∏à‡∏∞‡πÑ‡∏î‡πâ return ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£?\"\n\n---\n\n## üîë Bellman Equation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö V(s)\n\n### üìê ‡∏™‡∏π‡∏ï‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:\n\n```\nV(s) = E[R_{t+1} + Œ≥*G_{t+1} | S_t = s]\n     = E[R_{t+1} + Œ≥*V(S_{t+1}) | S_t = s]  ‚Üê Bellman Equation!\n```\n\n### üí≠ ‡∏ó‡∏≥‡πÑ‡∏° recursive ‡πÑ‡∏î‡πâ?\n\n```\nG_t = R_{t+1} + Œ≥*R_{t+2} + Œ≥¬≤*R_{t+3} + ...\n    = R_{t+1} + Œ≥*(R_{t+2} + Œ≥*R_{t+3} + ...)\n    = R_{t+1} + Œ≥*G_{t+1}  ‚Üê ‡πÅ‡∏¢‡∏Å‡πÑ‡∏î‡πâ!\n    \n‡πÅ‡∏ï‡πà G_{t+1} = V(S_{t+1}) (by definition)\n\n‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô:\nV(s) = E[R_{t+1} + Œ≥*V(S_{t+1})]\n```\n\n---\n\n## üéÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡πÜ: ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ï‡∏£‡∏á\n\n‡∏°‡∏µ 3 ‡∏ä‡πà‡∏≠‡∏á ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô‡∏ï‡∏£‡∏á‡πÜ:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   R=0    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   R=1    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  A  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  B  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Goal ‚îÇ\n‚îÇ     ‚îÇ          ‚îÇ     ‚îÇ          ‚îÇ R=10 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏´‡∏≤ V(A), V(B), V(Goal) ‡πÇ‡∏î‡∏¢ Œ≥ = 0.9\n\n### üîç ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Ñ‡∏¥‡∏î‡πÅ‡∏ö‡∏ö step-by-step:\n\n#### Step 1: ‡∏´‡∏≤ V(Goal)\n```\nGoal ‡πÄ‡∏õ‡πá‡∏ô terminal state ‚Üí ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï\nV(Goal) = 10 (‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ï‡∏≠‡∏ô‡∏à‡∏ö)\n```\n\n#### Step 2: ‡∏´‡∏≤ V(B)\n```\n‡∏à‡∏≤‡∏Å B ‚Üí Goal ‡πÑ‡∏î‡πâ reward = 1\nV(B) = R(B‚ÜíGoal) + Œ≥ √ó V(Goal)\n     = 1 + 0.9 √ó 10\n     = 1 + 9\n     = 10\n```\n\n**‡πÅ‡∏õ‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:**\n- ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà B ‚Üí ‡πÄ‡∏î‡∏¥‡∏ô‡πÑ‡∏õ Goal ‡πÑ‡∏î‡πâ reward 1\n- ‡πÅ‡∏•‡πâ‡∏ß Goal ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ 10\n- ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á discount ‡∏î‡πâ‡∏ß‡∏¢ 0.9\n- ‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ 10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô\n\n#### Step 3: ‡∏´‡∏≤ V(A)\n```\n‡∏à‡∏≤‡∏Å A ‚Üí B ‡πÑ‡∏î‡πâ reward = 0\nV(A) = R(A‚ÜíB) + Œ≥ √ó V(B)\n     = 0 + 0.9 √ó 10\n     = 9\n```\n\n**‡πÅ‡∏õ‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:**\n- ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà A ‚Üí ‡πÑ‡∏õ B ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ reward\n- ‡πÅ‡∏ï‡πà B ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ 10\n- discount ‡∏î‡πâ‡∏ß‡∏¢ 0.9\n- ‡πÑ‡∏î‡πâ 9 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô\n\n### ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ:\n```\nV(A) = 9\nV(B) = 10\nV(Goal) = 10\n```\n\n**‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:** A ‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ B ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏Å‡∏• Goal ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤!\n\n---\n\n## üé≤ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà 2: Stochastic MDP\n\n‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô!\n\n```\n         ‚îå‚îÄ‚îÄ‚Üí Good (+5)  [60%]\n         ‚îÇ\nStart ‚îÄ‚îÄ‚îÄ‚î§\n         ‚îÇ\n         ‚îî‚îÄ‚îÄ‚Üí Bad (-3)   [40%]\n```\n\n**‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏´‡∏≤ V(Start) ‡πÇ‡∏î‡∏¢ Œ≥ = 1 (‡πÑ‡∏°‡πà‡∏°‡∏µ discount)\n\n### üßÆ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:\n\n```\nV(Start) = Œ£ P(s'|Start) √ó [R(Start‚Üís') + Œ≥ √ó V(s')]\n         = 0.6 √ó [5 + 1√óV(Good)] + 0.4 √ó [-3 + 1√óV(Bad)]\n```\n\n‡∏™‡∏°‡∏°‡∏ï‡∏¥ Good ‡πÅ‡∏•‡∏∞ Bad ‡πÄ‡∏õ‡πá‡∏ô terminal:\n```\nV(Good) = 5\nV(Bad) = -3\n\nV(Start) = 0.6 √ó [5 + 5] + 0.4 √ó [-3 + (-3)]\n         = 0.6 √ó 10 + 0.4 √ó (-6)\n         = 6 - 2.4\n         = 3.6\n```\n\n**‡πÅ‡∏õ‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:**\n- ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ 60% ‡πÑ‡∏î‡πâ 10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô\n- ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ 40% ‡πÄ‡∏™‡∏µ‡∏¢ 6 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô\n- ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ 3.6 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (positive!)\n\n---\n\n## üîÑ Value Iteration Algorithm\n\n### üí° ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:\n\n‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ V(s) ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á ‚Üí ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ **iterative update**\n\n### üìã ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:\n\n```\n1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: V(s) = 0 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å state\n2. Repeat:\n     For each state s:\n         V_new(s) = E[R + Œ≥ √ó V_old(s')]\n3. ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ V ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (converge)\n```\n\n### üéØ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ï‡∏£‡∏á (A ‚Üí B ‚Üí Goal)\n\n| Iteration | V(A) | V(B) | V(Goal) |\n|-----------|------|------|---------|\n| 0 | 0.00 | 0.00 | 10.00 (fixed) |\n| 1 | 0.00 | 10.00 | 10.00 |\n| 2 | 9.00 | 10.00 | 10.00 |\n| 3 | 9.00 | 10.00 | 10.00 ‚úÖ |\n\n**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**\n- Iteration 0: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á = 0 ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô Goal\n- Iteration 1: B ‡∏£‡∏±‡∏ö‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ï‡∏¥‡∏î Goal ‚Üí V(B) = 1 + 0.9√ó10 = 10\n- Iteration 2: A ‡∏£‡∏±‡∏ö‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ï‡∏¥‡∏î B ‚Üí V(A) = 0 + 0.9√ó10 = 9\n- Iteration 3: Converge!\n\n---\n\n## üéì ‡∏™‡∏£‡∏∏‡∏õ Bellman Equation\n\n### üìå Key Points:\n\n1. **V(s) = Immediate Reward + Discounted Future Value**\n   ```\n   V(s) = E[R + Œ≥ √ó V(s')]\n   ```\n\n2. **Recursive Structure:**\n   - ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á state ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á state ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n   - ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ Fibonacci: F(n) = F(n-1) + F(n-2)\n\n3. **Value Iteration:**\n   - Update V(s) ‡∏ã‡πâ‡∏≥‡πÜ ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞ converge\n   - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å terminal states ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ propagate ‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö\n\n4. **Stochastic Environment:**\n   - ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô: Œ£ P(s'|s,a) √ó [R + Œ≥√óV(s')]\n   - ‡πÉ‡∏ä‡πâ expected value\n\n---\n\n## üí™ ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î\n\n### Exercise 1:\n‡∏°‡∏µ 4 states: S ‚Üí A ‚Üí B ‚Üí Goal\n\nRewards:\n- S ‚Üí A: +1\n- A ‚Üí B: +2\n- B ‚Üí Goal: +3\n- Goal: +10\n\n‡∏´‡∏≤ V(S), V(A), V(B) ‡πÇ‡∏î‡∏¢ Œ≥ = 0.9\n\n### Exercise 2:\nStochastic case:\n```\nS ‚Üí [50% ‚Üí Good (+10), 50% ‚Üí Bad (-5)]\n```\n‡∏´‡∏≤ V(S) ‡πÇ‡∏î‡∏¢ Œ≥ = 1\n\n---\n\n## üîú ‡∏ï‡πà‡∏≠‡πÑ‡∏õ: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏£‡∏¥‡∏á!\n\n‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Bellman Equation ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏Å‡∏±‡∏ô ‚¨áÔ∏è"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\"\"\"\nüéì SimpleMDP: ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏Ç‡∏≠‡∏á MDP ‡πÅ‡∏•‡∏∞ Value Iteration\n\n‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á MDP:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   R=0    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   R=1    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  A  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  B  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Goal ‚îÇ\n‚îÇ     ‚îÇ          ‚îÇ     ‚îÇ          ‚îÇ R=10 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n‡∏ó‡∏î‡∏™‡∏≠‡∏ö Bellman Equation ‡πÅ‡∏•‡∏∞ Value Iteration\n\"\"\"\n\nclass SimpleMDP:\n    def __init__(self, gamma=0.9):\n        \"\"\"\n        ‡∏™‡∏£‡πâ‡∏≤‡∏á Simple MDP\n        \n        Parameters:\n        -----------\n        gamma : float (0-1)\n            Discount factor (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á = ‡∏°‡∏≠‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)\n        \"\"\"\n        self.gamma = gamma\n        self.states = ['A', 'B', 'Goal']\n        \n        # Transition: {current_state: {action: (next_state, reward)}}\n        # ‡πÄ‡∏Å‡πá‡∏ö‡∏ß‡πà‡∏≤‡∏à‡∏≤‡∏Å state ‡∏ô‡∏µ‡πâ ‡∏ó‡∏≥ action ‡∏ô‡∏µ‡πâ ‚Üí ‡πÑ‡∏õ state ‡πÑ‡∏´‡∏ô ‡πÑ‡∏î‡πâ reward ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£\n        self.transitions = {\n            'A': {'right': ('B', 0)},      # A ‡πÑ‡∏õ B ‡πÑ‡∏î‡πâ reward 0\n            'B': {'right': ('Goal', 1)},   # B ‡πÑ‡∏õ Goal ‡πÑ‡∏î‡πâ reward 1\n            'Goal': {}  # Terminal state (‡∏à‡∏ö)\n        }\n        \n        # Terminal reward (‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ï‡∏≠‡∏ô‡∏à‡∏ö)\n        self.terminal_reward = 10\n    \n    def bellman_update(self, V):\n        \"\"\"\n        Update value function ‡∏î‡πâ‡∏ß‡∏¢ Bellman Equation\n        \n        Bellman Equation:\n        V(s) = R(s,a,s') + Œ≥ * V(s')\n        \n        Parameters:\n        -----------\n        V : dict\n            Value function ‡πÄ‡∏Å‡πà‡∏≤ {state: value}\n            \n        Returns:\n        --------\n        new_V : dict\n            Value function ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà update ‡πÅ‡∏•‡πâ‡∏ß\n        \"\"\"\n        new_V = V.copy()\n        \n        # Terminal state: V(Goal) = terminal reward (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)\n        new_V['Goal'] = self.terminal_reward\n        \n        # V(B) = R(B‚ÜíGoal) + Œ≥ * V(Goal)\n        next_state, reward = self.transitions['B']['right']\n        new_V['B'] = reward + self.gamma * V[next_state]\n        \n        # V(A) = R(A‚ÜíB) + Œ≥ * V(B)\n        next_state, reward = self.transitions['A']['right']\n        new_V['A'] = reward + self.gamma * V[next_state]\n        \n        return new_V\n    \n    def value_iteration(self, num_iterations=10):\n        \"\"\"\n        Value Iteration: ‡∏´‡∏≤ value function ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ update ‡∏ã‡πâ‡∏≥‡πÜ\n        \n        Algorithm:\n        1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô V(s) = 0 ‡∏ó‡∏∏‡∏Å state\n        2. Update ‡∏î‡πâ‡∏ß‡∏¢ Bellman equation ‡∏ã‡πâ‡∏≥‡πÜ\n        3. ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ V ‡∏à‡∏∞ converge (‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)\n        \n        Parameters:\n        -----------\n        num_iterations : int\n            ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞ update\n            \n        Returns:\n        --------\n        V : dict\n            Value function ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n        history : list\n            ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á V ‡πÅ‡∏ï‡πà‡∏•‡∏∞ iteration\n        \"\"\"\n        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ V(s) = 0 ‡∏ó‡∏∏‡∏Å state\n        V = {s: 0 for s in self.states}\n        history = [V.copy()]\n        \n        print(\"üîÑ Value Iteration Process:\")\n        print(\"=\"*60)\n        print(f\"{'Iteration':<12} {'V(A)':<10} {'V(B)':<10} {'V(Goal)':<10}\")\n        print(\"-\"*60)\n        print(f\"{'0':<12} {V['A']:<10.2f} {V['B']:<10.2f} {V['Goal']:<10.2f}\")\n        \n        for i in range(num_iterations):\n            # Update V ‡∏î‡πâ‡∏ß‡∏¢ Bellman equation\n            V = self.bellman_update(V)\n            history.append(V.copy())\n            \n            print(f\"{i+1:<12} {V['A']:<10.2f} {V['B']:<10.2f} {V['Goal']:<10.2f}\")\n        \n        return V, history\n\n\n# üöÄ Run Value Iteration\nprint(\"üìä Simple MDP Example: A ‚Üí B ‚Üí Goal\\n\")\nprint(\"Parameters:\")\nprint(\"  - Gamma (Œ≥) = 0.9\")\nprint(\"  - Rewards: A‚ÜíB = 0, B‚ÜíGoal = 1, Goal = 10\")\nprint(\"  - Policy: Always go right\\n\")\n\nmdp = SimpleMDP(gamma=0.9)\nfinal_V, history = mdp.value_iteration(num_iterations=10)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ Converged Values:\")\nprint(f\"  V(A)    = {final_V['A']:.2f}  ‚Üê ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á return ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å A\")\nprint(f\"  V(B)    = {final_V['B']:.2f}  ‚Üê ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á return ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å B\")\nprint(f\"  V(Goal) = {final_V['Goal']:.2f}  ‚Üê Terminal reward\")\n\nprint(\"\\nüí° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:\")\nprint(f\"  - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà A ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ return ‡∏£‡∏ß‡∏° {final_V['A']:.2f}\")\nprint(f\"  - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà B ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ return ‡∏£‡∏ß‡∏° {final_V['B']:.2f}\")\nprint(f\"  - A ‡πÑ‡∏î‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ B ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏Å‡∏• Goal ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ (discount!)\")\n\nprint(\"\\nüîç ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:\")\nprint(f\"  V(B) = 1 + 0.9√ó10 = {1 + 0.9*10}\")\nprint(f\"  V(A) = 0 + 0.9√ó{final_V['B']:.1f} = {0 + 0.9*final_V['B']:.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMDP:\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        self.states = ['A', 'B', 'Goal']\n",
    "        \n",
    "        # Transition: {current_state: {action: (next_state, reward)}}\n",
    "        self.transitions = {\n",
    "            'A': {'right': ('B', 0)},\n",
    "            'B': {'right': ('Goal', 1)},\n",
    "            'Goal': {}  # Terminal state\n",
    "        }\n",
    "        \n",
    "        # Terminal reward\n",
    "        self.terminal_reward = 10\n",
    "    \n",
    "    def bellman_update(self, V):\n",
    "        \"\"\"Update value function ‡∏î‡πâ‡∏ß‡∏¢ Bellman Equation\"\"\"\n",
    "        new_V = V.copy()\n",
    "        \n",
    "        # V(Goal) = terminal reward\n",
    "        new_V['Goal'] = self.terminal_reward\n",
    "        \n",
    "        # V(B) = R(B‚ÜíGoal) + Œ≥ * V(Goal)\n",
    "        next_state, reward = self.transitions['B']['right']\n",
    "        new_V['B'] = reward + self.gamma * V[next_state]\n",
    "        \n",
    "        # V(A) = R(A‚ÜíB) + Œ≥ * V(B)\n",
    "        next_state, reward = self.transitions['A']['right']\n",
    "        new_V['A'] = reward + self.gamma * V[next_state]\n",
    "        \n",
    "        return new_V\n",
    "    \n",
    "    def value_iteration(self, num_iterations=10):\n",
    "        \"\"\"Iterative ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤ V(s)\"\"\"\n",
    "        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ V(s) = 0 ‡∏ó‡∏∏‡∏Å state\n",
    "        V = {s: 0 for s in self.states}\n",
    "        history = [V.copy()]\n",
    "        \n",
    "        print(\"üîÑ Value Iteration Process:\\n\" + \"=\"*60)\n",
    "        print(f\"Iteration 0: V(A)={V['A']:.2f}, V(B)={V['B']:.2f}, V(Goal)={V['Goal']:.2f}\")\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            V = self.bellman_update(V)\n",
    "            history.append(V.copy())\n",
    "            print(f\"Iteration {i+1}: V(A)={V['A']:.2f}, V(B)={V['B']:.2f}, V(Goal)={V['Goal']:.2f}\")\n",
    "        \n",
    "        return V, history\n",
    "\n",
    "# Run Value Iteration\n",
    "mdp = SimpleMDP(gamma=0.9)\n",
    "final_V, history = mdp.value_iteration(num_iterations=10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Converged Values:\")\n",
    "print(f\"V(A) = {final_V['A']:.2f}\")\n",
    "print(f\"V(B) = {final_V['B']:.2f}\")\n",
    "print(f\"V(Goal) = {final_V['Goal']:.2f}\")\n",
    "\n",
    "print(\"\\nüí° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:\")\n",
    "print(f\"- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà A ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ return ‡∏£‡∏ß‡∏° {final_V['A']:.2f}\")\n",
    "print(f\"- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà B ‚Üí ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ return ‡∏£‡∏ß‡∏° {final_V['B']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Value Convergence\n",
    "iterations = range(len(history))\n",
    "V_A = [h['A'] for h in history]\n",
    "V_B = [h['B'] for h in history]\n",
    "V_Goal = [h['Goal'] for h in history]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(iterations, V_A, marker='o', label='V(A)', linewidth=2, markersize=6)\n",
    "plt.plot(iterations, V_B, marker='s', label='V(B)', linewidth=2, markersize=6)\n",
    "plt.plot(iterations, V_Goal, marker='^', label='V(Goal)', linewidth=2, markersize=6)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Value Function V(s)', fontsize=12, fontweight='bold')\n",
    "plt.title('Bellman Equation: Value Function Convergence', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\")\n",
    "print(\"- V(Goal) ‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á 10 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (terminal reward)\")\n",
    "print(\"- V(B) ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏à‡∏≤‡∏Å 0 ‚Üí ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö reward ‡∏à‡∏≤‡∏Å Goal\")\n",
    "print(\"- V(A) ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ä‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏Å‡∏• Goal\")\n",
    "print(\"- ‡∏Ñ‡πà‡∏≤ converge ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å ~5-7 iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé≤ MDP ‡∏Å‡∏±‡∏ö Stochastic Transitions\n",
    "\n",
    "‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏à‡∏£‡∏¥‡∏á action ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ deterministic ‡πÄ‡∏™‡∏°‡∏≠‡πÑ‡∏õ!\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Slippery Grid**\n",
    "\n",
    "```\n",
    "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏±‡πà‡∏á \"‡πÑ‡∏õ‡∏Ç‡∏ß‡∏≤\":\n",
    "- 80% ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ ‚Üí ‡πÑ‡∏õ‡∏Ç‡∏ß‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÜ ‚úÖ\n",
    "- 10% ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ ‚Üí ‡πÑ‡∏õ‡∏ö‡∏ô ‚¨ÜÔ∏è\n",
    "- 10% ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ ‚Üí ‡πÑ‡∏õ‡∏•‡πà‡∏≤‡∏á ‚¨áÔ∏è\n",
    "```\n",
    "\n",
    "Bellman Equation ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô:\n",
    "\n",
    "```\n",
    "V(s) = Œ£ P(s'|s,a) * [R(s,a,s') + Œ≥ * V(s')]\n",
    "     = 0.8 * [R + Œ≥*V(right)] + 0.1 * [R + Œ≥*V(up)] + 0.1 * [R + Œ≥*V(down)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMDP:\n",
    "    \"\"\"MDP ‡∏ó‡∏µ‡πà‡∏°‡∏µ stochastic transitions\"\"\"\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        self.states = ['Start', 'Middle', 'Good', 'Bad']\n",
    "        \n",
    "        # Transition probabilities: {state: {action: [(prob, next_state, reward)]}}\n",
    "        self.transitions = {\n",
    "            'Start': {\n",
    "                'forward': [\n",
    "                    (0.8, 'Middle', 0),   # 80% ‡πÑ‡∏õ‡∏ï‡∏£‡∏á\n",
    "                    (0.1, 'Good', 5),     # 10% ‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ!\n",
    "                    (0.1, 'Bad', -5)      # 10% ‡πÇ‡∏ä‡∏Ñ‡∏£‡πâ‡∏≤‡∏¢\n",
    "                ]\n",
    "            },\n",
    "            'Middle': {\n",
    "                'forward': [\n",
    "                    (0.7, 'Good', 5),\n",
    "                    (0.3, 'Bad', -5)\n",
    "                ]\n",
    "            },\n",
    "            'Good': {},   # Terminal\n",
    "            'Bad': {}     # Terminal\n",
    "        }\n",
    "        \n",
    "        self.terminal_values = {'Good': 10, 'Bad': -10}\n",
    "    \n",
    "    def bellman_update(self, V):\n",
    "        \"\"\"Update with stochastic Bellman equation\"\"\"\n",
    "        new_V = V.copy()\n",
    "        \n",
    "        # Terminal states\n",
    "        new_V['Good'] = self.terminal_values['Good']\n",
    "        new_V['Bad'] = self.terminal_values['Bad']\n",
    "        \n",
    "        # Non-terminal states\n",
    "        for state in ['Start', 'Middle']:\n",
    "            if state in self.transitions:\n",
    "                action = 'forward'\n",
    "                expected_value = 0\n",
    "                \n",
    "                for prob, next_state, reward in self.transitions[state][action]:\n",
    "                    expected_value += prob * (reward + self.gamma * V[next_state])\n",
    "                \n",
    "                new_V[state] = expected_value\n",
    "        \n",
    "        return new_V\n",
    "    \n",
    "    def value_iteration(self, num_iterations=15):\n",
    "        V = {s: 0 for s in self.states}\n",
    "        history = [V.copy()]\n",
    "        \n",
    "        print(\"üé≤ Stochastic MDP Value Iteration:\\n\" + \"=\"*70)\n",
    "        print(f\"Iter 0: V(Start)={V['Start']:.2f}, V(Middle)={V['Middle']:.2f}, V(Good)={V['Good']:.2f}, V(Bad)={V['Bad']:.2f}\")\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            V = self.bellman_update(V)\n",
    "            history.append(V.copy())\n",
    "            print(f\"Iter {i+1}: V(Start)={V['Start']:.2f}, V(Middle)={V['Middle']:.2f}, V(Good)={V['Good']:.2f}, V(Bad)={V['Bad']:.2f}\")\n",
    "        \n",
    "        return V, history\n",
    "\n",
    "# Run Stochastic MDP\n",
    "stoch_mdp = StochasticMDP(gamma=0.9)\n",
    "final_V_stoch, history_stoch = stoch_mdp.value_iteration(num_iterations=15)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Final Values:\")\n",
    "print(f\"V(Start) = {final_V_stoch['Start']:.2f}\")\n",
    "print(f\"V(Middle) = {final_V_stoch['Middle']:.2f}\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(f\"- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà Start ‚Üí Expected return = {final_V_stoch['Start']:.2f}\")\n",
    "print(\"- ‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ß‡∏Å ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡πÑ‡∏£\")\n",
    "print(\"- ‡πÅ‡∏°‡πâ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ 10-30% ‡πÑ‡∏õ Bad ‡πÅ‡∏ï‡πà probability ‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÑ‡∏õ‡∏ó‡∏≤‡∏á Good ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Stochastic MDP\n",
    "iterations = range(len(history_stoch))\n",
    "V_Start = [h['Start'] for h in history_stoch]\n",
    "V_Middle = [h['Middle'] for h in history_stoch]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ax.plot(iterations, V_Start, marker='o', label='V(Start)', linewidth=2, markersize=7, color='#2E86AB')\n",
    "ax.plot(iterations, V_Middle, marker='s', label='V(Middle)', linewidth=2, markersize=7, color='#A23B72')\n",
    "ax.axhline(y=10, color='green', linestyle='--', alpha=0.5, label='V(Good)')\n",
    "ax.axhline(y=-10, color='red', linestyle='--', alpha=0.5, label='V(Bad)')\n",
    "\n",
    "ax.set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Value Function V(s)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Stochastic MDP: Value Convergence', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(\"- Value functions converge ‡∏î‡πâ‡∏ß‡∏¢ expected value ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å possible outcomes\")\n",
    "print(\"- Stochastic transitions ‡∏ó‡∏≥‡πÉ‡∏´‡πâ MDP realistic ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\")\n",
    "print(\"- ‡πÉ‡∏ô Trading: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡πÅ‡∏ö‡∏ö stochastic ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏ô‡∏µ‡πâ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: Effect of Gamma (Œ≥)\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ß‡πà‡∏≤ discount factor ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ value function ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different gamma values\n",
    "gammas_to_test = [0.5, 0.9, 0.95, 0.99]\n",
    "results = {}\n",
    "\n",
    "print(\"üéõÔ∏è Comparing Different Gamma Values:\\n\" + \"=\"*60)\n",
    "\n",
    "for gamma in gammas_to_test:\n",
    "    mdp_test = SimpleMDP(gamma=gamma)\n",
    "    final_V, _ = mdp_test.value_iteration(num_iterations=10)\n",
    "    results[gamma] = final_V\n",
    "    print(f\"\\nŒ≥ = {gamma}:\")\n",
    "    print(f\"  V(A) = {final_V['A']:.2f}\")\n",
    "    print(f\"  V(B) = {final_V['B']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## üéì ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô MDP\n\n### üìö ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n\n#### 1Ô∏è‚É£ **Markov Property**\n- ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≥‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‚Üí ‡∏ó‡∏≥‡πÉ‡∏´‡πâ model ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô\n- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏´‡∏°‡∏≤‡∏Å‡∏£‡∏∏‡∏Å, GPS, ‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡πÉ‡∏ô‡∏ö‡∏±‡∏ç‡∏ä‡∏µ\n\n#### 2Ô∏è‚É£ **MDP Components (5 ‡∏ä‡∏¥‡πâ‡∏ô)**\n- **S** = States (‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)\n- **A** = Actions (‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏î‡πâ)\n- **P** = Transition Probability (‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô state)\n- **R** = Reward Function (‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•/‡πÇ‡∏ó‡∏©)\n- **Œ≥** = Discount Factor (‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô)\n\n#### 3Ô∏è‚É£ **Discount Factor (Œ≥)**\n- Œ≥ ‡∏ï‡πà‡∏≥ (0.1-0.5) ‚Üí ‡∏°‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô ‚Üí Scalping, Day Trading\n- Œ≥ ‡∏Å‡∏•‡∏≤‡∏á (0.7-0.9) ‚Üí ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‚Üí Swing Trading\n- Œ≥ ‡∏™‡∏π‡∏á (0.95-0.999) ‚Üí ‡∏°‡∏≠‡∏á‡∏¢‡∏≤‡∏ß ‚Üí Long-term Investment\n- ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: Time value of money, Uncertainty, Opportunity cost\n\n#### 4Ô∏è‚É£ **Return (G)**\n- G = Œ£ Œ≥^k √ó R_{t+k+1}\n- ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á reward ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏î‡πâ‡∏ß‡∏¢ Œ≥)\n- Œ≥ ‡∏™‡∏π‡∏á ‚Üí Return ‡∏™‡∏π‡∏á (‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)\n\n#### 5Ô∏è‚É£ **Bellman Equation**\n- **V(s) = E[R + Œ≥ √ó V(s')]**\n- ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á state = ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ + ‡∏Ñ‡πà‡∏≤‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (discounted)\n- Recursive structure ‚Üí ‡πÉ‡∏ä‡πâ Value Iteration ‡πÅ‡∏Å‡πâ‡πÑ‡∏î‡πâ\n\n#### 6Ô∏è‚É£ **Value Iteration**\n- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô V(s) = 0\n- Update ‡∏ã‡πâ‡∏≥‡πÜ ‡∏î‡πâ‡∏ß‡∏¢ Bellman equation\n- Converge ‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á\n- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô iterations ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö complexity ‡∏Ç‡∏≠‡∏á MDP\n\n---\n\n## üéØ Key Insights ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Trading\n\n### üíπ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ:\n\n1. **State Design:**\n   - ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô, Position, ‡πÄ‡∏á‡∏¥‡∏ô‡∏™‡∏î\n   - Technical indicators (RSI, MACD, etc.)\n   - Market regime (Trending, Range-bound)\n\n2. **Action Design:**\n   - Buy, Sell, Hold\n   - Position sizing (Ïñº‡∏°‡∏≤ Buy?)\n   - Stop-loss, Take-profit levels\n\n3. **Reward Design:**\n   - PnL (Profit & Loss)\n   - Sharpe Ratio (risk-adjusted return)\n   - Max Drawdown penalty\n\n4. **Gamma Selection:**\n   - Scalper ‚Üí Œ≥ = 0.3\n   - Day Trader ‚Üí Œ≥ = 0.6\n   - Swing Trader ‚Üí Œ≥ = 0.85\n   - Position Trader ‚Üí Œ≥ = 0.95\n\n---\n\n## üß© ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ö Notebook ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n\n‡∏à‡∏≤‡∏Å **Grid World (Notebook 01)**:\n- Agent ‡πÄ‡∏î‡∏¥‡∏ô‡πÉ‡∏ô‡∏Å‡∏£‡∏¥‡∏î ‚Üí ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô MDP\n- Random vs Rule-based ‚Üí ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà optimal\n- ‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏≤ **optimal policy** (‡πÉ‡∏ô Notebook ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ!)\n\n---\n\n## ‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ:\n\n1. **Policy ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**\n   - ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à: œÄ(s) ‚Üí a\n   - Deterministic vs Stochastic\n\n2. **Q-Function ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**\n   - Q(s,a) = ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ action a ‡∏ó‡∏µ‡πà state s\n   - ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å V(s) ‡∏¢‡∏±‡∏á‡πÑ‡∏á?\n\n3. **‡∏à‡∏∞‡∏´‡∏≤ Optimal Policy ‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á?**\n   - Policy Iteration\n   - Q-Learning\n   - Deep Q-Network (DQN)\n\n---\n\n## üöÄ Next Steps\n\n### üìñ Notebook ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ:\n\nüëâ **[03_value_functions.ipynb](03_value_functions.ipynb)**\n- ‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å V(s) ‡πÅ‡∏•‡∏∞ Q(s,a)\n- Optimal Value Function (V*, Q*)\n- Policy Evaluation\n\n### üí™ ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:\n\n1. **‡∏•‡∏≠‡∏á gamma ‡∏ï‡πà‡∏≤‡∏á‡πÜ:**\n   - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Œ≥ = 0.5, 0.95, 0.99 ‡πÉ‡∏ô SimpleMDP\n   - ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ú‡∏•‡∏ï‡πà‡∏≠ V(A), V(B)\n\n2. **‡∏™‡∏£‡πâ‡∏≤‡∏á MDP ‡πÄ‡∏≠‡∏á:**\n   - ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö MDP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Trading scenario\n   - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î states, actions, rewards\n   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì value function\n\n3. **Stochastic MDP:**\n   - ‡∏ó‡∏î‡∏•‡∏≠‡∏á StochasticMDP ‡∏Å‡∏±‡∏ö probability ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n   - ‡∏î‡∏π‡∏ß‡πà‡∏≤ expected value ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á\n\n---\n\n## üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n\n### üé• Videos:\n- [David Silver RL Course - Lecture 2: MDPs](https://www.youtube.com/watch?v=lfHX2hHRMVQ)\n- [DeepMind x UCL | Intro to RL - MDP](https://www.youtube.com/watch?v=TCCjZe0y4Qc)\n\n### üìñ Books:\n- [Sutton & Barto: Reinforcement Learning (Chapter 3)](http://incompleteideas.net/book/the-book-2nd.html)\n- [David Silver's Slides](https://www.davidsilver.uk/teaching/)\n\n### üíª Interactive:\n- [OpenAI Spinning Up - MDP](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n- [UC Berkeley CS188 - MDPs](https://inst.eecs.berkeley.edu/~cs188/)\n\n---\n\n## üí¨ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ (TL;DR)\n\n**MDP = ‡∏Å‡∏£‡∏≠‡∏ö‡∏Ñ‡∏¥‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô**\n\n```\nAgent ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô State (s)\n  ‚Üì\n‡∏ó‡∏≥ Action (a)\n  ‚Üì\n‡πÑ‡∏î‡πâ Reward (r) ‡πÅ‡∏•‡∏∞‡πÑ‡∏õ‡∏¢‡∏±‡∏á State ‡πÉ‡∏´‡∏°‡πà (s')\n  ‚Üì\n‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡πÄ‡∏Å‡πá‡∏ö Return (G) ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n  ‚Üì\n‡πÉ‡∏ä‡πâ Bellman Equation ‡∏´‡∏≤ Value Function (V)\n  ‚Üì\n‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ V ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î = Optimal Policy!\n```\n\n**Remember:**\n- **Markov Property:** ‡∏≠‡∏î‡∏µ‡∏ï‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç, ‡∏°‡∏≠‡∏á‡πÅ‡∏Ñ‡πà‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\n- **Discount Factor:** balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏°‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô vs ‡∏°‡∏≠‡∏á‡∏¢‡∏≤‡∏ß\n- **Bellman Equation:** recursive magic ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì value\n- **Value Iteration:** update ‡∏ã‡πâ‡∏≥‡πÜ ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞ converge\n\n---\n\n## üéâ ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏î‡πâ‡∏ß‡∏¢!\n\n‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à MDP ‡πÅ‡∏•‡πâ‡∏ß! üéä\n\n‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏õ‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö **Value Functions ‡πÅ‡∏•‡∏∞ Q-Learning** ‡πÅ‡∏•‡πâ‡∏ß!\n\nüëâ [Next: 03_value_functions.ipynb](03_value_functions.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ: MDP Concepts\n",
    "\n",
    "### 1Ô∏è‚É£ Markov Property\n",
    "- ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "- P(S_{t+1} | S_t, history) = P(S_{t+1} | S_t)\n",
    "\n",
    "### 2Ô∏è‚É£ MDP Components\n",
    "- **S**: States\n",
    "- **A**: Actions\n",
    "- **P**: Transition probabilities\n",
    "- **R**: Reward function\n",
    "- **Œ≥**: Discount factor\n",
    "\n",
    "### 3Ô∏è‚É£ Return & Discount\n",
    "- Return = Œ£ Œ≥·µè * R_{t+k+1}\n",
    "- Œ≥ ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏ß‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô\n",
    "\n",
    "### 4Ô∏è‚É£ Bellman Equation\n",
    "- V(s) = E[R + Œ≥ * V(s')]\n",
    "- ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á Value Function calculation\n",
    "- Recursive relationship\n",
    "\n",
    "### 5Ô∏è‚É£ Value Iteration\n",
    "- Update V(s) iteratively ‡∏î‡πâ‡∏ß‡∏¢ Bellman equation\n",
    "- Converge ‡πÑ‡∏õ‡∏™‡∏π‡πà true value function\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "1. **Notebook 03:** Value Functions (V ‡πÅ‡∏•‡∏∞ Q)\n",
    "2. **Notebook 04:** Policies (Deterministic vs Stochastic)\n",
    "3. **Notebook 05:** Exploration vs Exploitation\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î\n",
    "\n",
    "### Exercise 1: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Return\n",
    "Reward sequence: [2, 3, 4, 5, 6]\n",
    "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì return ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Œ≥ = 0.8\n",
    "\n",
    "### Exercise 2: Bellman Update\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á MDP 4 states: Start ‚Üí A ‚Üí B ‚Üí Goal\n",
    "- Rewards: 0, 1, 2, 10\n",
    "- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì value function ‡∏î‡πâ‡∏ß‡∏¢ value iteration (Œ≥=0.9)\n",
    "\n",
    "### Exercise 3: Stochastic Transitions\n",
    "‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö MDP ‡∏ó‡∏µ‡πà‡∏°‡∏µ stochastic transitions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö trading scenario:\n",
    "- States: No Position, Long Position, Short Position\n",
    "- Actions: Buy, Sell, Hold\n",
    "- Probability: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô 60%, ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏•‡∏á 40%\n",
    "\n",
    "---\n",
    "\n",
    "## üìö ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
    "\n",
    "- [Sutton & Barto: Reinforcement Learning Book](http://incompleteideas.net/book/the-book.html)\n",
    "- [David Silver RL Course Lecture 2: MDPs](https://www.youtube.com/watch?v=lfHX2hHRMVQ)\n",
    "- [UC Berkeley CS188: MDPs](https://inst.eecs.berkeley.edu/~cs188/sp21/)\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Ready to learn about Value Functions?**\n",
    "\n",
    "üëâ [Next: 03_value_functions.ipynb](03_value_functions.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}