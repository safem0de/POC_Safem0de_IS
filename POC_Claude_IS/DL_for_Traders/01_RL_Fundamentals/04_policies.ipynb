{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Policies: ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏Ç‡∏≠‡∏á Agent\n",
    "## ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Deterministic, Stochastic, ‡πÅ‡∏•‡∏∞ Epsilon-Greedy Policies\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß: ‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡∏£‡∏î‡∏™‡∏≤‡∏°‡∏™‡πÑ‡∏ï‡∏•‡πå\n",
    "\n",
    "‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏°‡∏µ‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡∏£‡∏î 3 ‡∏Ñ‡∏ô ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô:\n",
    "\n",
    "### üë§ ‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡∏£‡∏î A: \"The Robot\" (Deterministic)\n",
    "```bash\n",
    "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí ‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏™‡∏°‡∏≠ (100%)\n",
    "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏•‡∏á ‚Üí ‡∏Ç‡∏≤‡∏¢‡πÄ‡∏™‡∏°‡∏≠ (100%)\n",
    "```\n",
    "- ‡∏ó‡∏≥ action ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
    "- **Deterministic Policy**: œÄ(s) ‚Üí a\n",
    "\n",
    "---\n",
    "\n",
    "### üë• ‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡∏£‡∏î B: \"The Strategist\" (Stochastic)\n",
    "```bash\n",
    "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí ‡∏ã‡∏∑‡πâ‡∏≠ 70%, ‡∏£‡∏≠ 30%\n",
    "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏•‡∏á ‚Üí ‡∏Ç‡∏≤‡∏¢ 60%, ‡∏£‡∏≠ 40%\n",
    "```\n",
    "- ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "- **Stochastic Policy**: œÄ(a|s) = probability\n",
    "\n",
    "---\n",
    "\n",
    "### üé≤ ‡∏ô‡∏±‡∏Å‡πÄ‡∏ó‡∏£‡∏î C: \"The Explorer\" (Epsilon-Greedy)\n",
    "```bash\n",
    "90% ‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí ‡∏ó‡∏≥‡∏ï‡∏≤‡∏° best action\n",
    "10% ‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí ‡∏•‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡∏°‡πà‡πÜ (random)\n",
    "```\n",
    "- ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏•‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "- **Epsilon-Greedy Policy**: Balance exploitation & exploration\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Policy ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "\n",
    "**Policy (œÄ)** = ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà Agent ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action\n",
    "\n",
    "### ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Policy:\n",
    "\n",
    "#### 1Ô∏è‚É£ Deterministic Policy\n",
    "```bash\n",
    "œÄ(s) = a\n",
    "```\n",
    "- Input: State s\n",
    "- Output: Action a (‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)\n",
    "- **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**\n",
    "  - ‡∏£‡∏≤‡∏Ñ‡∏≤ > MA200 ‚Üí Buy\n",
    "  - ‡∏£‡∏≤‡∏Ñ‡∏≤ < MA200 ‚Üí Sell\n",
    "\n",
    "#### 2Ô∏è‚É£ Stochastic Policy\n",
    "```bash\n",
    "œÄ(a|s) = P(A_t=a | S_t=s)\n",
    "```\n",
    "- Input: State s\n",
    "- Output: Probability distribution over actions\n",
    "- **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**\n",
    "  - ‡∏£‡∏≤‡∏Ñ‡∏≤ > MA200 ‚Üí Buy (80%), Hold (20%)\n",
    "  - ‡∏£‡∏≤‡∏Ñ‡∏≤ < MA200 ‚Üí Sell (70%), Hold (30%)\n",
    "\n",
    "#### 3Ô∏è‚É£ Epsilon-Greedy Policy\n",
    "```bash\n",
    "œÄ(s) = {\n",
    "  argmax_a Q(s,a)  with probability 1-Œµ (exploit)\n",
    "  random action    with probability Œµ   (explore)\n",
    "}\n",
    "```\n",
    "- Balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á exploitation (‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ) ‡πÅ‡∏•‡∏∞ exploration (‡∏•‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üì¶ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Environment: Simple Grid World\n",
    "\n",
    "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å simple environment ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö policies ‡∏ï‡πà‡∏≤‡∏á‡πÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.start_pos = (0, 0)\n",
    "        self.goal_pos = (size-1, size-1)\n",
    "        self.holes = [(1, 1), (2, 3), (3, 1)]\n",
    "        self.actions = ['‚Üë', '‚Üì', '‚Üê', '‚Üí']\n",
    "        self.action_effects = {\n",
    "            '‚Üë': (-1, 0),\n",
    "            '‚Üì': (1, 0),\n",
    "            '‚Üê': (0, -1),\n",
    "            '‚Üí': (0, 1)\n",
    "        }\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = self.start_pos\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        dr, dc = self.action_effects[action]\n",
    "        new_pos = (self.pos[0] + dr, self.pos[1] + dc)\n",
    "        \n",
    "        # Check boundaries\n",
    "        if not (0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size):\n",
    "            new_pos = self.pos  # Stay in place\n",
    "        \n",
    "        self.pos = new_pos\n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.pos == self.goal_pos:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        elif self.pos in self.holes:\n",
    "            reward = -10\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "        \n",
    "        return self.pos, reward, done\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.pos\n",
    "\n",
    "# Create environment\n",
    "env = PolicyGridWorld(size=5)\n",
    "print(\"‚úÖ GridWorld Environment Created!\")\n",
    "print(f\"Start: {env.start_pos}, Goal: {env.goal_pos}, Holes: {env.holes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Deterministic Policy\n",
    "\n",
    "Policy ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicPolicy:\n",
    "    \"\"\"Simple deterministic policy: Always go right or down toward goal\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.name = \"Deterministic\"\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Always select action toward goal (greedy toward bottom-right)\"\"\"\n",
    "        r, c = state\n",
    "        goal_r, goal_c = self.env.goal_pos\n",
    "        \n",
    "        # Priority: right > down\n",
    "        if c < goal_c:\n",
    "            return '‚Üí'\n",
    "        elif r < goal_r:\n",
    "            return '‚Üì'\n",
    "        else:\n",
    "            return '‚Üí'  # Default\n",
    "    \n",
    "    def run_episode(self, max_steps=50):\n",
    "        \"\"\"Run one episode\"\"\"\n",
    "        state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        trajectory = [state]\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = self.select_action(state)\n",
    "            state, reward, done = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            trajectory.append(state)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return total_reward, trajectory, step + 1\n",
    "\n",
    "# Test Deterministic Policy\n",
    "det_policy = DeterministicPolicy(env)\n",
    "reward, trajectory, steps = det_policy.run_episode()\n",
    "\n",
    "print(\"ü§ñ Deterministic Policy Test:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Reward: {reward}\")\n",
    "print(f\"Steps: {steps}\")\n",
    "print(f\"Trajectory: {trajectory[:10]}...\")  # First 10 steps\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Run multiple episodes\n",
    "num_episodes = 100\n",
    "rewards = []\n",
    "for _ in range(num_episodes):\n",
    "    r, _, _ = det_policy.run_episode()\n",
    "    rewards.append(r)\n",
    "\n",
    "print(f\"\\nüìä Performance over {num_episodes} episodes:\")\n",
    "print(f\"Mean Reward: {np.mean(rewards):.2f}\")\n",
    "print(f\"Std Reward: {np.std(rewards):.2f}\")\n",
    "print(f\"Success Rate: {(np.array(rewards) > 0).sum() / num_episodes * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Stochastic Policy\n",
    "\n",
    "Policy ‡∏ó‡∏µ‡πà‡∏°‡∏µ probability distribution over actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticPolicy:\n",
    "    \"\"\"Stochastic policy with probability distribution\"\"\"\n",
    "    def __init__(self, env, main_action_prob=0.7):\n",
    "        self.env = env\n",
    "        self.main_action_prob = main_action_prob\n",
    "        self.name = \"Stochastic\"\n",
    "    \n",
    "    def get_action_probabilities(self, state):\n",
    "        \"\"\"Get probability distribution over actions\"\"\"\n",
    "        r, c = state\n",
    "        goal_r, goal_c = self.env.goal_pos\n",
    "        \n",
    "        # Initialize probabilities\n",
    "        probs = {a: 0.0 for a in self.env.actions}\n",
    "        \n",
    "        # Determine preferred action (toward goal)\n",
    "        if c < goal_c:\n",
    "            preferred = '‚Üí'\n",
    "        elif r < goal_r:\n",
    "            preferred = '‚Üì'\n",
    "        else:\n",
    "            preferred = '‚Üí'\n",
    "        \n",
    "        # Assign probabilities\n",
    "        probs[preferred] = self.main_action_prob\n",
    "        remaining_prob = 1.0 - self.main_action_prob\n",
    "        other_actions = [a for a in self.env.actions if a != preferred]\n",
    "        for action in other_actions:\n",
    "            probs[action] = remaining_prob / len(other_actions)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Sample action from probability distribution\"\"\"\n",
    "        probs = self.get_action_probabilities(state)\n",
    "        actions = list(probs.keys())\n",
    "        probabilities = list(probs.values())\n",
    "        return np.random.choice(actions, p=probabilities)\n",
    "    \n",
    "    def run_episode(self, max_steps=50):\n",
    "        \"\"\"Run one episode\"\"\"\n",
    "        state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        trajectory = [state]\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = self.select_action(state)\n",
    "            state, reward, done = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            trajectory.append(state)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return total_reward, trajectory, step + 1\n",
    "\n",
    "# Test Stochastic Policy\n",
    "stoch_policy = StochasticPolicy(env, main_action_prob=0.7)\n",
    "\n",
    "# Show probability distribution for a state\n",
    "example_state = (1, 2)\n",
    "probs = stoch_policy.get_action_probabilities(example_state)\n",
    "print(f\"üé≤ Stochastic Policy: Action Probabilities at {example_state}\")\n",
    "print(\"=\"*50)\n",
    "for action, prob in probs.items():\n",
    "    bar = '‚ñà' * int(prob * 50)\n",
    "    print(f\"{action}: {prob:.2f} {bar}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Run multiple episodes\n",
    "num_episodes = 100\n",
    "rewards_stoch = []\n",
    "for _ in range(num_episodes):\n",
    "    r, _, _ = stoch_policy.run_episode()\n",
    "    rewards_stoch.append(r)\n",
    "\n",
    "print(f\"\\nüìä Performance over {num_episodes} episodes:\")\n",
    "print(f\"Mean Reward: {np.mean(rewards_stoch):.2f}\")\n",
    "print(f\"Std Reward: {np.std(rewards_stoch):.2f}\")\n",
    "print(f\"Success Rate: {(np.array(rewards_stoch) > 0).sum() / num_episodes * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Epsilon-Greedy Policy\n",
    "\n",
    "Policy ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Q-Learning: balance exploitation & exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "    \"\"\"Epsilon-Greedy policy for Q-Learning\"\"\"\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.name = f\"Œµ-Greedy (Œµ={epsilon})\"\n",
    "        \n",
    "        # Mock Q-table (‡∏à‡∏≤‡∏Å training)\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        self._initialize_q_values()\n",
    "    \n",
    "    def _initialize_q_values(self):\n",
    "        \"\"\"Initialize Q-values (mock - normally learned)\"\"\"\n",
    "        for r in range(self.env.size):\n",
    "            for c in range(self.env.size):\n",
    "                state = (r, c)\n",
    "                goal_r, goal_c = self.env.goal_pos\n",
    "                \n",
    "                # Simple heuristic: actions toward goal have higher Q\n",
    "                for action in self.env.actions:\n",
    "                    if action == '‚Üí' and c < goal_c:\n",
    "                        self.Q[state][action] = 5.0\n",
    "                    elif action == '‚Üì' and r < goal_r:\n",
    "                        self.Q[state][action] = 5.0\n",
    "                    else:\n",
    "                        self.Q[state][action] = 1.0\n",
    "    \n",
    "    def select_action(self, state, mode='train'):\n",
    "        \"\"\"Select action using epsilon-greedy strategy\"\"\"\n",
    "        if mode == 'train' and np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.choice(self.env.actions)\n",
    "        else:\n",
    "            # Exploit: best action\n",
    "            q_values = self.Q[state]\n",
    "            max_q = max(q_values.values())\n",
    "            # Handle ties: random among best actions\n",
    "            best_actions = [a for a, q in q_values.items() if q == max_q]\n",
    "            return np.random.choice(best_actions)\n",
    "    \n",
    "    def run_episode(self, max_steps=50, mode='train'):\n",
    "        \"\"\"Run one episode\"\"\"\n",
    "        state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        trajectory = [state]\n",
    "        exploration_count = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Track if we explored\n",
    "            if mode == 'train' and np.random.random() < self.epsilon:\n",
    "                exploration_count += 1\n",
    "            \n",
    "            action = self.select_action(state, mode=mode)\n",
    "            state, reward, done = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            trajectory.append(state)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return total_reward, trajectory, step + 1, exploration_count\n",
    "\n",
    "# Test Epsilon-Greedy Policy\n",
    "eg_policy = EpsilonGreedyPolicy(env, epsilon=0.1)\n",
    "\n",
    "# Show Q-values for a state\n",
    "example_state = (1, 2)\n",
    "q_values = eg_policy.Q[example_state]\n",
    "print(f\"üéØ Œµ-Greedy Policy: Q-values at {example_state}\")\n",
    "print(\"=\"*50)\n",
    "for action, q in q_values.items():\n",
    "    marker = \" ‚Üê BEST\" if q == max(q_values.values()) else \"\"\n",
    "    print(f\"Q({action}): {q:.2f}{marker}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Run multiple episodes\n",
    "num_episodes = 100\n",
    "rewards_eg = []\n",
    "exploration_rates = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    r, _, steps, explorations = eg_policy.run_episode(mode='train')\n",
    "    rewards_eg.append(r)\n",
    "    exploration_rates.append(explorations / steps)\n",
    "\n",
    "print(f\"\\nüìä Performance over {num_episodes} episodes:\")\n",
    "print(f\"Mean Reward: {np.mean(rewards_eg):.2f}\")\n",
    "print(f\"Std Reward: {np.std(rewards_eg):.2f}\")\n",
    "print(f\"Success Rate: {(np.array(rewards_eg) > 0).sum() / num_episodes * 100:.1f}%\")\n",
    "print(f\"Avg Exploration Rate: {np.mean(exploration_rates) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all policies\n",
    "policies = [\n",
    "    DeterministicPolicy(env),\n",
    "    StochasticPolicy(env, main_action_prob=0.7),\n",
    "    EpsilonGreedyPolicy(env, epsilon=0.1),\n",
    "    EpsilonGreedyPolicy(env, epsilon=0.3)\n",
    "]\n",
    "\n",
    "policies[-1].name = \"Œµ-Greedy (Œµ=0.3)\"  # Update name\n",
    "\n",
    "results = {}\n",
    "num_episodes = 200\n",
    "\n",
    "print(\"üèÅ Running Policy Comparison...\\n\")\n",
    "\n",
    "for policy in policies:\n",
    "    rewards = []\n",
    "    steps_list = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        if isinstance(policy, EpsilonGreedyPolicy):\n",
    "            r, _, s, _ = policy.run_episode(mode='train')\n",
    "        else:\n",
    "            r, _, s = policy.run_episode()\n",
    "        rewards.append(r)\n",
    "        steps_list.append(s)\n",
    "    \n",
    "    results[policy.name] = {\n",
    "        'rewards': rewards,\n",
    "        'steps': steps_list,\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'success_rate': (np.array(rewards) > 0).sum() / num_episodes * 100,\n",
    "        'mean_steps': np.mean(steps_list)\n",
    "    }\n",
    "\n",
    "# Print comparison table\n",
    "print(\"üìä Policy Comparison Results:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Policy':<20} {'Mean Reward':>12} {'Std':>8} {'Success %':>10} {'Avg Steps':>10}\")\n",
    "print(\"=\"*80)\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:<20} {res['mean_reward']:>12.2f} {res['std_reward']:>8.2f} {res['success_rate']:>9.1f}% {res['mean_steps']:>10.1f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Mean Reward Comparison\n",
    "ax1 = axes[0, 0]\n",
    "names = list(results.keys())\n",
    "means = [results[n]['mean_reward'] for n in names]\n",
    "stds = [results[n]['std_reward'] for n in names]\n",
    "\n",
    "bars = ax1.bar(range(len(names)), means, yerr=stds, capsize=5, \n",
    "               color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'],\n",
    "               edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=15, ha='right')\n",
    "ax1.set_ylabel('Mean Reward', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Average Reward per Policy', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "# 2. Success Rate Comparison\n",
    "ax2 = axes[0, 1]\n",
    "success_rates = [results[n]['success_rate'] for n in names]\n",
    "bars = ax2.bar(range(len(names)), success_rates, \n",
    "               color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'],\n",
    "               edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax2.set_xticks(range(len(names)))\n",
    "ax2.set_xticklabels(names, rotation=15, ha='right')\n",
    "ax2.set_ylabel('Success Rate (%)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Success Rate per Policy', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim([0, 100])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add percentage labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 3. Reward Distribution\n",
    "ax3 = axes[1, 0]\n",
    "for name in names:\n",
    "    ax3.hist(results[name]['rewards'], bins=20, alpha=0.5, label=name)\n",
    "ax3.set_xlabel('Reward', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Reward Distribution', fontsize=13, fontweight='bold')\n",
    "ax3.legend(loc='upper left', fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Steps Distribution\n",
    "ax4 = axes[1, 1]\n",
    "box_data = [results[n]['steps'] for n in names]\n",
    "bp = ax4.boxplot(box_data, labels=names, patch_artist=True)\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax4.set_xticklabels(names, rotation=15, ha='right')\n",
    "ax4.set_ylabel('Steps to Completion', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Steps Distribution', fontsize=13, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"- Deterministic: Fast but may get stuck\")\n",
    "print(\"- Stochastic: More exploration, more variability\")\n",
    "print(\"- Œµ-Greedy (low Œµ): Good balance, mostly exploit\")\n",
    "print(\"- Œµ-Greedy (high Œµ): More exploration, slower convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Effect of Epsilon (Œµ)\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ß‡πà‡∏≤ epsilon ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ performance ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different epsilon values\n",
    "epsilon_values = [0.0, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "epsilon_results = {}\n",
    "\n",
    "print(\"üéõÔ∏è Testing Different Epsilon Values...\\n\")\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    policy = EpsilonGreedyPolicy(env, epsilon=eps)\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        r, _, _, _ = policy.run_episode(mode='train')\n",
    "        rewards.append(r)\n",
    "    \n",
    "    epsilon_results[eps] = {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'success_rate': (np.array(rewards) > 0).sum() / 100 * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"Œµ = {eps:.2f}: Mean Reward = {np.mean(rewards):6.2f}, Success = {epsilon_results[eps]['success_rate']:5.1f}%\")\n",
    "\n",
    "# Plot epsilon effect\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Mean Reward vs Epsilon\n",
    "eps_list = list(epsilon_results.keys())\n",
    "mean_rewards = [epsilon_results[e]['mean_reward'] for e in eps_list]\n",
    "ax1.plot(eps_list, mean_rewards, marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
    "ax1.set_xlabel('Epsilon (Œµ)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Mean Reward', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Effect of Epsilon on Mean Reward', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Success Rate vs Epsilon\n",
    "success_rates_eps = [epsilon_results[e]['success_rate'] for e in eps_list]\n",
    "ax2.plot(eps_list, success_rates_eps, marker='s', linewidth=2, markersize=8, color='#A23B72')\n",
    "ax2.set_xlabel('Epsilon (Œµ)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Effect of Epsilon on Success Rate', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"- Œµ = 0: Pure exploitation (no exploration) ‚Üí ‡∏≠‡∏≤‡∏à stuck ‡πÉ‡∏ô local optima\")\n",
    "print(\"- Œµ ‡∏ï‡πà‡∏≥ (0.05-0.1): Good balance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö testing\")\n",
    "print(\"- Œµ ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (0.2-0.3): ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training ‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å\")\n",
    "print(\"- Œµ ‡∏™‡∏π‡∏á (0.5+): Random ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ, performance ‡∏•‡∏î‡∏•‡∏á\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíπ Trading Policy Example\n",
    "\n",
    "‡∏°‡∏≤‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á policy ‡πÉ‡∏ô trading context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingPolicyComparison:\n",
    "    \"\"\"Compare different trading policies\"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = ['uptrend', 'downtrend', 'sideways']\n",
    "        self.actions = ['buy', 'sell', 'hold']\n",
    "        \n",
    "        # Mock Q-values from training\n",
    "        self.Q = {\n",
    "            'uptrend': {'buy': 8.5, 'sell': -2.0, 'hold': 3.2},\n",
    "            'downtrend': {'buy': -6.0, 'sell': 7.5, 'hold': 1.5},\n",
    "            'sideways': {'buy': 1.2, 'sell': 1.0, 'hold': 4.5}\n",
    "        }\n",
    "    \n",
    "    def deterministic_policy(self, state):\n",
    "        \"\"\"Always pick best action\"\"\"\n",
    "        return max(self.Q[state], key=self.Q[state].get)\n",
    "    \n",
    "    def stochastic_policy(self, state, temperature=1.0):\n",
    "        \"\"\"Softmax policy\"\"\"\n",
    "        q_values = np.array(list(self.Q[state].values()))\n",
    "        exp_q = np.exp(q_values / temperature)\n",
    "        probs = exp_q / exp_q.sum()\n",
    "        return dict(zip(self.actions, probs))\n",
    "    \n",
    "    def epsilon_greedy_policy(self, state, epsilon=0.1):\n",
    "        \"\"\"Epsilon-greedy\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            return max(self.Q[state], key=self.Q[state].get)\n",
    "    \n",
    "    def visualize_policies(self):\n",
    "        \"\"\"Visualize action selection for different policies\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "        \n",
    "        for idx, state in enumerate(self.states):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Deterministic\n",
    "            det_action = self.deterministic_policy(state)\n",
    "            det_probs = {a: 1.0 if a == det_action else 0.0 for a in self.actions}\n",
    "            \n",
    "            # Stochastic (softmax)\n",
    "            stoch_probs = self.stochastic_policy(state, temperature=2.0)\n",
    "            \n",
    "            # Plot\n",
    "            x = np.arange(len(self.actions))\n",
    "            width = 0.35\n",
    "            \n",
    "            bars1 = ax.bar(x - width/2, [det_probs[a] for a in self.actions], \n",
    "                          width, label='Deterministic', color='#2E86AB', alpha=0.7)\n",
    "            bars2 = ax.bar(x + width/2, [stoch_probs[a] for a in self.actions], \n",
    "                          width, label='Stochastic', color='#A23B72', alpha=0.7)\n",
    "            \n",
    "            ax.set_xlabel('Action', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('Probability', fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'State: {state.upper()}', fontsize=12, fontweight='bold')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(self.actions)\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.set_ylim([0, 1.1])\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add Q-values as text\n",
    "            for i, action in enumerate(self.actions):\n",
    "                q = self.Q[state][action]\n",
    "                ax.text(i, -0.15, f'Q={q:.1f}', ha='center', fontsize=9, \n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and visualize\n",
    "trading_comp = TradingPolicyComparison()\n",
    "trading_comp.visualize_policies()\n",
    "\n",
    "print(\"üíπ Trading Policy Insights:\")\n",
    "print(\"=\"*60)\n",
    "for state in trading_comp.states:\n",
    "    det = trading_comp.deterministic_policy(state)\n",
    "    stoch = trading_comp.stochastic_policy(state, temperature=2.0)\n",
    "    print(f\"\\n{state.upper()}:\")\n",
    "    print(f\"  Deterministic ‚Üí {det} (always)\")\n",
    "    print(f\"  Stochastic ‚Üí {max(stoch, key=stoch.get)} ({max(stoch.values())*100:.1f}% of time)\")\n",
    "    print(f\"  Q-values: {trading_comp.Q[state]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ: Policies\n",
    "\n",
    "### üìå Deterministic Policy\n",
    "- **œÄ(s) = a** (‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:**\n",
    "  - ‡∏á‡πà‡∏≤‡∏¢, ‡πÄ‡∏£‡πá‡∏ß\n",
    "  - Reproducible\n",
    "  - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö deployment\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:**\n",
    "  - ‡πÑ‡∏°‡πà‡∏°‡∏µ exploration\n",
    "  - ‡∏≠‡∏≤‡∏à‡∏ï‡∏¥‡∏î local optima\n",
    "- **Use case:** Production trading bot (‡∏´‡∏•‡∏±‡∏á training ‡πÄ‡∏™‡∏£‡πá‡∏à)\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Stochastic Policy\n",
    "- **œÄ(a|s) = probability**\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:**\n",
    "  - Natural exploration\n",
    "  - ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô\n",
    "  - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö continuous actions\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:**\n",
    "  - Variance ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤\n",
    "  - ‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ debug\n",
    "- **Use case:** Policy Gradient methods (PPO, A2C)\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Epsilon-Greedy Policy\n",
    "- **œÄ(s) = best action (1-Œµ), random (Œµ)**\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:**\n",
    "  - Balance exploration/exploitation\n",
    "  - Simple ‡πÅ‡∏•‡∏∞ effective\n",
    "  - ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Q-Learning ‡πÑ‡∏î‡πâ‡∏î‡∏µ\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:**\n",
    "  - Œµ ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö (decay)\n",
    "  - Exploration ‡πÅ‡∏ö‡∏ö uniform (‡πÑ‡∏°‡πà smart)\n",
    "- **Use case:** Q-Learning, DQN training\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Policy ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Trading\n",
    "\n",
    "### ‡∏ä‡πà‡∏ß‡∏á Training:\n",
    "- ‡πÉ‡∏ä‡πâ **Œµ-Greedy** ‡∏´‡∏£‡∏∑‡∏≠ **Stochastic**\n",
    "- Œµ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏π‡∏á (0.3-0.5) ‡πÅ‡∏•‡πâ‡∏ß decay ‡∏•‡∏á (0.05-0.1)\n",
    "- ‡πÄ‡∏û‡∏∑‡πà‡∏≠ explore strategies ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "\n",
    "### ‡∏ä‡πà‡∏ß‡∏á Testing/Backtesting:\n",
    "- ‡πÉ‡∏ä‡πâ **Deterministic** (greedy)\n",
    "- Œµ = 0 (pure exploitation)\n",
    "- ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π true performance\n",
    "\n",
    "### Production:\n",
    "- ‡πÉ‡∏ä‡πâ **Deterministic** ‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏ï‡∏•‡∏≠‡∏î\n",
    "- ‡∏≠‡∏≤‡∏à‡∏°‡∏µ Œµ ‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å (0.01) ‡πÄ‡∏û‡∏∑‡πà‡∏≠ adaptability\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "1. **Notebook 05:** Exploration vs Exploitation Deep Dive\n",
    "2. **Chapter 02:** Q-Learning Basics\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î\n",
    "\n",
    "### Exercise 1: Implement Softmax Policy\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á stochastic policy ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ softmax:\n",
    "```python\n",
    "œÄ(a|s) = exp(Q(s,a)/œÑ) / Œ£ exp(Q(s,a')/œÑ)\n",
    "```\n",
    "‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö temperature œÑ = 0.5, 1.0, 2.0\n",
    "\n",
    "### Exercise 2: Epsilon Decay Schedule\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á epsilon decay schedule:\n",
    "- Linear decay: Œµ = Œµ_start - (Œµ_start - Œµ_end) * (episode / max_episodes)\n",
    "- Exponential decay: Œµ = Œµ_end + (Œµ_start - Œµ_end) * exp(-decay_rate * episode)\n",
    "\n",
    "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance\n",
    "\n",
    "### Exercise 3: Trading Policy Design\n",
    "‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö policy ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö crypto trading:\n",
    "- States: (trend, volume, position)\n",
    "- Actions: buy, sell, hold\n",
    "- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á 3 policy types\n",
    "- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤ policy ‡πÑ‡∏´‡∏ô‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Ready for the final notebook?**\n",
    "\n",
    "üëâ [Next: 05_exploration_exploitation.ipynb](05_exploration_exploitation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
