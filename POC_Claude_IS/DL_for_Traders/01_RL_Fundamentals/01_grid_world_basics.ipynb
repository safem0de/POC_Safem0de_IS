{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Grid World Basics - ‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏´‡∏≤‡∏ó‡∏≤‡∏á\n",
    "\n",
    "## ü§ñ ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á Agent ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å\n",
    "\n",
    "‡∏•‡∏≠‡∏á‡∏à‡∏¥‡∏ô‡∏ï‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏∂‡πà‡∏á ‡∏ß‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏´‡πâ‡∏≠‡∏á‡∏Ç‡∏ô‡∏≤‡∏î 5√ó5 ‡∏ä‡πà‡∏≠‡∏á\n",
    "\n",
    "**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** ‡πÄ‡∏î‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡∏ó‡∏≤‡∏á‡∏≠‡∏≠‡∏Å üö™\n",
    "\n",
    "**‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ:**\n",
    "- ‡∏°‡∏µ‡∏´‡∏•‡∏∏‡∏°‡∏û‡∏±‡∏á (‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏Å!) ‚ùå\n",
    "- ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏´‡∏ô ü§î\n",
    "\n",
    "**‡∏Å‡∏é‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏°:**\n",
    "- ‡∏ñ‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏ï‡∏π ‚Üí ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• +10 üéâ\n",
    "- ‡∏ï‡∏Å‡∏´‡∏•‡∏∏‡∏° ‚Üí ‡∏•‡∏á‡πÇ‡∏ó‡∏© -10 üò±\n",
    "- ‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏∏‡∏Å‡∏Å‡πâ‡∏≤‡∏ß ‚Üí -1 (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î)\n",
    "\n",
    "‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á Grid World ‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏¢! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Part 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á Grid World Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Grid World Environment ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Reinforcement Learning\n",
    "    \n",
    "    Grid Layout (5x5):\n",
    "    S = Start (0,0)\n",
    "    G = Goal (4,4)\n",
    "    X = Hole (obstacles)\n",
    "    . = Empty space\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.start_pos = (0, 0)\n",
    "        self.goal_pos = (size-1, size-1)\n",
    "        \n",
    "        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏´‡∏•‡∏∏‡∏°\n",
    "        self.holes = [(1, 1), (1, 3), (3, 1)]\n",
    "        \n",
    "        # Current state\n",
    "        self.agent_pos = self.start_pos\n",
    "        \n",
    "        # Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "        self.actions = ['‚Üë', '‚Üì', '‚Üê', '‚Üí']\n",
    "        self.n_actions = len(self.actions)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï environment ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô\"\"\"\n",
    "        self.agent_pos = self.start_pos\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        ‡∏ó‡∏≥ action ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (next_state, reward, done)\n",
    "        \n",
    "        Args:\n",
    "            action: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "            \n",
    "        Returns:\n",
    "            next_state: ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "            reward: ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö\n",
    "            done: ‡∏à‡∏ö episode ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á\n",
    "        \"\"\"\n",
    "        row, col = self.agent_pos\n",
    "        \n",
    "        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "        if action == 0:  # Up\n",
    "            new_pos = (max(row - 1, 0), col)\n",
    "        elif action == 1:  # Down\n",
    "            new_pos = (min(row + 1, self.size - 1), col)\n",
    "        elif action == 2:  # Left\n",
    "            new_pos = (row, max(col - 1, 0))\n",
    "        elif action == 3:  # Right\n",
    "            new_pos = (row, min(col + 1, self.size - 1))\n",
    "        \n",
    "        self.agent_pos = new_pos\n",
    "        \n",
    "        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì reward\n",
    "        if new_pos == self.goal_pos:\n",
    "            reward = 10  # ‡∏ñ‡∏∂‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢!\n",
    "            done = True\n",
    "        elif new_pos in self.holes:\n",
    "            reward = -10  # ‡∏ï‡∏Å‡∏´‡∏•‡∏∏‡∏°!\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1  # penalty ‡πÄ‡∏ß‡∏•‡∏≤\n",
    "            done = False\n",
    "        \n",
    "        return new_pos, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û Grid World\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        \n",
    "        # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏¥‡∏î\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                color = 'white'\n",
    "                \n",
    "                # ‡∏™‡∏µ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á\n",
    "                if (i, j) == self.start_pos:\n",
    "                    color = 'lightgreen'\n",
    "                    label = 'S'\n",
    "                elif (i, j) == self.goal_pos:\n",
    "                    color = 'gold'\n",
    "                    label = 'G'\n",
    "                elif (i, j) in self.holes:\n",
    "                    color = 'red'\n",
    "                    label = 'X'\n",
    "                elif (i, j) == self.agent_pos:\n",
    "                    color = 'lightblue'\n",
    "                    label = 'ü§ñ'\n",
    "                else:\n",
    "                    label = ''\n",
    "                \n",
    "                # ‡∏ß‡∏≤‡∏î‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°\n",
    "                rect = Rectangle((j, self.size-1-i), 1, 1, \n",
    "                                facecolor=color, edgecolor='black', linewidth=2)\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # ‡πÉ‡∏™‡πà label\n",
    "                ax.text(j+0.5, self.size-0.5-i, label, \n",
    "                       ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(0, self.size)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.title('Grid World', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á environment\n",
    "env = GridWorld(size=5)\n",
    "print(\"‚úÖ Grid World created!\")\n",
    "print(f\"   Size: {env.size}√ó{env.size}\")\n",
    "print(f\"   Start: {env.start_pos}\")\n",
    "print(f\"   Goal: {env.goal_pos}\")\n",
    "print(f\"   Holes: {env.holes}\")\n",
    "print(f\"   Actions: {env.actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÅ‡∏™‡∏î‡∏á Grid World\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé≤ Part 2: Random Agent - ‡∏ß‡∏¥‡πà‡∏á‡∏™‡∏∏‡πà‡∏°‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(env, max_steps=50):\n",
    "    \"\"\"\n",
    "    Agent ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    trajectory = [state]\n",
    "    \n",
    "    print(\"üé≤ Random Agent starts!\\n\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°\n",
    "        action = np.random.randint(0, env.n_actions)\n",
    "        \n",
    "        # ‡∏ó‡∏≥ action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\n",
    "        trajectory.append(next_state)\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"Step {step+1}: {state} ‚Üí {env.actions[action]} ‚Üí {next_state}, Reward: {reward:+3.0f}\")\n",
    "        \n",
    "        if done:\n",
    "            if next_state == env.goal_pos:\n",
    "                print(f\"\\nüéâ SUCCESS! Reached goal in {step+1} steps!\")\n",
    "            else:\n",
    "                print(f\"\\nüò± FAILED! Fell into hole at {next_state}\")\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    else:\n",
    "        print(f\"\\n‚è±Ô∏è TIME OUT! Didn't reach goal in {max_steps} steps\")\n",
    "    \n",
    "    print(f\"\\nüìä Total Reward: {total_reward}\")\n",
    "    print(f\"üìç Trajectory: {' ‚Üí '.join([str(s) for s in trajectory])}\")\n",
    "    \n",
    "    return total_reward, trajectory\n",
    "\n",
    "# ‡∏ó‡∏î‡∏•‡∏≠‡∏á Random Agent\n",
    "reward, path = random_agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "print(\"üî¨ Running 100 episodes with Random Agent...\\n\")\n",
    "\n",
    "successes = 0\n",
    "failures = 0\n",
    "timeouts = 0\n",
    "rewards = []\n",
    "\n",
    "for episode in range(100):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(50):\n",
    "        action = np.random.randint(0, env.n_actions)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            if next_state == env.goal_pos:\n",
    "                successes += 1\n",
    "            else:\n",
    "                failures += 1\n",
    "            break\n",
    "    else:\n",
    "        timeouts += 1\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥\n",
    "print(\"üìä Statistics (100 episodes):\")\n",
    "print(f\"   ‚úÖ Success: {successes} ({successes}%)\")\n",
    "print(f\"   ‚ùå Failed (holes): {failures} ({failures}%)\")\n",
    "print(f\"   ‚è±Ô∏è Timeout: {timeouts} ({timeouts}%)\")\n",
    "print(f\"   üìà Average Reward: {np.mean(rewards):.2f}\")\n",
    "print(f\"   üèÜ Best Reward: {np.max(rewards)}\")\n",
    "print(f\"   üò¢ Worst Reward: {np.min(rewards)}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['Success', 'Failed', 'Timeout'], [successes, failures, timeouts], \n",
    "        color=['green', 'red', 'orange'])\n",
    "plt.title('Random Agent Performance', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rewards, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.axvline(np.mean(rewards), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(rewards):.1f}')\n",
    "plt.title('Reward Distribution', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Total Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation: Random Agent ‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÄ‡∏•‡∏¢!\")\n",
    "print(\"   ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Part 3: Rule-based Agent - ‡∏°‡∏µ‡∏Å‡∏é‡πÄ‡∏Å‡∏ì‡∏ë‡πå\n",
    "\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á Agent ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏é‡∏á‡πà‡∏≤‡∏¢‡πÜ: **\"‡πÄ‡∏î‡∏¥‡∏ô‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡∏Ç‡∏ß‡∏≤‡πÅ‡∏•‡∏∞‡∏•‡∏á\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_agent(env, max_steps=50):\n",
    "    \"\"\"\n",
    "    Agent ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏é: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ ‚Üí ‡πÄ‡∏î‡∏¥‡∏ô‡∏Ç‡∏ß‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏á\n",
    "    (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Goal ‡∏≠‡∏¢‡∏π‡πà‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏•‡πà‡∏≤‡∏á)\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    trajectory = [state]\n",
    "    \n",
    "    print(\"üß† Rule-based Agent starts!\\n\")\n",
    "    print(\"   Strategy: Prefer Right and Down\\n\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        row, col = state\n",
    "        \n",
    "        # ‡∏Å‡∏é: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Right ‡∏´‡∏£‡∏∑‡∏≠ Down (‡πÑ‡∏õ‡∏´‡∏≤ Goal)\n",
    "        if col < env.size - 1 and row < env.size - 1:\n",
    "            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Right ‡∏´‡∏£‡∏∑‡∏≠ Down ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°\n",
    "            action = np.random.choice([1, 3])  # Down or Right\n",
    "        elif col < env.size - 1:\n",
    "            action = 3  # Right\n",
    "        elif row < env.size - 1:\n",
    "            action = 1  # Down\n",
    "        else:\n",
    "            action = np.random.randint(0, env.n_actions)  # Random\n",
    "        \n",
    "        # ‡∏ó‡∏≥ action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\n",
    "        trajectory.append(next_state)\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"Step {step+1}: {state} ‚Üí {env.actions[action]} ‚Üí {next_state}, Reward: {reward:+3.0f}\")\n",
    "        \n",
    "        if done:\n",
    "            if next_state == env.goal_pos:\n",
    "                print(f\"\\nüéâ SUCCESS! Reached goal in {step+1} steps!\")\n",
    "            else:\n",
    "                print(f\"\\nüò± FAILED! Fell into hole at {next_state}\")\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    else:\n",
    "        print(f\"\\n‚è±Ô∏è TIME OUT!\")\n",
    "    \n",
    "    print(f\"\\nüìä Total Reward: {total_reward}\")\n",
    "    \n",
    "    return total_reward, trajectory\n",
    "\n",
    "# ‡∏ó‡∏î‡∏•‡∏≠‡∏á\n",
    "reward, path = rule_based_agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 4: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Random vs Rule-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á 2 ‡πÅ‡∏ö‡∏ö\n",
    "n_episodes = 100\n",
    "\n",
    "# Random Agent\n",
    "random_rewards = []\n",
    "random_success = 0\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(50):\n",
    "        action = np.random.randint(0, env.n_actions)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            if next_state == env.goal_pos:\n",
    "                random_success += 1\n",
    "            break\n",
    "    \n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "# Rule-based Agent  \n",
    "rule_rewards = []\n",
    "rule_success = 0\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(50):\n",
    "        row, col = env.agent_pos\n",
    "        \n",
    "        if col < env.size - 1 and row < env.size - 1:\n",
    "            action = np.random.choice([1, 3])\n",
    "        elif col < env.size - 1:\n",
    "            action = 3\n",
    "        elif row < env.size - 1:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = np.random.randint(0, env.n_actions)\n",
    "        \n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            if next_state == env.goal_pos:\n",
    "                rule_success += 1\n",
    "            break\n",
    "    \n",
    "    rule_rewards.append(total_reward)\n",
    "\n",
    "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "print(\"üìä Comparison (100 episodes each):\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüé≤ Random Agent:\")\n",
    "print(f\"   Success Rate: {random_success}%\")\n",
    "print(f\"   Avg Reward: {np.mean(random_rewards):.2f}\")\n",
    "\n",
    "print(f\"\\nüß† Rule-based Agent:\")\n",
    "print(f\"   Success Rate: {rule_success}%\")\n",
    "print(f\"   Avg Reward: {np.mean(rule_rewards):.2f}\")\n",
    "\n",
    "print(f\"\\nüí° Improvement: {rule_success - random_success}% higher success rate!\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Success Rate\n",
    "axes[0].bar(['Random', 'Rule-based'], [random_success, rule_success], \n",
    "           color=['red', 'green'], alpha=0.7)\n",
    "axes[0].set_ylabel('Success Rate (%)')\n",
    "axes[0].set_title('Success Rate Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Reward Distribution\n",
    "axes[1].hist(random_rewards, bins=20, alpha=0.5, label='Random', color='red')\n",
    "axes[1].hist(rule_rewards, bins=20, alpha=0.5, label='Rule-based', color='green')\n",
    "axes[1].axvline(np.mean(random_rewards), color='red', linestyle='--', linewidth=2)\n",
    "axes[1].axvline(np.mean(rule_rewards), color='green', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Total Reward')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Reward Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô\n",
    "\n",
    "### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "1. **Grid World Environment**\n",
    "   - State, Action, Reward\n",
    "   - Terminal states (Goal, Hole)\n",
    "   - Episode ‡πÅ‡∏•‡∏∞ Trajectory\n",
    "\n",
    "2. **Random Agent**\n",
    "   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°\n",
    "   - Success rate ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å (~10-20%)\n",
    "   - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "\n",
    "3. **Rule-based Agent**\n",
    "   - ‡∏°‡∏µ‡∏Å‡∏é‡∏á‡πà‡∏≤‡∏¢‡πÜ (‡πÄ‡∏î‡∏¥‡∏ô‡∏Ç‡∏ß‡∏≤/‡∏•‡∏á)\n",
    "   - Success rate ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ (~30-50%)\n",
    "   - ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå\n",
    "\n",
    "### ü§î ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:\n",
    "\n",
    "**‡∏à‡∏∞‡∏ó‡∏≥‡∏¢‡∏±‡∏á‡πÑ‡∏á‡πÉ‡∏´‡πâ Agent ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ?**\n",
    "\n",
    "‚Üí ‡∏ï‡∏≠‡∏ö: ‡πÉ‡∏ä‡πâ **Q-Learning**!\n",
    "\n",
    "### üöÄ Next Step:\n",
    "\n",
    "üëâ [02_mdp_concepts.ipynb](02_mdp_concepts.ipynb)\n",
    "\n",
    "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Markov Decision Process ‡πÅ‡∏•‡∏∞ Bellman Equation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
